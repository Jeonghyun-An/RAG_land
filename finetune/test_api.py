#!/usr/bin/env python3
"""
vLLM API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
finetune/test_api.py

ì‚¬ìš©ë²•:
    python finetune/test_api.py --url http://localhost:28080
"""

import argparse
import requests
import json

def test_vllm_api(base_url: str):
    """vLLM API í…ŒìŠ¤íŠ¸"""
    
    print("="*80)
    print("ğŸ§ª vLLM API í…ŒìŠ¤íŠ¸")
    print("="*80)
    print(f"ğŸ“Œ API URL: {base_url}")
    print("="*80)
    
    # 1. Health Check
    print("\n1ï¸âƒ£ Health Check")
    try:
        response = requests.get(f"{base_url}/health", timeout=5)
        if response.status_code == 200:
            print("   âœ… ì„œë²„ ì •ìƒ")
        else:
            print(f"   âŒ ì„œë²„ ì‘ë‹µ ì´ìƒ: {response.status_code}")
            return
    except Exception as e:
        print(f"   âŒ ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    # 2. Models í™•ì¸
    print("\n2ï¸âƒ£ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸")
    try:
        response = requests.get(f"{base_url}/v1/models", timeout=10)
        models = response.json()
        print(f"   ëª¨ë¸: {json.dumps(models, indent=2, ensure_ascii=False)}")
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    # 3. Chat Completion í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ Chat Completion í…ŒìŠ¤íŠ¸")
    
    test_messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. KINAC ê·œì •ê³¼ IAEA ê°€ì´ë“œë¼ì¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        },
        {
            "role": "user",
            "content": "ë°©ì‚¬ì„ ì‘ì—…ì¢…ì‚¬ìì˜ ì—°ê°„ ì„ ëŸ‰í•œë„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
        }
    ]
    
    payload = {
        "model": "Qwen/Qwen2.5-7B-Instruct",  # ë˜ëŠ” ë³‘í•© ëª¨ë¸ ê²½ë¡œ
        "messages": test_messages,
        "max_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.9
    }
    
    try:
        response = requests.post(
            f"{base_url}/v1/chat/completions",
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            answer = result['choices'][0]['message']['content']
            
            print("   âœ… ì‘ë‹µ ì„±ê³µ")
            print(f"\n   ì§ˆë¬¸: {test_messages[1]['content']}")
            print(f"\n   ë‹µë³€:\n   {answer}")
            print(f"\n   ì‚¬ìš© í† í°: {result.get('usage', {})}")
        else:
            print(f"   âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            print(f"   {response.text}")
    
    except Exception as e:
        print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 4. ì›ìë ¥ ë„ë©”ì¸ íŠ¹í™” í…ŒìŠ¤íŠ¸
    print("\n4ï¸âƒ£ ì›ìë ¥ ë„ë©”ì¸ íŠ¹í™” í…ŒìŠ¤íŠ¸")
    
    nuclear_questions = [
        "IAEA Safety Standardsì˜ Defence in Depth ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì›ìë¡œ ëƒ‰ê°ì¬ ìƒì‹¤ì‚¬ê³ (LOCA) ë°œìƒ ì‹œ ëŒ€ì‘ ì ˆì°¨ëŠ”?",
        "ê²©ë‚©ê±´ë¬¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    for i, question in enumerate(nuclear_questions, 1):
        print(f"\n   [{i}] {question}")
        
        payload["messages"] = [
            test_messages[0],  # system message
            {"role": "user", "content": question}
        ]
        
        try:
            response = requests.post(
                f"{base_url}/v1/chat/completions",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                answer = response.json()['choices'][0]['message']['content']
                print(f"   ë‹µë³€: {answer[:200]}...")
            else:
                print(f"   âŒ ì‹¤íŒ¨: {response.status_code}")
        
        except Exception as e:
            print(f"   âŒ ì—ëŸ¬: {e}")
    
    print("\n" + "="*80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="vLLM API í…ŒìŠ¤íŠ¸")
    parser.add_argument("--url", required=True, help="vLLM API URL (ì˜ˆ: http://localhost:28080)")
    
    args = parser.parse_args()
    test_vllm_api(args.url)