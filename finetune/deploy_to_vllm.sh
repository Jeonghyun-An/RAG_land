#!/bin/bash
"""
vLLM ì„œë²„ì— íŒŒì¸íŠœë‹ ëª¨ë¸ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
finetune/deploy_to_vllm.sh

ì‚¬ìš©ë²•:
    bash finetune/deploy_to_vllm.sh [OPTIONS]
    
ì˜µì…˜:
    --use-lora          LoRA ì–´ëŒ‘í„° ë°©ì‹ìœ¼ë¡œ ë°°í¬ (ê¸°ë³¸ê°’)
    --use-merged        ë³‘í•©ëœ ëª¨ë¸ë¡œ ë°°í¬
    --port 28080        vLLM ì„œë²„ í¬íŠ¸ (ê¸°ë³¸: 28080)
    
ì˜ˆì‹œ:
    # LoRA ì–´ëŒ‘í„° ë°©ì‹ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    bash finetune/deploy_to_vllm.sh --use-lora
    
    # ë³‘í•© ëª¨ë¸ ë°©ì‹
    bash finetune/deploy_to_vllm.sh --use-merged
"""

set -e  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¤‘ë‹¨

# ==================== ê¸°ë³¸ ì„¤ì • ====================
DEPLOYMENT_MODE="lora"  # lora or merged
VLLM_PORT=28080
BASE_MODEL="Qwen/Qwen2.5-7B-Instruct"
LORA_PATH="/workspace/output/qwen2.5-7b-nuclear-lora"
MERGED_PATH="/workspace/output/qwen2.5-7b-nuclear-merged"

# ==================== ì¸ì íŒŒì‹± ====================
while [[ $# -gt 0 ]]; do
    case $1 in
        --use-lora)
            DEPLOYMENT_MODE="lora"
            shift
            ;;
        --use-merged)
            DEPLOYMENT_MODE="merged"
            shift
            ;;
        --port)
            VLLM_PORT="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

echo "========================================"
echo "vLLM ì„œë²„ ë°°í¬"
echo "========================================"
echo "ë°°í¬ ëª¨ë“œ: $DEPLOYMENT_MODE"
echo "í¬íŠ¸: $VLLM_PORT"
echo "========================================"

# ==================== LoRA ë°©ì‹ ë°°í¬ ====================
if [ "$DEPLOYMENT_MODE" == "lora" ]; then
    echo ""
    echo "ğŸ“¦ LoRA ì–´ëŒ‘í„° ë°©ì‹ ë°°í¬"
    echo ""
    
    # LoRA ì–´ëŒ‘í„° ì¡´ì¬ í™•ì¸
    if [ ! -f "$LORA_PATH/adapter_config.json" ]; then
        echo " LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $LORA_PATH"
        echo "   ë¨¼ì € train_qlora.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!"
        exit 1
    fi
    
    echo " LoRA ì–´ëŒ‘í„° í™•ì¸ë¨: $LORA_PATH"
    
    # docker-compose íŒŒì¼ ìƒì„±
    cat > docker-compose.vllm-finetuned.yml << EOF
version: "3.9"

services:
  vllm-finetuned:
    image: vllm/vllm-openai:latest
    container_name: nuclear-vllm-finetuned
    
    volumes:
      - finetune-output:/workspace/lora_adapters:ro
      - /data/models/.hf-cache:/root/.cache/huggingface:rw
    
    environment:
      CUDA_VISIBLE_DEVICES: "0"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      HF_TOKEN: "\${HUGGINGFACE_TOKEN}"
    
    command:
      - --model
      - "$BASE_MODEL"
      - --enable-lora
      - --lora-modules
      - nuclear-lora=/workspace/lora_adapters/qwen2.5-7b-nuclear-lora
      - --max-lora-rank
      - "64"
      - --port
      - "8000"
      - --host
      - "0.0.0.0"
      - --dtype
      - bfloat16
      - --max-model-len
      - "4096"
      - --gpu-memory-utilization
      - "0.9"
      - --trust-remote-code
    
    ports:
      - "$VLLM_PORT:8000"
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]
    
    networks:
      - ragnet
    
    restart: unless-stopped

volumes:
  finetune-output:
    external: true
    name: nuclear-finetune-output

networks:
  ragnet:
    external: true
    name: ragnet
EOF

    echo ""
    echo " docker-compose.vllm-finetuned.yml ìƒì„±ë¨"
    echo ""
    echo " vLLM ì„œë²„ ì‹œì‘ ì¤‘..."
    
    docker-compose -f docker-compose.vllm-finetuned.yml up -d
    
    echo ""
    echo "â³ ì„œë²„ ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘ (60ì´ˆ)..."
    sleep 60
    
    # í—¬ìŠ¤ ì²´í¬
    echo ""
    echo "ğŸ” ì„œë²„ ìƒíƒœ í™•ì¸..."
    
    if curl -s http://localhost:$VLLM_PORT/health > /dev/null 2>&1; then
        echo " vLLM ì„œë²„ ì •ìƒ ì‘ë™ ì¤‘"
        echo ""
        echo " API ì—”ë“œí¬ì¸íŠ¸:"
        echo "   http://localhost:$VLLM_PORT/v1/completions"
        echo "   http://localhost:$VLLM_PORT/v1/chat/completions"
        echo ""
        echo " í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:"
        echo "   curl http://localhost:$VLLM_PORT/v1/models"
    else
        echo "âš ï¸  ì„œë²„ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        echo "   docker logs nuclear-vllm-finetuned ë¡œ ìƒíƒœ í™•ì¸"
    fi

# ==================== ë³‘í•© ëª¨ë¸ ë°©ì‹ ë°°í¬ ====================
elif [ "$DEPLOYMENT_MODE" == "merged" ]; then
    echo ""
    echo "ğŸ“¦ ë³‘í•© ëª¨ë¸ ë°©ì‹ ë°°í¬"
    echo ""
    
    # ë³‘í•© ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if [ ! -f "$MERGED_PATH/config.json" ]; then
        echo " ë³‘í•© ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $MERGED_PATH"
        echo "   ë¨¼ì € merge_model.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!"
        exit 1
    fi
    
    echo " ë³‘í•© ëª¨ë¸ í™•ì¸ë¨: $MERGED_PATH"
    
    # docker-compose íŒŒì¼ ìƒì„±
    cat > docker-compose.vllm-finetuned.yml << EOF
version: "3.9"

services:
  vllm-finetuned:
    image: vllm/vllm-openai:latest
    container_name: nuclear-vllm-finetuned
    
    volumes:
      - finetune-output:/workspace/models:ro
      - /data/models/.hf-cache:/root/.cache/huggingface:rw
    
    environment:
      CUDA_VISIBLE_DEVICES: "0"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      HF_TOKEN: "\${HUGGINGFACE_TOKEN}"
    
    command:
      - --model
      - "/workspace/models/qwen2.5-7b-nuclear-merged"
      - --port
      - "8000"
      - --host
      - "0.0.0.0"
      - --dtype
      - bfloat16
      - --max-model-len
      - "4096"
      - --gpu-memory-utilization
      - "0.9"
      - --trust-remote-code
    
    ports:
      - "$VLLM_PORT:8000"
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]
    
    networks:
      - ragnet
    
    restart: unless-stopped

volumes:
  finetune-output:
    external: true
    name: nuclear-finetune-output

networks:
  ragnet:
    external: true
    name: ragnet
EOF

    echo ""
    echo " docker-compose.vllm-finetuned.yml ìƒì„±ë¨"
    echo ""
    echo " vLLM ì„œë²„ ì‹œì‘ ì¤‘..."
    
    docker-compose -f docker-compose.vllm-finetuned.yml up -d
    
    echo ""
    echo " ì„œë²„ ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘ (60ì´ˆ)..."
    sleep 60
    
    # í—¬ìŠ¤ ì²´í¬
    echo ""
    echo " ì„œë²„ ìƒíƒœ í™•ì¸..."
    
    if curl -s http://localhost:$VLLM_PORT/health > /dev/null 2>&1; then
        echo " vLLM ì„œë²„ ì •ìƒ ì‘ë™ ì¤‘"
        echo ""
        echo " API ì—”ë“œí¬ì¸íŠ¸:"
        echo "   http://localhost:$VLLM_PORT/v1/completions"
        echo "   http://localhost:$VLLM_PORT/v1/chat/completions"
        echo ""
        echo " í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:"
        echo "   curl http://localhost:$VLLM_PORT/v1/models"
    else
        echo "  ì„œë²„ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        echo "   docker logs nuclear-vllm-finetuned ë¡œ ìƒíƒœ í™•ì¸"
    fi
fi

echo ""
echo "========================================"
echo " ë°°í¬ ì™„ë£Œ!"
echo "========================================"
echo ""
echo "ë‹¤ìŒ ë‹¨ê³„:"
echo "1. API í…ŒìŠ¤íŠ¸:"
echo "   python finetune/test_api.py --url http://localhost:$VLLM_PORT"
echo ""
echo "2. ì„±ëŠ¥ ë¹„êµ:"
echo "   python finetune/compare_models.py \\"
echo "     --base-url http://localhost:18080 \\"
echo "     --finetuned-url http://localhost:$VLLM_PORT"
echo ""
echo "3. ìš´ì˜ í™˜ê²½ ì—…ë°ì´íŠ¸:"
echo "   - nuclearchatì˜ VLLM_BASE_URLì„ http://nuclear-vllm-finetuned:8000ìœ¼ë¡œ ë³€ê²½"
echo "   - docker-compose restart llama"
echo ""
echo "========================================"