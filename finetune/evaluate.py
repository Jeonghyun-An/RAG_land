# finetune/evaluate.py
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset
import json
from tqdm import tqdm

# ì„¤ì •
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-7B-Instruct")
LORA_PATH = os.getenv("OUTPUT_DIR", "/workspace/output/qwen2.5-7b-nuclear-lora")
TEST_DATA = os.getenv("TEST_DATASET_PATH", "/workspace/data/test_qa.jsonl")

print("="*60)
print("ğŸ§ª Model Evaluation")
print("="*60)
print(f"ğŸ“¦ Base Model: {MODEL_NAME}")
print(f"ğŸ¯ LoRA Adapter: {LORA_PATH}")
print(f"ğŸ“Š Test Data: {TEST_DATA}")
print("="*60)

# ëª¨ë¸ ë¡œë“œ
print("ğŸ“¥ Loading model...")
tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model = PeftModel.from_pretrained(model, LORA_PATH)
model.eval()

print("âœ… Model loaded")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
print(f"ğŸ“‚ Loading test data...")
dataset = load_dataset('json', data_files=TEST_DATA)
test_data = dataset['train']

print(f"âœ… Loaded {len(test_data)} test examples")

# í‰ê°€
results = []
print("\nğŸ” Evaluating...")

for example in tqdm(test_data):
    instruction = example['instruction']
    expected_output = example['output']
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
"""
    
    # ìƒì„±
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ê²°ê³¼ ì €ì¥
    results.append({
        "instruction": instruction,
        "expected": expected_output,
        "generated": generated,
    })

# ê²°ê³¼ ì €ì¥
output_file = os.path.join(LORA_PATH, "evaluation_results.json")
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print("="*60)
print(f"âœ… Evaluation completed")
print(f"ğŸ’¾ Results saved to: {output_file}")
print("="*60)

# ìƒ˜í”Œ ì¶œë ¥
print("\nğŸ“ Sample Results (first 3):\n")
for i, result in enumerate(results[:3]):
    print(f"--- Example {i+1} ---")
    print(f"Q: {result['instruction']}")
    print(f"Expected: {result['expected'][:100]}...")
    print(f"Generated: {result['generated'][:100]}...")
    print()