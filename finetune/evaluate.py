#!/usr/bin/env python3
"""
íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
finetune/evaluate.py

ì‚¬ìš©ë²•:
    docker exec -it nuclear-finetune bash
    python finetune/evaluate.py
"""

import os
import sys
import torch
import json
from pathlib import Path
from typing import List, Dict
from tqdm import tqdm
from datetime import datetime

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset

# ==================== ì„¤ì • ====================
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-7B-Instruct")
LORA_PATH = os.getenv("OUTPUT_DIR", "/workspace/output/qwen2.5-7b-nuclear-lora")
TEST_DATA = os.getenv("TEST_DATASET_PATH", "/workspace/data/test_qa.jsonl")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("="*80)
print("ğŸ§ª íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€")
print("="*80)
print(f"ğŸ“¦ Base Model: {MODEL_NAME}")
print(f"ğŸ¯ LoRA Adapter: {LORA_PATH}")
print(f"ğŸ“Š Test Data: {TEST_DATA}")
print(f"ğŸ”§ Device: {DEVICE}")
print("="*80)

# ==================== ëª¨ë¸ ë¡œë“œ ====================
print("\nğŸ“¥ Loading model...")

try:
    # LoRA ì–´ëŒ‘í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    lora_config_path = Path(LORA_PATH) / "adapter_config.json"
    if not lora_config_path.exists():
        print(f"âŒ LoRA adapter not found at {LORA_PATH}")
        print("   Please run train_qlora.py first!")
        sys.exit(1)
    
    # Tokenizer ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        LORA_PATH,
        trust_remote_code=True
    )
    
    # Base ëª¨ë¸ ë¡œë“œ
    print(f"   Loading base model: {MODEL_NAME}")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # LoRA ì–´ëŒ‘í„° ì ìš©
    print(f"   Applying LoRA adapter: {LORA_PATH}")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()
    
    print("âœ… Model loaded successfully")
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“Š Model Info:")
    print(f"   Total params: {total_params:,}")
    print(f"   Trainable params: {trainable_params:,}")
    print(f"   Trainable ratio: {100 * trainable_params / total_params:.2f}%")

except Exception as e:
    print(f"âŒ Failed to load model: {e}")
    traceback.print_exc()
    sys.exit(1)

# ==================== í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ====================
print(f"\nğŸ“‚ Loading test data: {TEST_DATA}")

try:
    if not Path(TEST_DATA).exists():
        print(f"âŒ Test data not found: {TEST_DATA}")
        print("   Please run prepare_from_milvus.py first!")
        sys.exit(1)
    
    dataset = load_dataset('json', data_files=TEST_DATA)
    test_data = dataset['train']
    
    print(f"âœ… Loaded {len(test_data)} test examples")

except Exception as e:
    print(f"âŒ Failed to load test data: {e}")
    traceback.print_exc()
    sys.exit(1)

# ==================== í‰ê°€ í•¨ìˆ˜ ====================
def generate_response(instruction: str, input_text: str = "") -> str:
    """ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if input_text:
        prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. KINAC ê·œì •ê³¼ IAEA ê°€ì´ë“œë¼ì¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.<|im_end|>
<|im_start|>user
{instruction}

ì¶”ê°€ ì •ë³´: {input_text}<|im_end|>
<|im_start|>assistant
"""
    else:
        prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. KINAC ê·œì •ê³¼ IAEA ê°€ì´ë“œë¼ì¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
"""
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract assistant response
    if "<|im_start|>assistant" in generated_text:
        response = generated_text.split("<|im_start|>assistant")[-1].strip()
    else:
        response = generated_text
    
    return response

# ==================== í‰ê°€ ì‹¤í–‰ ====================
print("\nğŸ” Evaluating model...")
print("="*80)

results = []
total_time = 0

for i, example in enumerate(tqdm(test_data, desc="Evaluating")):
    instruction = example['instruction']
    input_text = example.get('input', '')
    expected_output = example['output']
    
    # ì‹œê°„ ì¸¡ì •
    start_time = datetime.now()
    
    try:
        generated_output = generate_response(instruction, input_text)
        
        end_time = datetime.now()
        inference_time = (end_time - start_time).total_seconds()
        total_time += inference_time
        
        result = {
            "index": i,
            "instruction": instruction,
            "input": input_text,
            "expected": expected_output,
            "generated": generated_output,
            "inference_time": inference_time,
            "success": True
        }
        
    except Exception as e:
        result = {
            "index": i,
            "instruction": instruction,
            "input": input_text,
            "expected": expected_output,
            "generated": f"[ERROR] {str(e)}",
            "inference_time": 0,
            "success": False
        }
    
    results.append(result)

# ==================== ê²°ê³¼ ë¶„ì„ ====================
print("\n" + "="*80)
print("ğŸ“Š í‰ê°€ ê²°ê³¼")
print("="*80)

successful_count = sum(1 for r in results if r['success'])
failed_count = len(results) - successful_count
avg_time = total_time / len(results) if results else 0

print(f"\nğŸ“ˆ í†µê³„:")
print(f"   ì´ í…ŒìŠ¤íŠ¸: {len(results)}")
print(f"   ì„±ê³µ: {successful_count}")
print(f"   ì‹¤íŒ¨: {failed_count}")
print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f}ì´ˆ")
print(f"   ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

# ==================== ê²°ê³¼ ì €ì¥ ====================
output_dir = Path(LORA_PATH)
output_dir.mkdir(parents=True, exist_ok=True)

# ì „ì²´ ê²°ê³¼ ì €ì¥
results_file = output_dir / "evaluation_results.json"
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump({
        "metadata": {
            "model_name": MODEL_NAME,
            "lora_path": LORA_PATH,
            "test_data": TEST_DATA,
            "total_examples": len(results),
            "successful": successful_count,
            "failed": failed_count,
            "avg_inference_time": avg_time,
            "total_time": total_time,
            "evaluated_at": datetime.now().isoformat()
        },
        "results": results
    }, f, ensure_ascii=False, indent=2)

print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥:")
print(f"   {results_file}")

# ìƒ˜í”Œ ê²°ê³¼ ì €ì¥ (í…ìŠ¤íŠ¸ íŒŒì¼)
samples_file = output_dir / "evaluation_samples.txt"
with open(samples_file, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ ìƒ˜í”Œ ê²°ê³¼\n")
    f.write("="*80 + "\n\n")
    
    for i, result in enumerate(results[:10]):  # ì²« 10ê°œë§Œ
        f.write(f"[Sample {i+1}]\n")
        f.write("-"*80 + "\n")
        f.write(f"ì§ˆë¬¸: {result['instruction']}\n")
        if result['input']:
            f.write(f"ì…ë ¥: {result['input']}\n")
        f.write(f"\nê¸°ëŒ€ ì‘ë‹µ:\n{result['expected']}\n")
        f.write(f"\nìƒì„± ì‘ë‹µ:\n{result['generated']}\n")
        f.write(f"\nì¶”ë¡  ì‹œê°„: {result['inference_time']:.2f}ì´ˆ\n")
        f.write("="*80 + "\n\n")

print(f"   {samples_file}")

# ==================== ìƒ˜í”Œ ì¶œë ¥ ====================
print("\nğŸ“ ìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 3ê°œ):\n")
print("="*80)

for i, result in enumerate(results[:3]):
    print(f"\n[Sample {i+1}]")
    print("-"*80)
    print(f"ì§ˆë¬¸: {result['instruction']}")
    if result['input']:
        print(f"ì…ë ¥: {result['input']}")
    print(f"\nê¸°ëŒ€ ì‘ë‹µ:\n{result['expected'][:200]}...")
    print(f"\nìƒì„± ì‘ë‹µ:\n{result['generated'][:200]}...")
    print(f"\nì¶”ë¡  ì‹œê°„: {result['inference_time']:.2f}ì´ˆ")
    print("="*80)

print("\nâœ… í‰ê°€ ì™„ë£Œ!")
print(f"ğŸ“‚ ìƒì„¸ ê²°ê³¼: {output_dir}")