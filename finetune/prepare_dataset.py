# finetune/prepare_dataset.py
import json
import random
from pathlib import Path
from typing import List, Dict

def create_sample_dataset():
    """
    ìƒ˜í”Œ ì›ìë ¥ ì•ˆì „ QA ë°ì´í„°ì…‹ ìƒì„±
    ì‹¤ì œë¡œëŠ” Milvusë‚˜ CUBRIDì—ì„œ ì¶”ì¶œ
    """
    
    # ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ìˆ˜ì²œ ê°œ í•„ìš”)
    qa_pairs = [
        {
            "instruction": "ë°©ì‚¬ì„ ì‘ì—…ì¢…ì‚¬ìì˜ ì—°ê°„ ìœ íš¨ì„ ëŸ‰ í•œë„ëŠ”?",
            "input": "",
            "output": "ë°©ì‚¬ì„ ì‘ì—…ì¢…ì‚¬ìì˜ ìœ íš¨ì„ ëŸ‰ í•œë„ëŠ” ì—°ê°„ 50mSvì´ë©°, 5ë…„ê°„ ëˆ„ì ì„ ëŸ‰ì´ 100mSvë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. (ì›ìë ¥ì•ˆì „ë²• ì‹œí–‰ë ¹ ì œ57ì¡°)"
        },
        {
            "instruction": "ì›ìë¡œ ëƒ‰ê°ì¬ ê³„í†µì˜ ì£¼ìš” êµ¬ì„±ìš”ì†ŒëŠ”?",
            "input": "ê°€ì••ê²½ìˆ˜ë¡œ(PWR) ê¸°ì¤€",
            "output": "ì›ìë¡œ ëƒ‰ê°ì¬ ê³„í†µì€ ì›ìë¡œ ì••ë ¥ìš©ê¸°, ì¦ê¸°ë°œìƒê¸°(2~4ê°œ), ì›ìë¡œëƒ‰ê°ì¬íŒí”„(4ê°œ), ê°€ì••ê¸°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. 1ì°¨ ê³„í†µì€ ì•½ 15.5 MPaì˜ ê³ ì••ìœ¼ë¡œ ìš´ì „ë©ë‹ˆë‹¤."
        },
        {
            "instruction": "IAEA Safety Standards ì¤‘ SF-1ì˜ ëª©ì ì€?",
            "input": "",
            "output": "SF-1(Fundamental Safety Principles)ì€ ì›ìë ¥ ì•ˆì „ì˜ ê¸°ë³¸ ì›ì¹™ì„ ì œì‹œí•˜ëŠ” ìµœìƒìœ„ ë¬¸ì„œë¡œ, ë°©ì‚¬ì„  ìœ„í—˜ìœ¼ë¡œë¶€í„° ì‚¬ëŒê³¼ í™˜ê²½ì„ ë³´í˜¸í•˜ê¸° ìœ„í•œ 10ê°€ì§€ ì•ˆì „ ì›ì¹™ì„ ì •ì˜í•©ë‹ˆë‹¤."
        },
        {
            "instruction": "ì›ìë ¥ë°œì „ì†Œì˜ ì‹¬ì¸µë°©ì–´ ê°œë…ì€?",
            "input": "",
            "output": "ì‹¬ì¸µë°©ì–´ëŠ” 5ê°œ ì¸µìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤: 1ì¸µ-ì„¤ê³„ í’ˆì§ˆ ë³´ì¦, 2ì¸µ-ìš´ì „ ì•ˆì „, 3ì¸µ-ì„¤ê³„ê¸°ì¤€ì‚¬ê³  ëŒ€ì‘, 4ì¸µ-ì¤‘ëŒ€ì‚¬ê³  ê´€ë¦¬, 5ì¸µ-ë°©ì‚¬ëŠ¥ ëˆ„ì¶œ ì™„í™”ì…ë‹ˆë‹¤."
        },
        {
            "instruction": "ì œì–´ë´‰ì˜ ì£¼ìš” ê¸°ëŠ¥ì€?",
            "input": "",
            "output": "ì œì–´ë´‰ì€ ì¤‘ì„±ìë¥¼ í¡ìˆ˜í•˜ì—¬ í•µë¶„ì—´ ë°˜ì‘ì„ ì œì–´í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ì€ ë°˜ì‘ë„ ì œì–´, ì¶œë ¥ ì¡°ì ˆ, ê¸´ê¸‰ ì •ì§€(SCRAM) ìˆ˜í–‰ì…ë‹ˆë‹¤."
        },
        {
            "instruction": "ê²©ë‚©ê±´ë¬¼ì˜ ì„¤ê³„ ì••ë ¥ì€?",
            "input": "APR1400 ê¸°ì¤€",
            "output": "APR1400ì˜ ê²©ë‚©ê±´ë¬¼ ì„¤ê³„ì••ë ¥ì€ ì•½ 392 kPa(absolute)ì´ë©°, LOCA ì‹œ ì••ë ¥ ìƒìŠ¹ì„ ê²¬ë”œ ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë©ë‹ˆë‹¤."
        },
        {
            "instruction": "ì•ˆì „ì£¼ì…ê³„í†µì˜ ëª©ì ì€?",
            "input": "",
            "output": "ì•ˆì „ì£¼ì…ê³„í†µì€ ëƒ‰ê°ì¬ ìƒì‹¤ì‚¬ê³ (LOCA) ì‹œ ì›ìë¡œì— ë¶•ì‚°ìˆ˜ë¥¼ ì£¼ì…í•˜ì—¬ ë…¸ì‹¬ì„ ëƒ‰ê°í•˜ê³  í•µë¶„ì—´ ë°˜ì‘ì„ ì •ì§€ì‹œí‚µë‹ˆë‹¤."
        },
        {
            "instruction": "ë°©ì‚¬ì„±íê¸°ë¬¼ì˜ ë¶„ë¥˜ëŠ”?",
            "input": "",
            "output": "ë°©ì‚¬ì„±íê¸°ë¬¼ì€ ì¤‘Â·ì €ì¤€ìœ„ì™€ ê³ ì¤€ìœ„ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. ì¤‘Â·ì €ì¤€ìœ„ëŠ” ìš´ì „íê¸°ë¬¼, ê³ ì¤€ìœ„ëŠ” ì‚¬ìš©í›„í•µì—°ë£Œê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤."
        },
    ]
    
    # ë°ì´í„° ì¦ê°•: ë³€í˜• ìƒì„±
    augmented_data = []
    for qa in qa_pairs:
        # ì›ë³¸
        augmented_data.append(qa)
        
        # ë³€í˜• 1: ë‹¤ë¥¸ í‘œí˜„
        if "í•œë„" in qa['instruction']:
            augmented_data.append({
                "instruction": qa['instruction'].replace("í•œë„", "ì œí•œê°’"),
                "input": qa['input'],
                "output": qa['output']
            })
        
        # ë³€í˜• 2: ì¶•ì•½í˜•
        if len(qa['output']) > 100:
            augmented_data.append({
                "instruction": f"{qa['instruction']} (ê°„ë‹¨íˆ)",
                "input": qa['input'],
                "output": qa['output'].split('.')[0] + "."
            })
        
        # ë³€í˜• 3: ìƒì„¸ ì§ˆë¬¸
        if "ì£¼ìš”" in qa['instruction']:
            augmented_data.append({
                "instruction": qa['instruction'].replace("ì£¼ìš”", "ëª¨ë“ "),
                "input": qa['input'],
                "output": qa['output']
            })
    
    return augmented_data

def save_dataset(data: List[Dict], output_path: str):
    """JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… Saved {len(data)} examples to {output_path}")

def main():
    # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œëŠ” /workspace/dataë¡œ ë§ˆìš´íŠ¸ë¨
    base_path = "/workspace/data"
    
    # ë°ì´í„° ìƒì„±
    dataset = create_sample_dataset()
    
    # Train/Test ë¶„í•  (90/10)
    random.shuffle(dataset)
    split_idx = int(len(dataset) * 0.9)
    
    train_data = dataset[:split_idx]
    test_data = dataset[split_idx:]
    
    # ì €ì¥ (Docker ë‚´ë¶€ ê²½ë¡œ)
    save_dataset(train_data, f"{base_path}/nuclear_qa.jsonl")
    save_dataset(test_data, f"{base_path}/test_qa.jsonl")
    
    print(f"ğŸ“Š Train: {len(train_data)} | Test: {len(test_data)}")
    print(f"ğŸ’¾ Files saved to: {base_path}")

if __name__ == "__main__":
    main()