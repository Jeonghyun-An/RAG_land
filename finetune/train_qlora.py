# finetune/train_qlora.py
import os
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== ì„¤ì • ====================
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-7B-Instruct")
DATASET_PATH = os.getenv("DATASET_PATH", "/workspace/data/nuclear_qa.jsonl")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "/workspace/output/qwen2.5-7b-nuclear-lora")

# QLoRA ì„¤ì • (A4000 16GB ìµœì í™”)
LORA_R = int(os.getenv("LORA_R", "16"))
LORA_ALPHA = int(os.getenv("LORA_ALPHA", "32"))
LORA_DROPOUT = float(os.getenv("LORA_DROPOUT", "0.05"))

# í•™ìŠµ ì„¤ì •
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "2"))
GRADIENT_ACCUMULATION = int(os.getenv("GRADIENT_ACCUMULATION", "8"))
NUM_EPOCHS = int(os.getenv("NUM_EPOCHS", "3"))
LEARNING_RATE = float(os.getenv("LEARNING_RATE", "2e-4"))
MAX_SEQ_LENGTH = int(os.getenv("MAX_SEQ_LENGTH", "2048"))

logger.info("="*60)
logger.info("ğŸš€ Nuclear Safety Fine-tuning (QLoRA)")
logger.info("="*60)
logger.info(f"ğŸ“¦ Model: {MODEL_NAME}")
logger.info(f"ğŸ“Š Dataset: {DATASET_PATH}")
logger.info(f"ğŸ’¾ Output: {OUTPUT_DIR}")
logger.info(f"ğŸ›ï¸  LoRA Config: r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}")
logger.info(f"ğŸ”§ Batch: {BATCH_SIZE}, Grad Accum: {GRADIENT_ACCUMULATION}")
logger.info(f"ğŸ“ˆ Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}")
logger.info("="*60)

# ==================== 4bit ì–‘ìí™” ì„¤ì • ====================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,  # Nested quantization
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ==================== ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ ====================
logger.info("ğŸ“¥ Loading model and tokenizer...")

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# Gradient checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

logger.info("âœ… Model loaded successfully")

# ==================== LoRA ì„¤ì • ====================
lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
logger.info("="*60)
model.print_trainable_parameters()
logger.info("="*60)

# ==================== ë°ì´í„°ì…‹ ë¡œë“œ ====================
logger.info(f"ğŸ“‚ Loading dataset from {DATASET_PATH}...")

try:
    dataset = load_dataset('json', data_files=DATASET_PATH)
    logger.info(f"âœ… Dataset loaded: {len(dataset['train'])} examples")
except Exception as e:
    logger.error(f"âŒ Failed to load dataset: {e}")
    raise

def format_instruction(example):
    """
    Qwen2.5 Instruct í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    instruction = example['instruction']
    input_text = example.get('input', '')
    output = example['output']
    
    # Qwen2.5 Chat Template
    if input_text:
        prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. KINAC ê·œì •ê³¼ IAEA ê°€ì´ë“œë¼ì¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.<|im_end|>
<|im_start|>user
{instruction}

ì¶”ê°€ ì •ë³´: {input_text}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""
    else:
        prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. KINAC ê·œì •ê³¼ IAEA ê°€ì´ë“œë¼ì¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""
    
    return {"text": prompt}

# ë°ì´í„°ì…‹ í¬ë§¤íŒ…
tokenized_dataset = dataset.map(
    format_instruction,
    remove_columns=dataset['train'].column_names
)

def tokenize_function(examples):
    result = tokenizer(
        examples["text"],
        truncation=True,
        max_length=MAX_SEQ_LENGTH,
        padding="max_length"
    )
    result["labels"] = result["input_ids"].copy()
    return result

tokenized_dataset = tokenized_dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

logger.info(f"âœ… Tokenization completed")

# ==================== Trainer ì„¤ì • ====================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    
    # ë°°ì¹˜ ì„¤ì •
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    
    # í•™ìŠµ ì„¤ì •
    num_train_epochs=NUM_EPOCHS,
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    
    # ìµœì í™”
    fp16=False,
    bf16=True,  # A4000ì—ì„œ bfloat16 ê¶Œì¥
    optim="paged_adamw_8bit",  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì˜µí‹°ë§ˆì´ì €
    
    # ë¡œê¹…
    logging_steps=10,
    logging_dir=f"{OUTPUT_DIR}/logs",
    report_to="tensorboard",
    
    # ì €ì¥
    save_strategy="steps",
    save_steps=100,
    save_total_limit=3,
    
    # ê¸°íƒ€
    remove_unused_columns=False,
    dataloader_pin_memory=True,
    group_by_length=True,  # ê¸¸ì´ë³„ ê·¸ë£¹í™”ë¡œ íš¨ìœ¨ í–¥ìƒ
    ddp_find_unused_parameters=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

# ==================== í•™ìŠµ ì‹œì‘ ====================
logger.info("="*60)
logger.info("ğŸ”¥ Starting training...")
logger.info("="*60)

try:
    trainer.train()
    logger.info("="*60)
    logger.info("âœ… Training completed successfully!")
    logger.info("="*60)
    
    # LoRA ì–´ëŒ‘í„° ì €ì¥
    logger.info(f"ğŸ’¾ Saving LoRA adapter to {OUTPUT_DIR}...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    logger.info("="*60)
    logger.info("ğŸ‰ Fine-tuning completed!")
    logger.info(f"ğŸ“‚ Output saved to: {OUTPUT_DIR}")
    logger.info("="*60)
    
except Exception as e:
    logger.error("="*60)
    logger.error(f"âŒ Training failed: {e}")
    logger.error("="*60)
    raise