#!/usr/bin/env python3
"""
ê°œì„ ëœ ê²½ëŸ‰ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ - Tokenizer ë¬¸ì œ í•´ê²°
finetune/evaluate_lite.py
"""

import os
import sys
import torch
import time
from datetime import datetime
from pathlib import Path

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ==================== ì„¤ì • ====================
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-7B-Instruct")
LORA_PATH = os.getenv("OUTPUT_DIR", "/workspace/output/qwen2.5-7b-nuclear-lora")
MAX_NEW_TOKENS = 200
GENERATION_TIMEOUT = 30

print("="*80)
print(" ê°œì„ ëœ ê²½ëŸ‰ íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€ (v2)")
print("="*80)
print(f" Base Model: {MODEL_NAME}")
print(f" LoRA Path: {LORA_PATH}")
print("="*80)

# ==================== ëª¨ë¸ ë¡œë“œ ====================
print("\nğŸ“¥ Loading model...")
start_time = time.time()

try:
    tokenizer = AutoTokenizer.from_pretrained(LORA_PATH, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()
    
    load_time = time.time() - start_time
    print(f" Model loaded ({load_time:.1f}s)")
    
    if torch.cuda.is_available():
        gpu_allocated = torch.cuda.memory_allocated(0) / 1e9
        print(f" GPU Memory: {gpu_allocated:.2f} GB")

except Exception as e:
    print(f" Failed: {e}")
    sys.exit(1)

# ==================== í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ====================
test_questions = [
    {
        "question": "ë°©ì‚¬ì„ ì‘ì—…ì¢…ì‚¬ìì˜ ì—°ê°„ ì„ ëŸ‰í•œë„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
        "category": "ë²•ê·œ_ê¸°ë³¸",
        "expected_keywords": ["50mSv", "ì—°ê°„", "ì„ ëŸ‰í•œë„", "ë°€ë¦¬ì‹œë²„íŠ¸"]
    },
    {
        "question": "ì›ìë ¥ì•ˆì „ë²•ì—ì„œ ê·œì •í•˜ëŠ” ë°©ì‚¬ì„ ê´€ë¦¬êµ¬ì—­ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "category": "ë²•ê·œ_ê¸°ë³¸",
        "expected_keywords": ["ë°©ì‚¬ì„ ê´€ë¦¬êµ¬ì—­", "ì„ ëŸ‰", "ê¸°ì¤€"]
    },
    {
        "question": "IAEAì˜ Defence in Depth(ì‹¬ì¸µë°©í˜¸) ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "category": "IAEA",
        "expected_keywords": ["defence", "depth", "ì‹¬ì¸µë°©í˜¸", "level", "ë‹¨ê³„"]
    },
    {
        "question": "ì›ìë¡œ ëƒ‰ê°ì¬ ìƒì‹¤ì‚¬ê³  LOCAê°€ ë°œìƒí•˜ë©´ ì–´ë–»ê²Œ ëŒ€ì‘í•˜ë‚˜ìš”?",
        "category": "ê¸°ìˆ ",
        "expected_keywords": ["ëƒ‰ê°ì¬", "LOCA", "ECCS", "ë¹„ìƒ", "ëƒ‰ê°"]
    },
    {
        "question": "ê²©ë‚©ê±´ë¬¼ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "category": "ê¸°ìˆ ",
        "expected_keywords": ["ê²©ë‚©ê±´ë¬¼", "ë°©ì‚¬ì„±ë¬¼ì§ˆ", "ì°¨ë‹¨", "ë³´í˜¸"]
    },
]

print(f"\nğŸ“ Test questions: {len(test_questions)}")

# ==================== í‰ê°€ í•¨ìˆ˜ ====================
def generate_response(question: str) -> tuple:
    """ì‘ë‹µ ìƒì„± - ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬"""
    
    # Qwen2.5 Chat í…œí”Œë¦¿ ì‚¬ìš©
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. KINAC ê·œì •ê³¼ IAEA ê°€ì´ë“œë¼ì¸ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."},
        {"role": "user", "content": question}
    ]
    
    # apply_chat_template ì‚¬ìš© (ê¶Œì¥ ë°©ì‹)
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    try:
        start = time.time()
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        inference_time = time.time() - start
        
        # ì…ë ¥ ë¶€ë¶„ ì œê±°í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”©
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(
            generated_ids, 
            skip_special_tokens=True
        )[0].strip()
        
        return response, inference_time, True
    
    except Exception as e:
        return f"[ERROR: {str(e)}]", 0, False

# ==================== í‰ê°€ ì‹¤í–‰ ====================
print("\n Evaluating...\n" + "="*80)

results = []
total_time = 0
success_count = 0

for i, test_case in enumerate(test_questions, 1):
    question = test_case["question"]
    category = test_case["category"]
    expected_keywords = test_case.get("expected_keywords", [])
    
    print(f"\n[{i}/{len(test_questions)}] [{category}]")
    print(f"Q: {question}")
    
    response, inference_time, success = generate_response(question)
    
    if success:
        success_count += 1
        total_time += inference_time
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        response_lower = response.lower()
        matched = sum(1 for kw in expected_keywords if kw.lower() in response_lower)
        match_rate = matched / len(expected_keywords) if expected_keywords else 0
        
        # ì‘ë‹µ ì¶œë ¥ (ì²˜ìŒ 200ì)
        display_text = response[:200] + "..." if len(response) > 200 else response
        print(f"A: {display_text}")
        print(f"â±  {inference_time:.2f}s | í‚¤ì›Œë“œ: {matched}/{len(expected_keywords)} ({match_rate*100:.0f}%)")
        
        result = {
            "index": i,
            "category": category,
            "question": question,
            "response": response,
            "inference_time": inference_time,
            "matched_keywords": matched,
            "total_keywords": len(expected_keywords),
            "match_rate": match_rate,
            "success": True
        }
    else:
        print(f" Failed: {response}")
        result = {
            "index": i,
            "category": category,
            "question": question,
            "response": response,
            "inference_time": 0,
            "success": False
        }
    
    results.append(result)

# ==================== ê²°ê³¼ ë¶„ì„ ====================
print("\n" + "="*80)
print(" í‰ê°€ ê²°ê³¼")
print("="*80)

successful_results = [r for r in results if r["success"]]

if successful_results:
    avg_time = total_time / len(successful_results)
    avg_match_rate = sum(r["match_rate"] for r in successful_results) / len(successful_results)
    
    print(f"\n ì„±ê³µ: {success_count}/{len(test_questions)}")
    print(f"â±  í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f}s")
    print(f" í‰ê·  í‚¤ì›Œë“œ ë§¤ì¹­ë¥ : {avg_match_rate*100:.1f}%")
    
    # ì¹´í…Œê³ ë¦¬ë³„
    categories = {}
    for r in successful_results:
        cat = r["category"]
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(r["match_rate"])
    
    print(f"\n ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹­ë¥ :")
    for cat, rates in categories.items():
        avg_rate = sum(rates) / len(rates)
        print(f"   [{cat}]: {avg_rate*100:.1f}%")
    
    # íŒì •
    print(f"\n ì¢…í•© í‰ê°€:")
    if avg_match_rate >= 0.7:
        print("    ìš°ìˆ˜ - íŒŒì¸íŠœë‹ ì„±ê³µ!")
    elif avg_match_rate >= 0.5:
        print("     ë³´í†µ - ì¶”ê°€ í•™ìŠµ ê¶Œì¥")
    else:
        print("    ë¯¸í¡ - ì¬í•™ìŠµ í•„ìš”")
else:
    print("\n ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

# ==================== ê²°ê³¼ ì €ì¥ ====================
output_dir = Path(LORA_PATH)
output_file = output_dir / "evaluation_lite_v2_results.txt"

with open(output_file, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("ê°œì„ ëœ ê²½ëŸ‰ í‰ê°€ ê²°ê³¼ (v2)\n")
    f.write("="*80 + "\n\n")
    f.write(f"í‰ê°€ ì‹œê°„: {datetime.now().isoformat()}\n")
    f.write(f"ì„±ê³µ: {success_count}/{len(test_questions)}\n")
    if successful_results:
        f.write(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f}s\n")
        f.write(f"í‰ê·  í‚¤ì›Œë“œ ë§¤ì¹­ë¥ : {avg_match_rate*100:.1f}%\n")
    f.write("\n" + "="*80 + "\n\n")
    
    for r in results:
        f.write(f"[{r['index']}] [{r['category']}]\n")
        f.write(f"Q: {r['question']}\n")
        if r['success']:
            f.write(f"A: {r['response']}\n")
            f.write(f"ì‹œê°„: {r['inference_time']:.2f}s | ë§¤ì¹­: {r['matched_keywords']}/{r['total_keywords']}\n")
        else:
            f.write(f" ì‹¤íŒ¨: {r['response']}\n")
        f.write("\n" + "-"*80 + "\n\n")

print(f"\n ê²°ê³¼ ì €ì¥: {output_file}")
print("\n" + "="*80)
print(" í‰ê°€ ì™„ë£Œ!")
print("="*80)