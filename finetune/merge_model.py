#!/usr/bin/env python3
"""
LoRA ì–´ëŒ‘í„° ë³‘í•© ìŠ¤í¬ë¦½íŠ¸
finetune/merge_model.py

ì‚¬ìš©ë²•:
    docker exec -it nuclear-finetune bash
    python finetune/merge_model.py
    
ì„¤ëª…:
    LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
    vLLMì—ì„œ LoRAë¥¼ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import os
import sys
import torch
from pathlib import Path
from datetime import datetime

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ==================== ì„¤ì • ====================
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-7B-Instruct")
LORA_PATH = os.getenv("OUTPUT_DIR", "/workspace/output/qwen2.5-7b-nuclear-lora")
MERGED_OUTPUT_DIR = os.getenv("MERGED_OUTPUT_DIR", "/workspace/output/qwen2.5-7b-nuclear-merged")

print("="*80)
print("ğŸ”§ LoRA ì–´ëŒ‘í„° ë³‘í•©")
print("="*80)
print(f"ğŸ“¦ Base Model: {MODEL_NAME}")
print(f"ğŸ¯ LoRA Adapter: {LORA_PATH}")
print(f"ğŸ’¾ Output Directory: {MERGED_OUTPUT_DIR}")
print("="*80)

# ==================== ê²€ì¦ ====================
lora_config_path = Path(LORA_PATH) / "adapter_config.json"
if not lora_config_path.exists():
    print(f"âŒ LoRA adapter not found at {LORA_PATH}")
    print("   Please run train_qlora.py first!")
    sys.exit(1)

# ==================== ëª¨ë¸ ë¡œë“œ ====================
print("\nğŸ“¥ Loading models...")

try:
    # Tokenizer
    print("   Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        LORA_PATH,
        trust_remote_code=True
    )
    
    # Base ëª¨ë¸
    print(f"   Loading base model: {MODEL_NAME}")
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # LoRA ì–´ëŒ‘í„° ì ìš©
    print(f"   Applying LoRA adapter: {LORA_PATH}")
    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    
    print(" Models loaded successfully")

except Exception as e:
    print(f" Failed to load models: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ==================== ë³‘í•© ====================
print("\nğŸ”„ Merging LoRA weights into base model...")
print("   This may take several minutes...")

try:
    # LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©
    merged_model = model.merge_and_unload()
    
    print(" Merge completed successfully")

except Exception as e:
    print(f" Failed to merge models: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ==================== ì €ì¥ ====================
print(f"\nğŸ’¾ Saving merged model to {MERGED_OUTPUT_DIR}...")

try:
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(MERGED_OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ëª¨ë¸ ì €ì¥
    print("   Saving model weights...")
    merged_model.save_pretrained(
        MERGED_OUTPUT_DIR,
        safe_serialization=True,  # Safetensors í˜•ì‹ ì‚¬ìš©
        max_shard_size="2GB"
    )
    
    # Tokenizer ì €ì¥
    print("   Saving tokenizer...")
    tokenizer.save_pretrained(MERGED_OUTPUT_DIR)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "base_model": MODEL_NAME,
        "lora_adapter": LORA_PATH,
        "merged_at": datetime.now().isoformat(),
        "merge_method": "merge_and_unload",
        "dtype": "bfloat16"
    }
    
    import json
    metadata_file = output_dir / "merge_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(" Model saved successfully")

except Exception as e:
    print(f" Failed to save model: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# ==================== ê²€ì¦ ====================
print("\n Verifying saved model...")

try:
    # ì €ì¥ëœ ëª¨ë¸ì´ ë¡œë“œ ê°€ëŠ¥í•œì§€ í™•ì¸
    test_model = AutoModelForCausalLM.from_pretrained(
        MERGED_OUTPUT_DIR,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    test_tokenizer = AutoTokenizer.from_pretrained(
        MERGED_OUTPUT_DIR,
        trust_remote_code=True
    )
    
    print(" Verification successful")
    
    # ëª¨ë¸ í¬ê¸° ì •ë³´
    total_params = sum(p.numel() for p in test_model.parameters())
    print(f"\n Merged Model Info:")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Model size: ~{total_params * 2 / 1e9:.2f} GB (bfloat16)")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("\n Quick test:")
    test_input = "ì›ìë ¥ ì•ˆì „ì˜ ì¤‘ìš”ì„±ì€?"
    prompt = f"""<|im_start|>system
ë‹¹ì‹ ì€ ì›ìë ¥ ì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.<|im_end|>
<|im_start|>user
{test_input}<|im_end|>
<|im_start|>assistant
"""
    
    inputs = test_tokenizer(prompt, return_tensors="pt").to(test_model.device)
    with torch.no_grad():
        outputs = test_model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True
        )
    
    response = test_tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "<|im_start|>assistant" in response:
        response = response.split("<|im_start|>assistant")[-1].strip()
    
    print(f"   Q: {test_input}")
    print(f"   A: {response[:200]}...")

except Exception as e:
    print(f"âš ï¸  Verification failed: {e}")
    import traceback
    traceback.print_exc()

# ==================== ì™„ë£Œ ====================
print("\n" + "="*80)
print("ë³‘í•© ì™„ë£Œ!")
print("="*80)
print(f"\nì¶œë ¥ ê²½ë¡œ: {MERGED_OUTPUT_DIR}")
print("\në‹¤ìŒ ë‹¨ê³„:")
print("1. vLLM ì„¤ì •ì—ì„œ MODEL_NAMEì„ ë³€ê²½:")
print(f"   MODEL_NAME={MERGED_OUTPUT_DIR}")
print("\n2. docker-compose.yml ì¬ì‹œì‘:")
print("   docker-compose restart vllm")
print("\n3. ë˜ëŠ” ìƒˆ ì»¨í…Œì´ë„ˆë¡œ ë°°í¬:")
print("   docker-compose up -d")
print("="*80)