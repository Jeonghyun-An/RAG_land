#!/usr/bin/env python3
"""
ë² ì´ìŠ¤ ëª¨ë¸ vs íŒŒì¸íŠœë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
finetune/compare_models.py

ì‚¬ìš©ë²•:
    python finetune/compare_models.py --base-url http://192.168.12.72:18080 --finetuned-url http://192.168.12.72:28080
    
ì„¤ëª…:
    ìš´ì˜ ì¤‘ì¸ ë‘ vLLM ì„œë²„(ë² ì´ìŠ¤/íŒŒì¸íŠœë‹)ì˜ ì‘ë‹µì„ ë¹„êµ í‰ê°€í•©ë‹ˆë‹¤.
"""

import argparse
import requests
import json
from typing import List, Dict
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# ==================== í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸ ====================
NUCLEAR_TEST_QUESTIONS = [
    # ê¸°ë³¸ ê·œì •
    {
        "category": "ë²•ê·œ_ê¸°ë³¸",
        "question": "ë°©ì‚¬ì„ ì‘ì—…ì¢…ì‚¬ìì˜ ì—°ê°„ ì„ ëŸ‰í•œë„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
        "expected_keywords": ["50mSv", "ì—°ê°„", "ì„ ëŸ‰í•œë„"]
    },
    {
        "category": "ë²•ê·œ_ê¸°ë³¸",
        "question": "ì›ìë ¥ì•ˆì „ë²•ì—ì„œ ê·œì •í•˜ëŠ” ë°©ì‚¬ì„ ê´€ë¦¬êµ¬ì—­ì˜ ì •ì˜ëŠ”?",
        "expected_keywords": ["ë°©ì‚¬ì„ ê´€ë¦¬êµ¬ì—­", "ì„ ëŸ‰", "êµ¬ì—­"]
    },
    
    # IAEA ê°€ì´ë“œë¼ì¸
    {
        "category": "IAEA",
        "question": "IAEA Safety Standardsì—ì„œ Defence in Depthì˜ 5ê°€ì§€ ë ˆë²¨ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "expected_keywords": ["defence", "depth", "level", "5"]
    },
    {
        "category": "IAEA",
        "question": "IAEAì˜ Safety Culture ì •ì˜ëŠ”?",
        "expected_keywords": ["safety culture", "commitment", "protection"]
    },
    
    # ê¸°ìˆ ì  ì§ˆë¬¸
    {
        "category": "ê¸°ìˆ ",
        "question": "ì›ìë¡œ ëƒ‰ê°ì¬ ìƒì‹¤ì‚¬ê³ (LOCA) ì‹œ ëŒ€ì‘ ì ˆì°¨ëŠ”?",
        "expected_keywords": ["ëƒ‰ê°ì¬", "LOCA", "ë¹„ìƒë…¸ì‹¬ëƒ‰ê°ê³„í†µ", "ECCS"]
    },
    {
        "category": "ê¸°ìˆ ",
        "question": "ê²©ë‚©ê±´ë¬¼ ì„¤ê³„ì••ë ¥ ì‚°ì • ì‹œ ê³ ë ¤ì‚¬í•­ì€?",
        "expected_keywords": ["ê²©ë‚©ê±´ë¬¼", "ì„¤ê³„ì••ë ¥", "ì‚¬ê³ "]
    },
    
    # ì ˆì°¨/ë§¤ë‰´ì–¼
    {
        "category": "ì ˆì°¨",
        "question": "ë°©ì‚¬ì„  ë¹„ìƒì‹œ ì£¼ë¯¼ë³´í˜¸ì¡°ì¹˜ ì ˆì°¨ëŠ”?",
        "expected_keywords": ["ë¹„ìƒ", "ì£¼ë¯¼", "ë³´í˜¸ì¡°ì¹˜", "ëŒ€í”¼"]
    },
    {
        "category": "ì ˆì°¨",
        "question": "ë°©ì‚¬ì„±íê¸°ë¬¼ ì²˜ë¦¬ ì ˆì°¨ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "expected_keywords": ["íê¸°ë¬¼", "ì²˜ë¦¬", "ì €ì¥", "ì²˜ë¶„"]
    },
    
    # ë³µí•© ì§ˆë¬¸
    {
        "category": "ë³µí•©",
        "question": "ì¤‘ëŒ€ì‚¬ê³  ë°œìƒ ì‹œ ê²©ë‚©ê±´ë¬¼ ê±´ì „ì„± ìœ ì§€ë¥¼ ìœ„í•œ ì„¤ê³„ íŠ¹ì§•ê³¼ ìš´ì „ ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "expected_keywords": ["ì¤‘ëŒ€ì‚¬ê³ ", "ê²©ë‚©ê±´ë¬¼", "ê±´ì „ì„±", "ì„¤ê³„", "ì ˆì°¨"]
    },
    {
        "category": "ë³µí•©",
        "question": "ì›ìë ¥ë°œì „ì†Œì˜ ì‹¬ì¸µë°©í˜¸ ê°œë…ì´ ì‹¤ì œ ì•ˆì „ê³„í†µ ì„¤ê³„ì— ì–´ë–»ê²Œ ë°˜ì˜ë˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "expected_keywords": ["ì‹¬ì¸µë°©í˜¸", "defence in depth", "ì•ˆì „ê³„í†µ", "ë‹¤ì¤‘ì„±", "ë…ë¦½ì„±"]
    }
]

# ==================== API í˜¸ì¶œ í•¨ìˆ˜ ====================
def query_rag_api(base_url: str, query: str, doc_filter: List[str] = None) -> Dict:
    """RAG API í˜¸ì¶œ"""
    
    endpoint = f"{base_url}/java/chat"
    
    payload = {
        "query": query,
        "top_k": 5,
        "use_rerank": True
    }
    
    if doc_filter:
        payload["doc_filter"] = doc_filter
    
    try:
        response = requests.post(
            endpoint,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=120
        )
        response.raise_for_status()
        return response.json()
    
    except requests.exceptions.RequestException as e:
        return {
            "error": str(e),
            "answer": "[ERROR] API í˜¸ì¶œ ì‹¤íŒ¨",
            "chunks": []
        }

# ==================== í‰ê°€ í•¨ìˆ˜ ====================
def evaluate_response(response: str, expected_keywords: List[str]) -> Dict:
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    
    response_lower = response.lower()
    
    # í‚¤ì›Œë“œ ë§¤ì¹­
    matched_keywords = [kw for kw in expected_keywords if kw.lower() in response_lower]
    keyword_score = len(matched_keywords) / len(expected_keywords) if expected_keywords else 0
    
    # ì‘ë‹µ ê¸¸ì´ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
    length = len(response)
    if length < 50:
        length_score = 0.5
    elif length > 2000:
        length_score = 0.8
    else:
        length_score = 1.0
    
    # ì¢…í•© ì ìˆ˜
    total_score = (keyword_score * 0.7 + length_score * 0.3)
    
    return {
        "keyword_score": keyword_score,
        "length_score": length_score,
        "total_score": total_score,
        "matched_keywords": matched_keywords,
        "response_length": length
    }

# ==================== ë¹„êµ ì‹¤í–‰ ====================
def compare_models(base_url: str, finetuned_url: str, output_dir: str = "./comparison_results"):
    """ë‘ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
    
    print("="*80)
    print("ğŸ”¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("="*80)
    print(f"ğŸ“Œ Base Model URL: {base_url}")
    print(f"ğŸ“Œ Finetuned Model URL: {finetuned_url}")
    print(f"ğŸ“Š Test Questions: {len(NUCLEAR_TEST_QUESTIONS)}")
    print("="*80)
    
    results = []
    
    for i, test_case in enumerate(tqdm(NUCLEAR_TEST_QUESTIONS, desc="Testing")):
        question = test_case["question"]
        category = test_case["category"]
        expected_keywords = test_case["expected_keywords"]
        
        # Base ëª¨ë¸ ì¿¼ë¦¬
        base_response = query_rag_api(base_url, question)
        base_answer = base_response.get("answer", "[ERROR]")
        
        # Finetuned ëª¨ë¸ ì¿¼ë¦¬
        finetuned_response = query_rag_api(finetuned_url, question)
        finetuned_answer = finetuned_response.get("answer", "[ERROR]")
        
        # í‰ê°€
        base_eval = evaluate_response(base_answer, expected_keywords)
        finetuned_eval = evaluate_response(finetuned_answer, expected_keywords)
        
        # ê²°ê³¼ ì €ì¥
        result = {
            "index": i,
            "category": category,
            "question": question,
            "expected_keywords": expected_keywords,
            "base_model": {
                "answer": base_answer,
                "evaluation": base_eval,
                "chunks_count": len(base_response.get("chunks", []))
            },
            "finetuned_model": {
                "answer": finetuned_answer,
                "evaluation": finetuned_eval,
                "chunks_count": len(finetuned_response.get("chunks", []))
            },
            "winner": "finetuned" if finetuned_eval["total_score"] > base_eval["total_score"] else "base" if base_eval["total_score"] > finetuned_eval["total_score"] else "tie"
        }
        
        results.append(result)
    
    # ==================== í†µê³„ ë¶„ì„ ====================
    total_tests = len(results)
    base_wins = sum(1 for r in results if r["winner"] == "base")
    finetuned_wins = sum(1 for r in results if r["winner"] == "finetuned")
    ties = sum(1 for r in results if r["winner"] == "tie")
    
    avg_base_score = sum(r["base_model"]["evaluation"]["total_score"] for r in results) / total_tests
    avg_finetuned_score = sum(r["finetuned_model"]["evaluation"]["total_score"] for r in results) / total_tests
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    category_stats = {}
    for result in results:
        cat = result["category"]
        if cat not in category_stats:
            category_stats[cat] = {
                "base_scores": [],
                "finetuned_scores": [],
                "base_wins": 0,
                "finetuned_wins": 0
            }
        
        category_stats[cat]["base_scores"].append(result["base_model"]["evaluation"]["total_score"])
        category_stats[cat]["finetuned_scores"].append(result["finetuned_model"]["evaluation"]["total_score"])
        
        if result["winner"] == "base":
            category_stats[cat]["base_wins"] += 1
        elif result["winner"] == "finetuned":
            category_stats[cat]["finetuned_wins"] += 1
    
    # ==================== ê²°ê³¼ ì¶œë ¥ ====================
    print("\n" + "="*80)
    print("ğŸ“Š ë¹„êµ ê²°ê³¼")
    print("="*80)
    print(f"\nì „ì²´ í†µê³„:")
    print(f"  ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
    print(f"  Base ëª¨ë¸ ìŠ¹ë¦¬: {base_wins} ({base_wins/total_tests*100:.1f}%)")
    print(f"  Finetuned ëª¨ë¸ ìŠ¹ë¦¬: {finetuned_wins} ({finetuned_wins/total_tests*100:.1f}%)")
    print(f"  ë™ì : {ties} ({ties/total_tests*100:.1f}%)")
    print(f"\ní‰ê·  ì ìˆ˜:")
    print(f"  Base ëª¨ë¸: {avg_base_score:.3f}")
    print(f"  Finetuned ëª¨ë¸: {avg_finetuned_score:.3f}")
    print(f"  ê°œì„ ìœ¨: {(avg_finetuned_score - avg_base_score) / avg_base_score * 100:+.1f}%")
    
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
    for cat, stats in category_stats.items():
        avg_base = sum(stats["base_scores"]) / len(stats["base_scores"])
        avg_finetuned = sum(stats["finetuned_scores"]) / len(stats["finetuned_scores"])
        print(f"  [{cat}]")
        print(f"    Base: {avg_base:.3f} (ìŠ¹ë¦¬ {stats['base_wins']}íšŒ)")
        print(f"    Finetuned: {avg_finetuned:.3f} (ìŠ¹ë¦¬ {stats['finetuned_wins']}íšŒ)")
        print(f"    ê°œì„ ìœ¨: {(avg_finetuned - avg_base) / avg_base * 100:+.1f}%")
    
    # ==================== ê²°ê³¼ ì €ì¥ ====================
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # JSON ì €ì¥
    results_file = output_path / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "metadata": {
                "base_url": base_url,
                "finetuned_url": finetuned_url,
                "total_tests": total_tests,
                "tested_at": datetime.now().isoformat()
            },
            "statistics": {
                "base_wins": base_wins,
                "finetuned_wins": finetuned_wins,
                "ties": ties,
                "avg_base_score": avg_base_score,
                "avg_finetuned_score": avg_finetuned_score,
                "improvement": (avg_finetuned_score - avg_base_score) / avg_base_score * 100
            },
            "category_stats": category_stats,
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_file}")
    
    # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥
    report_file = output_path / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸\n")
        f.write("="*80 + "\n\n")
        
        # ìƒ˜í”Œ ë¹„êµ (ìŠ¹ì/íŒ¨ì ê° 3ê°œ)
        finetuned_better = [r for r in results if r["winner"] == "finetuned"][:3]
        base_better = [r for r in results if r["winner"] == "base"][:3]
        
        if finetuned_better:
            f.write(" Finetuned ëª¨ë¸ì´ ìš°ìˆ˜í•œ ì¼€ì´ìŠ¤:\n\n")
            for r in finetuned_better:
                f.write(f"[{r['category']}] {r['question']}\n")
                f.write(f"Base ì ìˆ˜: {r['base_model']['evaluation']['total_score']:.3f}\n")
                f.write(f"Finetuned ì ìˆ˜: {r['finetuned_model']['evaluation']['total_score']:.3f}\n")
                f.write("-"*80 + "\n\n")
        
        if base_better:
            f.write(" Base ëª¨ë¸ì´ ìš°ìˆ˜í•œ ì¼€ì´ìŠ¤:\n\n")
            for r in base_better:
                f.write(f"[{r['category']}] {r['question']}\n")
                f.write(f"Base ì ìˆ˜: {r['base_model']['evaluation']['total_score']:.3f}\n")
                f.write(f"Finetuned ì ìˆ˜: {r['finetuned_model']['evaluation']['total_score']:.3f}\n")
                f.write("-"*80 + "\n\n")
    
    print(f"ğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    print("="*80)

# ==================== ë©”ì¸ ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    parser.add_argument("--base-url", required=True, help="Base ëª¨ë¸ API URL")
    parser.add_argument("--finetuned-url", required=True, help="Finetuned ëª¨ë¸ API URL")
    parser.add_argument("--output-dir", default="./comparison_results", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    compare_models(args.base_url, args.finetuned_url, args.output_dir)