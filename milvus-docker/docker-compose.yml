services:
  etcd:
    image: quay.io/coreos/etcd:v3.5.0
    container_name: etcd
    environment:
      - ETCD_AUTO_COMPACTION_RETENTION=1
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
      - ETCD_ENABLE_V2=true
      - ETCD_LOG_LEVEL=info
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
      - ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380
      - ETCD_INITIAL_ADVERTISE_PEER_URLS=http://etcd:2380
      - ETCD_INITIAL_CLUSTER=default=http://etcd:2380
      - ETCD_NAME=default
      - ETCDCTL_API=3
    ports:
      - "2379:2379"
      - "2380:2380"
    volumes:
      - milvus_etcd_data:/etcd
    networks:
      - ragnet
    healthcheck:
      # wget 대신 etcdctl로 헬스체크
      test: ["CMD", "etcdctl", "endpoint", "health", "--endpoints=http://127.0.0.1:2379"]
      interval: 5s
      timeout: 3s
      retries: 10
    restart: unless-stopped
  gotenberg:
    image: gotenberg/gotenberg:8
    container_name: gotenberg
    networks: [ragnet]
    ports: []               # 외부 노출 불필요 (fastapi가 내부에서 호출)
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/health"]
      interval: 5s
      timeout: 3s
      retries: 20
      start_period: 10s
    restart: unless-stopped

  minio:
    image: minio/minio:RELEASE.2023-01-20T02-05-44Z
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
      # 최신 이미지로 바꾸면:
      # MINIO_ROOT_USER: minioadmin
      # MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks:
      - ragnet
    healthcheck:
      # 이 이미지에 curl 있음(이미 Healthy 떴던 걸로 확인)
      test: ["CMD", "curl", "-fsS", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped

  milvus:
    image: milvusdb/milvus:v2.2.11
    container_name: milvus
    command: ["milvus", "run", "standalone"]
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_USE_SSL=false
      # 버킷 이름(없으면 Milvus가 만들어줍니다)
      - MINIO_BUCKET_NAME=milvus-bucket
      # 일부 버전에서는 이 키를 씁니다(겸사겸사 넣어도 무해)
      - MINIO_BUCKET=milvus-bucket
    ports:
      - "19530:19530"
      - "9091:9091"
    volumes:
      - milvus_data:/var/lib/milvus
    networks:
      - ragnet
    depends_on:
      etcd:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:9091/healthz"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    networks: [ragnet]
    ports:
      - "8001:8000"               # 호스트 8001 → 컨테이너 8000
    environment:
      HF_HUB_OFFLINE: "0"         # 오프라인이면 1
      TRANSFORMERS_OFFLINE: "0"
    volumes:
      - ../models/.hf-cache:/models:ro   # 오프라인 캐시 마운트(아래 4단계에서 채움)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           capabilities: [gpu]
    # 로컬 PC에선 GPU 없을 수 있어서, 프로필로 꺼 두기
    profiles: ["gpu"]
    # 서버(L40s)에서만 이 프로필을 켜서 실행하면 됨
    # 로컬에서 굳이 GPU 설정 안 넣어도 됨. 서버에선 다음 중 하나 사용:
    # gpus: all   (Compose v2)
    # 또는 swarm 모드라면 deploy.resources.reservations.devices...
    command: [
      "python","-m","vllm.entrypoints.openai.api_server",
      "--model","/models/meta-llama/Llama-3.2-1B-Instruct",
      "--served-model-name","llama-1b",   # 나중에 서버 이식 시에 두 토큰 추가
      "--max-model-len","8192",
      "--tensor-parallel-size","1",
      "--trust-remote-code"
    ]
    restart: unless-stopped

  vllm-8b:
    image: vllm/vllm-openai:latest
    container_name: vllm-8b
    networks: [ragnet]
    ports:
      - "8002:8000"
    environment:
      HF_HUB_OFFLINE: "0"
      TRANSFORMERS_OFFLINE: "0"
    volumes:
      - ../models/.hf-cache:/models:ro
    profiles: ["gpu"]
    command: [
      "python","-m","vllm.entrypoints.openai.api_server",
      "--model","/models/saltlux/Ko-Llama3-Luxia-8B",
      "--served-model-name","ko-8b",      # 별칭과 동일하게
      "--max-model-len","4096",
      "--tensor-parallel-size","1",
      "--trust-remote-code"
    ]
    restart: unless-stopped

  fastapi:
    build:
      context: ..
      dockerfile: milvus-docker/Dockerfile
    container_name: fastapi
    ports:
      - "8000:8000"
    volumes:
      - ..:/app
      - ../models/.hf-cache:/models
    environment:
      IS_DOCKER: "true"
      # MinIO/Milvus 접속 명시 (컨테이너 네트워크 이름으로)
      MINIO_ENDPOINT: "minio:9000"
      MINIO_ACCESS_KEY: "minioadmin"
      MINIO_SECRET_KEY: "minioadmin"
      MILVUS_HOST: "milvus"
      MILVUS_PORT: "19530"
      MILVUS_COLLECTION: "rag_chunks_v2"
      RAG_JOB_STATE_PERSIST: "minio"
      MILVUS_SECTION_MAX: "512"  # 섹션 최대 문자 수
      MILVUS_DOCID_MAX: "256"     # doc_id 최대 문자 수
      MILVUS_CHUNK_MAX: "8192"    # 청크 최대 문자 수
      RAG_SECTION_MAX: "160"  # 레이아웃 인지 청킹 섹션 최대 문자 수
      SEC_CAP: "160"  # 레이아웃 인지 청킹 섹션 최대 문자 수
      # 컬렉션 초기화/업서트 옵션 (필요 시만 1로)
      RAG_RESET_COLLECTION: "0"
      # Dedup / 교체 정책
      RAG_SKIP_IF_EXISTS: "0"              # 같은 doc_id 있으면 스킵 (매니페스트 OFF일 때)
      RAG_REPLACE_DOC: "1"                 # 같은 doc_id 있으면 삭제 후 재삽입
      RAG_DEDUP_MANIFEST: "1"              # MinIO docs/{doc_id}.json 해시 비교/저장
      RAG_UNIQUE_SUFFIX_ON_CONFLICT: "1"   # REPLACE=0이고 충돌이면 doc_id__hash 로 새로 삽입
      RAG_DELETE_AFTER_INDEX: "0"
      # RAG 청킹/인코딩 옵션
      RAG_CHUNK_TOKENS: 256             # 청킹 토큰 수
      RAG_CHUNK_OVERLAP: 64              # 청킹 오버랩 토큰 수
      RAG_SCORE_THRESHOLD: 0.2           # 리랭커 스코어 컷오프
      RAG_PARSE_LAYOUT: 1                # 레이아웃 인지 청킹
      RAG_PARSE_LAYOUT_MIN_CHARS: 100    # 레이아웃 인지 청킹 최소 문자 수
      RAG_PARSE_LAYOUT_MAX_TOKENS: 512   # 레이아웃 인지 청킹 최대 토큰 수
      RAG_PARSE_LAYOUT_MIN_WORDS: 10     # 레이아웃 인지 청킹 최소 단어 수
      RAG_PARSE_LAYOUT_MAX_WORDS: 100    # 레이아웃 인지 청킹 최대 단어 수
      # --- RAG 청킹/인코딩 옵션 ---
      RAG_PARSE_PDF: "1"                 # PDF 파싱 여부
      RAG_PARSE_DOCX: "1"                # DOCX 파싱 여부
      RAG_PARSE_XLSX: "1"                # XLSX 파싱 여부
      RAG_PARSE_TXT: "1"                 # TXT 파싱 여부
      RAG_PARSE_HTML: "1"                # HTML 파싱 여부
      RAG_PARSE_PPTX: "1"                # PPTX 파싱 여부
      # --- DEBUG/로깅 ---
      DEBUG_PEEK_MAX_CHARS: 0  # 디버그용: 청크 미리보기 최대 문자 수
      # --- OCR/파서 ---
      OCR_MODE: "auto"             # auto | always | never
      OCR_ENGINE: "paddle"         # paddle | tesseract | easyocr
      OCR_LANG: "korean"           # paddle: korean / tesseract: kor+eng / easyocr: ko,en
      OCR_DPI: "300"
      OCR_MIN_CHARS: "40"
      OCR_MIN_CHARS_PER_PAGE : "50"
      OCR_MAX_PAGES_FOR_OCR : "500"
      # tesseract 바이너리 경로가 필요하면(윈도우 베이스 아님) 비워둬도 OK
      OCR_TESSERACT_CMD: ""
      OCR_EASYOCR_GPU: "0"

      # --- 변환(pdf_converter) ---
      GOTENBERG_URL: "http://gotenberg:3000"
      CONVERT_BACKENDS: "gotenberg,soffice"  # gotenberg 실패 시 soffice 폴백
      GOTENBERG_TIMEOUT: "120"
      GOTENBERG_MAX_RETRIES: "3"
      GOTENBERG_BACKOFF_BASE: "0.6"
      PDF_PAPER: "A4"              # 보고서 품질 안정용
      PDF_MARGIN_MM: "10"

      # --- DOCX/XLSX direct 파싱 여부 ---
      RAG_PARSE_DIRECT_DOCX: "1"
      RAG_PARSE_DIRECT_XLSX: "1"
      RAG_CONVERT_NONPDF_TO_PDF: "1"   # 기타 포맷 변환 허용
      # 임베딩
      EMBEDDING_MODEL: "jhgan/ko-sbert-nli" 
      EMBED_MAX_TOKENS: "128"
      # 모델 기본값/토큰
      MODEL_ID: "meta-llama/Llama-3.2-1B-Instruct"
      HUGGINGFACE_TOKEN: "${HUGGINGFACE_TOKEN}"   # .env나 OS env에서 가져옴(없으면 빈 값)
      PYTHONUNBUFFERED: "1"
      OPENAI_BASE_URL: "http://vllm:8000/v1"
      OPENAI_API_KEY: "not-used"
      MODEL_ALIASES: >
        {"llama-1b":"meta-llama/Llama-3.2-1B-Instruct",
        "ko-8b":"saltlux/Ko-Llama3-Luxia-8B"}
      USE_VLLM: "0"  # vllm 프로필을 켜면 1로 바꾸기
      HF_HOME: "/models"
      OPENAI_ALIAS_URLS: >
        {"llama-1b":"http://vllm:8000/v1", "ko-8b":"http://vllm-8b:8000/v1"}
      # (선택) vllm 프로필을 켜면 아래 ENV도 설정
    # (선택) 기본 별칭명도 ENV로
    # DEFAULT_MODEL_ALIAS: "llama-1b"
    networks:
      - ragnet
    depends_on:
      milvus:
        condition: service_healthy
      minio:
        condition: service_healthy
      gotenberg:
        condition: service_healthy
         # ⚠ 로컬에서 vllm 프로필을 안 켜면 depends_on에 넣지 않음
      # vllm:
      #   condition: service_started
    restart: unless-stopped

volumes:
  milvus_etcd_data:
  milvus_data:
  minio_data:

networks:
  ragnet:
