services:
  etcd:
    image: quay.io/coreos/etcd:v3.5.0
    container_name: etcd
    environment:
      - ETCD_AUTO_COMPACTION_RETENTION=1
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
      - ETCD_ENABLE_V2=true
      - ETCD_LOG_LEVEL=info
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
      - ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380
      - ETCD_INITIAL_ADVERTISE_PEER_URLS=http://etcd:2380
      - ETCD_INITIAL_CLUSTER=default=http://etcd:2380
      - ETCD_NAME=default
      - ETCDCTL_API=3
    ports:
      - "2379:2379"
      - "2380:2380"
    volumes:
      - milvus_etcd_data:/etcd
    networks:
      - ragnet
    healthcheck:
      # wget 대신 etcdctl로 헬스체크
      test: ["CMD", "etcdctl", "endpoint", "health", "--endpoints=http://127.0.0.1:2379"]
      interval: 5s
      timeout: 3s
      retries: 10
    restart: unless-stopped

  minio:
    image: minio/minio:RELEASE.2023-01-20T02-05-44Z
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
      # 최신 이미지로 바꾸면:
      # MINIO_ROOT_USER: minioadmin
      # MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks:
      - ragnet
    healthcheck:
      # 이 이미지에 curl 있음(이미 Healthy 떴던 걸로 확인)
      test: ["CMD", "curl", "-fsS", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped

  milvus:
    image: milvusdb/milvus:v2.2.11
    container_name: milvus
    command: ["milvus", "run", "standalone"]
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MINIO_USE_SSL=false
      # 버킷 이름(없으면 Milvus가 만들어줍니다)
      - MINIO_BUCKET_NAME=milvus-bucket
      # 일부 버전에서는 이 키를 씁니다(겸사겸사 넣어도 무해)
      - MINIO_BUCKET=milvus-bucket
    ports:
      - "19530:19530"
      - "9091:9091"
    volumes:
      - milvus_data:/var/lib/milvus
    networks:
      - ragnet
    depends_on:
      etcd:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:9091/healthz"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    networks: [ragnet]
    ports:
      - "8001:8000"               # 호스트 8001 → 컨테이너 8000
    environment:
      HF_HUB_OFFLINE: "0"         # 오프라인이면 1
      TRANSFORMERS_OFFLINE: "0"
    volumes:
      - ../models/.hf-cache:/models:ro   # 오프라인 캐시 마운트(아래 4단계에서 채움)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           capabilities: [gpu]
    # 로컬 PC에선 GPU 없을 수 있어서, 프로필로 꺼 두기
    profiles: ["gpu"]
    # 서버(L40s)에서만 이 프로필을 켜서 실행하면 됨
    # 로컬에서 굳이 GPU 설정 안 넣어도 됨. 서버에선 다음 중 하나 사용:
    # gpus: all   (Compose v2)
    # 또는 swarm 모드라면 deploy.resources.reservations.devices...
    command: [
      "python","-m","vllm.entrypoints.openai.api_server",
      "--model","/models/meta-llama/Llama-3.2-1B-Instruct",
      # "--served-model-name","llama-1b",   # 나중에 서버 이식 시에 두 토큰 추가
      "--max-model-len","8192",
      "--tensor-parallel-size","1",
      "--trust-remote-code"
    ]
    restart: unless-stopped

  fastapi:
    build:
      context: ..
      dockerfile: milvus-docker/Dockerfile
    container_name: fastapi
    ports:
      - "8000:8000"
    volumes:
      - ..:/app
      - ../models/.hf-cache:/models
    environment:
      IS_DOCKER: "true"
      # MinIO/Milvus 접속 명시 (컨테이너 네트워크 이름으로)
      MINIO_ENDPOINT: "minio:9000"
      MINIO_ACCESS_KEY: "minioadmin"
      MINIO_SECRET_KEY: "minioadmin"
      MILVUS_HOST: "milvus"
      MILVUS_PORT: "19530"
      # 모델 기본값/토큰
      MODEL_ID: "meta-llama/Llama-3.2-1B-Instruct"
      HUGGINGFACE_TOKEN: "${HUGGINGFACE_TOKEN}"   # .env나 OS env에서 가져옴(없으면 빈 값)
      PYTHONUNBUFFERED: "1"
      OPENAI_BASE_URL: "http://vllm:8000/v1"
      OPENAI_API_KEY: "not-used"
      MODEL_ALIASES: >
        {"llama-1b":"meta-llama/Llama-3.2-1B-Instruct",
        "ko-8b":"saltlux/Ko-Llama3-Luxia-8B"}
      USE_VLLM: "0"  # vllm 프로필을 켜면 1로 바꾸기
      HF_HOME: "/models"
      # (선택) vllm 프로필을 켜면 아래 ENV도 설정
    # (선택) 기본 별칭명도 ENV로
    # DEFAULT_MODEL_ALIAS: "llama-1b"
    networks:
      - ragnet
    depends_on:
      milvus:
        condition: service_healthy
      minio:
        condition: service_healthy
         # ⚠ 로컬에서 vllm 프로필을 안 켜면 depends_on에 넣지 않음
      # vllm:
      #   condition: service_started
    restart: unless-stopped

volumes:
  milvus_etcd_data:
  milvus_data:
  minio_data:

networks:
  ragnet:
