# stt-service/app.py
"""
Faster-Whisper Í∏∞Î∞ò STT ÎßàÏù¥ÌÅ¨Î°úÏÑúÎπÑÏä§
ÏõêÏûêÎ†• Î∂ÑÏïº Ï†ÑÎ¨∏ Ïö©Ïñ¥ ÏµúÏ†ÅÌôî Ìè¨Ìï®
"""
import os
import tempfile
import logging
from pathlib import Path
from typing import Literal, Optional, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from faster_whisper import WhisperModel

# ==================== Î°úÍπÖ ÏÑ§Ï†ï ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== ÌôòÍ≤Ω Î≥ÄÏàò ====================
MODEL_SIZE = os.getenv("STT_MODEL_SIZE", "medium")  # tiny, base, small, medium, large-v3
DEVICE = os.getenv("STT_DEVICE", "cuda")  # cuda, cpu
COMPUTE_TYPE = os.getenv("STT_COMPUTE_TYPE", "int8")  # int8, float16, float32
DOWNLOAD_ROOT = os.getenv("HF_HOME", "/models")
BEAM_SIZE = int(os.getenv("STT_BEAM_SIZE", "5"))
VAD_FILTER = os.getenv("STT_VAD_FILTER", "true").lower() == "true"

# ==================== ÏõêÏûêÎ†• Ï†ÑÎ¨∏ Ïö©Ïñ¥ ÏÇ¨Ï†Ñ ====================
NUCLEAR_GLOSSARY = [
    # Íµ≠Ï†úÍ∏∞Íµ¨
    "IAEA", "International Atomic Energy Agency",
    "KINS", "Korea Institute of Nuclear Safety",
    "KINAC", "Korea Institute of Nuclear Nonproliferation And Control",
    
    # ÌïµÏã¨ Í∞úÎÖê
    "ÌïµÎ¨ºÏßà", "nuclear material", "safeguards", "ÏïàÏ†ÑÏ°∞Ïπò",
    "PIV", "Physical Inventory Verification", "Î¨ºÎ¶¨Ï†Å Ïû¨Í≥†Í≤ÄÏ¶ù",
    "PIT", "Physical Inventory Taking", "Ïã§Î¨ºÏû¨Í≥†Ï°∞ÏÇ¨",
    "MBA", "Material Balance Area", "Î¨ºÏßàÏàòÏßÄÍµ¨Ïó≠",
    "KMP", "Key Measurement Point", "Ï£ºÏöîÏ∏°Ï†ïÏ†ê",
    
    # Ï†àÏ∞® Î∞è Î≥¥Í≥†
    "Ïû¨Í≥†Î≥ÄÎèôÎ≥¥Í≥†", "inventory change report", "ICR",
    "Î¨ºÏßàÏàòÏßÄÎ≥¥Í≥†", "material balance report", "MBR",
    "ÏãúÏÑ§Î∂ÄÏÜçÏÑú", "facility attachment",
    "ÏÑ§Í≥ÑÏ†ïÎ≥¥ÏßàÎ¨∏ÏÑú", "design information questionnaire", "DIQ",
    
    # ÏãúÏÑ§ Î∞è Ïû•ÎπÑ
    "Í≤©ÎÇ©Í±¥Î¨º", "containment building",
    "ÏõêÏûêÎ°ú", "reactor", "ÎÖ∏Ïã¨", "core",
    "ÏÇ¨Ïö©ÌõÑÏó∞Î£å", "spent fuel", "Ïã†Ïó∞Î£å", "fresh fuel",
    "CANDU", "Ï§ëÏàòÎ°ú", "Í≤ΩÏàòÎ°ú", "PWR", "pressurized water reactor",
    
    # Ï∏°Ï†ï Î∞è Î∂ÑÏÑù
    "ÏÑ†ÎüâÌïúÎèÑ", "dose limit", "Î∞ÄÎ¶¨ÏãúÎ≤ÑÌä∏", "millisievert", "mSv",
    "Î∞©ÏÇ¨ÏÑ†ÏûëÏóÖÏ¢ÖÏÇ¨Ïûê", "radiation worker",
    "Î∞©ÏÇ¨ÏÑ†Í¥ÄÎ¶¨Íµ¨Ïó≠", "radiation controlled area",
    "Ïò§ÏóºÎèÑ", "contamination level",
    
    # ÏïàÏ†Ñ Î∞è ÏÇ¨Í≥†
    "LOCA", "loss of coolant accident", "ÎÉâÍ∞ÅÏû¨ÏÉÅÏã§ÏÇ¨Í≥†",
    "ECCS", "emergency core cooling system", "ÎπÑÏÉÅÎÖ∏Ïã¨ÎÉâÍ∞ÅÍ≥ÑÌÜµ",
    "Ïã¨Ï∏µÎ∞©Ìò∏", "defence in depth", "defense in depth",
    "Ï§ëÎåÄÏÇ¨Í≥†", "severe accident",
    
    # Î≤ïÍ∑ú
    "ÏõêÏûêÎ†•ÏïàÏ†ÑÎ≤ï", "Nuclear Safety Act",
    "ÏõêÏûêÎ†•ÏãúÏÑ§Îì±ÏùòÎ∞©Ìò∏Î∞èÎ∞©ÏÇ¨Îä•Î∞©Ïû¨ÎåÄÏ±ÖÎ≤ï",
    "Ï†ú57Ï°∞", "article 57", "Ï†ú58Ï°∞", "article 58",
]

NUCLEAR_PROMPT = """ÏõêÏûêÎ†• ÏïàÏ†Ñ, KINAC Í∑úÏ†ï, IAEA ÏïàÏ†ÑÏ°∞Ïπò, ÌïµÎ¨ºÏßà Ïû¨Í≥† Í¥ÄÎ¶¨, 
Î∞©ÏÇ¨ÏÑ† ÏïàÏ†Ñ, ÏõêÏûêÎ°ú Ïö¥ÏòÅ, ÏÇ¨Ïö©ÌõÑÏó∞Î£å Í¥ÄÎ¶¨"""

# ==================== Ï†ÑÏó≠ Î™®Îç∏ ====================
_MODEL = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏÉùÎ™ÖÏ£ºÍ∏∞ Í¥ÄÎ¶¨"""
    global _MODEL
    
    logger.info("="*60)
    logger.info("STT Service Starting")
    logger.info("="*60)
    logger.info(f"Model Size: {MODEL_SIZE}")
    logger.info(f"Device: {DEVICE}")
    logger.info(f"Compute Type: {COMPUTE_TYPE}")
    logger.info(f"Download Root: {DOWNLOAD_ROOT}")
    logger.info(f"Beam Size: {BEAM_SIZE}")
    logger.info(f"VAD Filter: {VAD_FILTER}")
    logger.info("="*60)
    
    try:
        logger.info(f"üì• Loading Faster-Whisper {MODEL_SIZE}...")
        _MODEL = WhisperModel(
            MODEL_SIZE,
            device=DEVICE,
            compute_type=COMPUTE_TYPE,
            download_root=DOWNLOAD_ROOT,
            num_workers=4,  # Î≥ëÎ†¨ Ï≤òÎ¶¨
        )
        logger.info("Model loaded successfully")
        
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        raise
    
    yield
    
    logger.info("STT Service Shutting Down")

# ==================== FastAPI Ïï± ====================
app = FastAPI(
    title="Nuclear STT Service",
    description="Faster-Whisper Í∏∞Î∞ò ÏõêÏûêÎ†• Î∂ÑÏïº ÌäπÌôî ÏùåÏÑ±Ïù∏Ïãù ÏÑúÎπÑÏä§",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ÏÑ§Ï†ï
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ÏóîÎìúÌè¨Ïù∏Ìä∏ ====================

@app.get("/")
async def root():
    """Î£®Ìä∏ ÏóîÎìúÌè¨Ïù∏Ìä∏"""
    return {
        "service": "Nuclear STT Service",
        "model": MODEL_SIZE,
        "device": DEVICE,
        "compute_type": COMPUTE_TYPE,
        "status": "ready" if _MODEL else "loading"
    }


@app.get("/health")
async def health_check():
    """Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
    if _MODEL is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    return {
        "status": "healthy",
        "model": MODEL_SIZE,
        "device": DEVICE
    }


@app.post("/transcribe")
async def transcribe_audio(
    audio: UploadFile = File(...),
    language: str = Form("ko"),  # ko, en, auto
    task: Literal["transcribe", "translate"] = Form("transcribe"),
    use_nuclear_context: bool = Form(True),  # ÏõêÏûêÎ†• Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© Ïó¨Î∂Ä
    return_segments: bool = Form(False),  # ÏÉÅÏÑ∏ ÏÑ∏Í∑∏Î®ºÌä∏ Î∞òÌôò Ïó¨Î∂Ä
):
    """
    ÏùåÏÑ±ÏùÑ ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò
    
    Parameters:
    -----------
    audio : UploadFile
        Ïò§ÎîîÏò§ ÌååÏùº (webm, mp3, wav, m4a, ogg Îì±)
    language : str
        Ïñ∏Ïñ¥ ÏΩîÎìú (ko=ÌïúÍµ≠Ïñ¥, en=ÏòÅÏñ¥, auto=ÏûêÎèôÍ∞êÏßÄ)
    task : str
        transcribe (Ï†ÑÏÇ¨) ÎòêÎäî translate (ÏòÅÏñ¥Î°ú Î≤àÏó≠)
    use_nuclear_context : bool
        ÏõêÏûêÎ†• Ï†ÑÎ¨∏ Ïö©Ïñ¥ Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© Ïó¨Î∂Ä
    return_segments : bool
        ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Ìè¨Ìï® ÏÉÅÏÑ∏ ÏÑ∏Í∑∏Î®ºÌä∏ Î∞òÌôò Ïó¨Î∂Ä
    
    Returns:
    --------
    {
        "text": str,              # Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏
        "language": str,          # Í∞êÏßÄÎêú Ïñ∏Ïñ¥
        "duration": float,        # Ïò§ÎîîÏò§ Í∏∏Ïù¥ (Ï¥à)
        "segments": List[dict]    # (ÏÑ†ÌÉù) ÏÑ∏Í∑∏Î®ºÌä∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥
    }
    """
    
    if _MODEL is None:
        raise HTTPException(status_code=503, detail="Model not ready")
    
    # Content-Type Í≤ÄÏ¶ù
    if not audio.content_type:
        raise HTTPException(status_code=400, detail="Missing content type")
    
    # ÌååÏùº ÌôïÏû•Ïûê Í≤ÄÏ¶ù
    valid_extensions = {".webm", ".mp3", ".wav", ".m4a", ".ogg", ".flac", ".aac"}
    file_ext = Path(audio.filename or "").suffix.lower()
    
    if file_ext not in valid_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"Unsupported file format: {file_ext}. Supported: {valid_extensions}"
        )
    
    logger.info(f"Received audio: {audio.filename} ({audio.content_type})")
    
    tmp_path = None
    try:
        # ÏûÑÏãú ÌååÏùºÎ°ú Ï†ÄÏû•
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
            content = await audio.read()
            tmp.write(content)
            tmp_path = tmp.name
        
        logger.info(f"Saved to temp: {tmp_path}")
        
        # Faster-Whisper Î≥ÄÌôò ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï
        transcribe_kwargs = {
            "language": None if language == "auto" else language,
            "task": task,
            "beam_size": BEAM_SIZE,
            "vad_filter": VAD_FILTER,
        }
        
        # VAD ÌååÎùºÎØ∏ÌÑ∞ (Î¨µÏùå Ï†úÍ±∞)
        if VAD_FILTER:
            transcribe_kwargs["vad_parameters"] = dict(
                min_silence_duration_ms=500,  # 0.5Ï¥à Ïù¥ÏÉÅ Î¨µÏùåÎßå Ï†úÍ±∞
                speech_pad_ms=400,  # ÏùåÏÑ± ÏïûÎí§ Ìå®Îî©
            )
        
        # ÏõêÏûêÎ†• Ïª®ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä
        if use_nuclear_context:
            transcribe_kwargs["initial_prompt"] = NUCLEAR_PROMPT
            transcribe_kwargs["hotwords"] = " ".join(NUCLEAR_GLOSSARY)
        
        logger.info(f"Transcribing with params: {transcribe_kwargs}")
        
        # Î≥ÄÌôò Ïã§Ìñâ
        segments, info = _MODEL.transcribe(tmp_path, **transcribe_kwargs)
        
        # Í≤∞Í≥º ÏàòÏßë
        full_text_parts = []
        segment_list = []
        
        for seg in segments:
            text_clean = seg.text.strip()
            full_text_parts.append(text_clean)
            
            if return_segments:
                segment_list.append({
                    "start": round(seg.start, 2),
                    "end": round(seg.end, 2),
                    "text": text_clean,
                })
        
        result_text = " ".join(full_text_parts).strip()
        
        logger.info(f"Transcription complete: {len(segment_list)} segments, "
                   f"language={info.language}, duration={info.duration:.1f}s")
        logger.info(f"Result preview: {result_text[:100]}...")
        
        response_data = {
            "text": result_text,
            "language": info.language,
            "duration": round(info.duration, 2),
        }
        
        if return_segments:
            response_data["segments"] = segment_list
        
        return JSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"Transcription failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")
        
    finally:
        # ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
                logger.info(f"Cleaned up temp file: {tmp_path}")
            except Exception as e:
                logger.warning(f"Failed to clean temp file: {e}")


@app.post("/batch-transcribe")
async def batch_transcribe(
    audios: List[UploadFile] = File(...),
    language: str = Form("ko"),
    use_nuclear_context: bool = Form(True),
):
    """
    Ïó¨Îü¨ Ïò§ÎîîÏò§ ÌååÏùºÏùÑ ÏùºÍ¥Ñ Î≥ÄÌôò
    
    Parameters:
    -----------
    audios : List[UploadFile]
        Ïò§ÎîîÏò§ ÌååÏùº Î™©Î°ù
    language : str
        Ïñ∏Ïñ¥ ÏΩîÎìú
    use_nuclear_context : bool
        ÏõêÏûêÎ†• Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö© Ïó¨Î∂Ä
    
    Returns:
    --------
    List[dict]: Í∞Å ÌååÏùºÏùò Î≥ÄÌôò Í≤∞Í≥º
    """
    
    if _MODEL is None:
        raise HTTPException(status_code=503, detail="Model not ready")
    
    if len(audios) > 10:
        raise HTTPException(status_code=400, detail="Maximum 10 files per batch")
    
    results = []
    
    for idx, audio in enumerate(audios):
        logger.info(f"Processing batch {idx+1}/{len(audios)}: {audio.filename}")
        
        try:
            result = await transcribe_audio(
                audio=audio,
                language=language,
                task="transcribe",
                use_nuclear_context=use_nuclear_context,
                return_segments=False
            )
            
            results.append({
                "filename": audio.filename,
                "status": "success",
                "data": result
            })
            
        except Exception as e:
            logger.error(f"Failed to process {audio.filename}: {e}")
            results.append({
                "filename": audio.filename,
                "status": "error",
                "error": str(e)
            })
    
    return JSONResponse(content={"results": results})


@app.get("/models")
async def list_models():
    """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Ï†ïÎ≥¥"""
    return {
        "current_model": MODEL_SIZE,
        "available_models": ["tiny", "base", "small", "medium", "large-v3"],
        "device": DEVICE,
        "compute_type": COMPUTE_TYPE
    }


if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "8000"))
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )