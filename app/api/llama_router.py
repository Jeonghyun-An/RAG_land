# app/api/llama_router.py
from __future__ import annotations

import mimetypes
import hashlib, tempfile
import os, re
import uuid
from urllib.parse import unquote, quote
from typing import List, Optional
from starlette.responses import StreamingResponse

from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks, Query
from pydantic import BaseModel
import asyncio, json
from sse_starlette.sse import EventSourceResponse  # âœ… ìš”êµ¬ì‚¬í•­: sse-starlette
from app.services import job_state, milvus_store
from datetime import datetime, timedelta, timezone

from app.services.file_parser import (
    parse_pdf,                    # (local path) -> [(page_no, text)]
    parse_pdf_blocks,             # (local path) -> [(page_no, [ {text,bbox}, ... ])]
    parse_any_bytes,              # (filename, bytes) -> {"kind":"pdf", "pages":[...], "blocks":[...]}
    parse_pdf_blocks_from_bytes,  # (bytes) -> [(page_no, [ {text,bbox}, ... ])]
)
from app.services.llama_model import generate_answer_unified
from app.services.minio_store import MinIOStore
from app.services.pdf_converter import convert_stream_to_pdf_bytes, convert_to_pdf,convert_bytes_to_pdf_bytes, ConvertError
from app.services.milvus_store_v2 import MilvusStoreV2
from app.services.embedding_model import get_embedding_model, embed
from app.services.reranker import rerank

router = APIRouter(tags=["llama"])

UPLOAD_DIR = "data"
os.makedirs(UPLOAD_DIR, exist_ok=True)
# ---------- Schemas ----------
class GenerateReq(BaseModel):
    prompt: str
    model_name: str = "llama-3.2-3b"

class AskReq(BaseModel):
    question: str
    model_name: str = "llama-3.2-3b"
    top_k: int = 3

class UploadResp(BaseModel):
    filename: str
    minio_object: str
    indexed: str  # "background"
    job_id: Optional[str] = None

class AskResp(BaseModel):
    answer: str
    used_chunks: int
    sources: Optional[List[dict]] = None  # (ì„ íƒ) ì¶œì²˜ ì œê³µ

# --- í´ë°± ì „ìš©: pages ì •ê·œí™” ë„ìš°ë¯¸ ---------------------------------
def _normalize_pages_for_chunkers(pages):
    """
    pagesë¥¼ [(page_no:int, text:str), ...] ë¡œ ê°•ì œ ë³€í™˜.
    í—ˆìš© ì…ë ¥:
      - [(int, str)], [[int, str]]
      - ["page text", ...]  -> enumerate 1-based
      - [{"page":..,"text":..}], [{"page_no":..,"body":..}], [{"index":..,"lines":[..]}]
    ê·¸ ì™¸ëŠ” ë¬¸ìì—´í™”í•´ì„œ ì•ˆì „í•˜ê²Œ ìˆ˜ìš©.
    """
    out = []
    if not pages:
        return out

    for i, item in enumerate(pages, start=1):
        # (int,str) íŠœí”Œ/ë¦¬ìŠ¤íŠ¸
        if isinstance(item, (tuple, list)):
            if len(item) >= 2:
                pno, txt = item[0], item[1]
            else:
                pno, txt = i, (item[0] if item else "")
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, "" if txt is None else str(txt)))
            continue

        # dict
        if isinstance(item, dict):
            pno = item.get("page") or item.get("page_no") or item.get("index") or i
            txt = (
                item.get("text")
                or item.get("body")
                or ("\n".join(item.get("lines") or []) if item.get("lines") else "")
                or ""
            )
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, str(txt)))
            continue

        # ë¬¸ìì—´
        if isinstance(item, str):
            out.append((i, item))
            continue

        # ê¸°íƒ€: ë¬¸ìì—´í™”
        out.append((i, str(item)))

    return out

# ---------- Helpers ----------
def _coerce_chunks_for_milvus(chs):
    """
    (í…ìŠ¤íŠ¸, ë©”íƒ€) ë¦¬ìŠ¤íŠ¸ë¥¼ Milvus insert í˜•íƒœë¡œ ì •ê·œí™”:
    - ë©”íƒ€ íƒ€ì… ë³´ì •(dict ê°•ì œ), page=int, section<=512ì
    - ë‹¤ì¤‘ í˜ì´ì§€ ì§€ì›: meta.pagesê°€ ìˆìœ¼ë©´ pageëŠ” ì²« í˜ì´ì§€ë¡œ
    - ë¹ˆ í…ìŠ¤íŠ¸/ì—°ì† ì¤‘ë³µ ì œê±°
    """
    safe = []
    for t in chs or []:
        if not isinstance(t, (list, tuple)) or len(t) < 2:
            continue
        text, meta = t[0], t[1]
        text = "" if text is None else str(text)
        if not isinstance(meta, dict):
            meta = {}

        # section ìš°ì„  ê²°ì •
        section = str(meta.get("section", ""))[:512]
        # page ì •ê·œí™”: pagesê°€ ìˆìœ¼ë©´ ì²« í˜ì´ì§€
        pages = meta.get("pages")
        if isinstance(pages, (list, tuple)) and len(pages) > 0:
            try:
                page = int(pages[0])
            except Exception:
                page = int(meta.get("page", 0))
        else:
            try:
                page = int(meta.get("page", 0))
            except Exception:
                page = 0

        safe.append((text, {"page": page, "section": section, "pages": pages or [], "bboxes": meta.get("bboxes", {})}))

    out = []
    last = None
    for it in safe:
        if it[0] and it != last:
            out.append(it)
            last = it
    return out

def _make_encoder():
    m = get_embedding_model()
    tok = getattr(m, "tokenizer", None)
    max_len = int(getattr(m, "max_seq_length", 128))
    def enc(s: str):
        if tok is None:
            return []
        return tok.encode(s, add_special_tokens=False) or []
    return enc, max_len

def _sha256_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()

def meta_key(doc_id: str) -> str:
    return f"uploaded/__meta__/{doc_id}/meta.json"

def legacy_meta_key(doc_id: str) -> str:
    return f"uploaded/__meta__/{doc_id}.json"

def index_pdf_to_milvus(
    job_id: str,
    file_path: str | None = None,
    minio_object: str | None = None,
    uploaded: bool = True,
    remove_local: bool = True,
    doc_id: str | None = None,
) -> None:
    """
    ì—…ë¡œë“œëœ(ë˜ëŠ” MinIO ìƒì˜) PDFë¥¼ íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ Milvus upsert.
    - bytes ê²½ë¡œ(minio) ìš°ì„  ì‚¬ìš© (RAG_NO_LOCAL=1 ì´ê±°ë‚˜ file_pathê°€ None)
    - í˜ì´ì§€ í…ìŠ¤íŠ¸ê°€ ë¹ˆì•½í•˜ë©´ bytes-OCR í´ë°± ìˆ˜í–‰(ENV: OCR_MODE != off)
    - ë ˆì´ì•„ì›ƒ ì²­í‚¹ ì‹¤íŒ¨ ì‹œ smart/ê¸°ë³¸ ì²­í‚¹ ìˆœì°¨ í´ë°±
    - ê·¸ë˜ë„ 0ì²­í¬ë©´ ìµœí›„ ë³´í˜¸ë§‰: í†µì§œ í…ìŠ¤íŠ¸ë¡œ 1ê°œ ì´ìƒ ì²­í¬ ìƒì„±
    """
    try:
        job_state.update(job_id, status="parsing", step="parse_pdf:start")
        print(f"[INDEX] start: {file_path or minio_object}")

        NO_LOCAL = os.getenv("RAG_NO_LOCAL", "0") == "1"
        SKIP_IF_ALREADY_UPLOADED = os.getenv("RAG_SKIP_IF_UPLOADED", "1") == "1"

        if not uploaded and SKIP_IF_ALREADY_UPLOADED:
            job_state.update(job_id, status="done", step="skipped:already_uploaded", progress=100)
            print(f"[INDEX] skip: uploaded=False (already uploaded), job_id={job_id}")
            return

        # ---------- 1) PDF â†’ í…ìŠ¤íŠ¸/ë ˆì´ì•„ì›ƒ ----------
        pages: list | None = None          # í˜ì´ì§€ í…ìŠ¤íŠ¸ (ë¬¸ìì—´/íŠœí”Œ í˜¼ì¬ ê°€ëŠ¥ â†’ ì•„ë˜ì„œ í‘œì¤€í™”)
        layout_map: dict[int, list[dict]] = {}  # {page_no: [ {text, bbox}, ... ]}
        pdf_bytes: bytes | None = None

        use_bytes_path = (NO_LOCAL or file_path is None) and bool(minio_object)
        if use_bytes_path:
            # MinIO â†’ bytes
            from app.services.minio_store import MinIOStore
            mstore = MinIOStore()
            pdf_bytes = mstore.get_bytes(minio_object)

            # bytes íŒŒì„œ
            try:
                from app.services.file_parser import parse_any_bytes, parse_pdf_blocks_from_bytes
                parsed = parse_any_bytes(os.path.basename(minio_object), pdf_bytes)
                if parsed.get("kind") != "pdf":
                    raise RuntimeError("PDF íŒŒì´í”„ë¼ì¸ë§Œ ì¸ë±ì‹±í•©ë‹ˆë‹¤. (ë³€í™˜ ë‹¨ê³„ í™•ì¸)")

                pages = parsed.get("pages") or []  # ë¦¬ìŠ¤íŠ¸(ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” í˜¼ì¬)
                blocks_by_page_list = parsed.get("blocks")

                # blocks í‘œì¤€í™” â†’ layout_map
                if isinstance(blocks_by_page_list, dict):
                    layout_map = {int(k): v for k, v in blocks_by_page_list.items()}
                else:
                    layout_map = {int(p): blks for p, blks in (blocks_by_page_list or [])}
            except Exception as ee:
                raise RuntimeError(f"bytes parsing unavailable or failed: {ee}") from ee
        else:
            # ë¡œì»¬ ê²½ë¡œ íŒŒì„œ
            from app.services.file_parser import parse_pdf, parse_pdf_blocks
            pages = parse_pdf(file_path, by_page=True)
            if not pages:
                raise RuntimeError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            blocks_by_page_list = parse_pdf_blocks(file_path)
            layout_map = {int(p): blks for p, blks in (blocks_by_page_list or [])}

        # ---------- 1-1) bytes-OCR í´ë°± (í˜ì´ì§€ í…ìŠ¤íŠ¸ ë°€ë„ ë‚®ì„ ë•Œ) ----------
        def _count_chars(pages_like) -> int:
            total = 0
            for it in (pages_like or []):
                if isinstance(it, (list, tuple)) and len(it) >= 2:
                    total += len(str(it[1] or ""))
                elif isinstance(it, dict):
                    total += len(str(it.get("text", "")))
                elif isinstance(it, str):
                    total += len(it)
            return total

        # OCR í™œì„± ì¡°ê±´
        OCR_BYTES_ENABLED = (os.getenv("OCR_MODE", "auto").lower() != "off")
        MIN_TOTAL = int(os.getenv("OCR_MIN_CHARS_TOTAL", "120"))

        if OCR_BYTES_ENABLED and use_bytes_path:
            total_chars = _count_chars(pages)
            if total_chars < MIN_TOTAL:
                try:
                    # ì§€ì—° ì„í¬íŠ¸(ì´ íŒŒì¼ ìƒë‹¨ì— import ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ NameError ë°©ì§€)
                    from app.services.ocr_service import try_ocr_pdf_bytes
                    ocr_text = _get_clean_ocr_text(pdf_bytes)
                except Exception as _e:
                    print(f"[OCR] bytes fallback failed: {getattr(_e, 'message', _e)}")
                    ocr_text = None
                if ocr_text:
                    print(f"[OCR] bytes fallback used, chars={len(ocr_text)}")
                    pages = [(1, ocr_text)]        # ë‹¨ì¼ í˜ì´ì§€ ì·¨ê¸‰
                    layout_map = {}                # OCR í…ìŠ¤íŠ¸ì—” bbox ì‹ ë¢° ë¶ˆê°€ â†’ ë¹„ì›€

        # ---------- 1-2) í˜ì´ì§€ í‘œì¤€í™” ----------
        pages_std = _normalize_pages_for_chunkers(pages)
        if not any((t or "").strip() for _, t in pages_std):
            print("[PARSE] no textual content after parsing/OCR; will use fallback if chunkers return empty")
        job_state.update(job_id, status="parsing", step="parse_pdf:done", progress=25)

        # ---------- 2) ê³ ë„í™”ëœ ì²­í‚¹ ì‹œìŠ¤í…œ ----------
        job_state.update(job_id, status="chunking", step="chunk:start", progress=35)

        # ì¸ì½”ë”/ê¸¸ì´
        enc, max_len = _make_encoder()
        default_target = max(64, max_len - 16)
        default_overlap = min(96, default_target // 3)
        target_tokens = int(os.getenv("RAG_CHUNK_TOKENS", str(default_target)))
        overlap_tokens = int(os.getenv("RAG_CHUNK_OVERLAP", str(default_overlap)))
        min_chunk_tokens = int(os.getenv("RAG_MIN_CHUNK_TOKENS", "100"))

        chunks: list[tuple[str, dict]] | None = None
        
        print(f"[CHUNK] Starting advanced chunking - target_tokens={target_tokens}, pages={len(pages_std)}")
        
        # 2-0) ì›ìë ¥ ë²•ë ¹/ë§¤ë‰´ì–¼ ì „ìš© ì²­ì»¤ (ìµœìš°ì„ ) ìƒˆë¡œìš´ ê³ ë„í™” ì²­ì»¤
        try:
            from app.services.law_chunker import law_chunk_pages
            print("[CHUNK] Trying nuclear legal chunker...")
            
            law_chunks = law_chunk_pages(
                pages_std, enc,
                target_tokens=target_tokens, 
                overlap_tokens=overlap_tokens,
                layout_blocks=layout_map,
                min_chunk_tokens=min_chunk_tokens,
            )
            
            if law_chunks:
                chunks = law_chunks
                print(f"[CHUNK] Nuclear legal chunker succeeded: {len(law_chunks)} chunks")
            else:
                print("[CHUNK] Nuclear legal chunker returned empty - not legal document")
                
        except ImportError as ie:
            print(f"[CHUNK] law_chunker not available: {ie}")
        except Exception as e0:
            print(f"[CHUNK] law_chunker failed: {e0}")

        # 2-1) ë ˆì´ì•„ì›ƒ ì¸ì§€ ì²­í‚¹ (ê³ ë„í™”ëœ ë²„ì „) 
        if not chunks:
            try:
                from app.services.layout_chunker import layout_aware_chunks
                print("[CHUNK] Trying advanced layout-aware chunker...")
                
                chunks = layout_aware_chunks(
                    pages_std, enc, target_tokens, overlap_tokens,
                    slide_rows=4, layout_blocks=layout_map
                )
                
                if chunks:
                    print(f"[CHUNK] Layout-aware chunker succeeded: {len(chunks)} chunks")
                else:
                    print("[CHUNK] Layout-aware chunker returned empty")
                    
            except ImportError as ie:
                print(f"[CHUNK] layout_chunker not available: {ie}")
            except Exception as e1:
                print(f"[CHUNK] layout_aware_chunks failed: {e1}")

        # 2-2) ìŠ¤ë§ˆíŠ¸ ì²­ì»¤ í”ŒëŸ¬ìŠ¤ (ë ˆì´ì•„ì›ƒ ì •ë³´ í™œìš©) 
        if not chunks:
            try:
                from app.services.chunker import smart_chunk_pages_plus
                print("[CHUNK] Trying smart chunker plus...")
                
                chunks = smart_chunk_pages_plus(
                    pages_std, enc,
                    target_tokens=target_tokens, 
                    overlap_tokens=overlap_tokens,
                    layout_blocks=layout_map
                )
                
                if chunks:
                    print(f"[CHUNK] Smart chunker plus succeeded: {len(chunks)} chunks")
                else:
                    print("[CHUNK] Smart chunker plus returned empty")
                    
            except Exception as e2:
                print(f"[CHUNK] smart_chunk_pages_plus failed: {e2}")

        # 2-3) ê¸°ë³¸ ìŠ¤ë§ˆíŠ¸ ì²­ì»¤ (í´ë°±)
        if not chunks:
            try:
                from app.services.chunker import smart_chunk_pages
                print("[CHUNK] Falling back to basic smart chunker...")
                
                chunks = smart_chunk_pages(
                    pages_std, enc,
                    target_tokens=target_tokens, 
                    overlap_tokens=overlap_tokens
                )
                
                if chunks:
                    print(f"[CHUNK] Basic smart chunker succeeded: {len(chunks)} chunks")
                else:
                    print("[CHUNK] All chunkers failed - will use fallback protection")
                    
            except Exception as e3:
                print(f"[CHUNK] smart_chunk_pages failed: {e3}")

        # 2-4) bytes-OCR ì„¸ì»¨ë“œ ì°¬ìŠ¤ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        if (not chunks or len(chunks) == 0) and use_bytes_path and OCR_BYTES_ENABLED:
            if pdf_bytes:
                try:
                    from app.services.ocr_service import try_ocr_pdf_bytes
                    print("[CHUNK] Trying OCR second chance...")
                    ocr_text2 = try_ocr_pdf_bytes(pdf_bytes, enabled=True)
                except Exception as _e:
                    print(f"[CHUNK] second-chance OCR failed: {getattr(_e, 'message', _e)}")
                    ocr_text2 = None

                if ocr_text2:
                    pages_std = _normalize_pages_for_chunkers([(1, ocr_text2)])
                    from app.services.chunker import smart_chunk_pages
                    chunks = smart_chunk_pages(
                        pages_std, enc,
                        target_tokens=target_tokens, 
                        overlap_tokens=overlap_tokens
                    )
                    print(f"[CHUNK] OCR second-chance succeeded: {len(chunks or [])} chunks")

        # 2-5) ìµœí›„ ë³´í˜¸ë§‰ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€í•˜ë˜ í…ìŠ¤íŠ¸ ì •ë¦¬ ê°•í™”)
        if not chunks or len(chunks) == 0:
            print("[CHUNK] All chunkers failed - using fallback protection")
            
            flat_texts = []
            for _, t in pages_std or []:
                tt = (t or "").strip()
                if tt:
                    # ì´ìƒí•œ ë¼ë²¨ ì œê±°
                    tt = re.sub(r'\bì¸ì ‘í–‰\s*ë¬¶ìŒ\b', '', tt)
                    tt = re.sub(r'\b[ê°€-í£]*\s*ë¬¶ìŒ\b', '', tt)  
                    tt = re.sub(r'[\r\n\s]+', ' ', tt)  # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
                    if tt.strip():
                        flat_texts.append(tt.strip())

            fallback_text = "\n\n".join(flat_texts).strip()
            
            if not fallback_text:
                # ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
                try:
                    if pdf_bytes:
                        import fitz
                        doc_dbg = fitz.open(stream=pdf_bytes, filetype="pdf")
                        placeholders = [f"[page {i+1}: image or low-text content]" for i in range(doc_dbg.page_count)]
                        fallback_text = "\n".join(placeholders).strip()
                except Exception:
                    pass

            # ìµœì¢… í´ë°±
            if not fallback_text:
                if os.getenv("RAG_ALLOW_EMPTY_FALLBACK", "1") == "1":
                    fallback_text = "[Document processed but no readable text content found]"
                else:
                    raise RuntimeError("ëª¨ë“  ì²­í‚¹ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

            # ë©”íƒ€ êµ¬ì„± (ê°œì„ ëœ ë²„ì „)
            import json
            meta = {
                "type": "emergency_fallback",
                "section": "ë¬¸ì„œ ì „ì²´ (ë¹„ìƒ ëª¨ë“œ)",
                "pages": [int(p) for p, _ in (pages_std or [])] or [1],
                "token_count": len(enc(fallback_text)) if fallback_text else 0,
                "bboxes": {},
                "note": "Advanced chunking failed, using emergency fallback"
            }
            
            meta_line = "META: " + json.dumps(meta, ensure_ascii=False)
            chunk_text = meta_line + "\n" + fallback_text
            first_page = int(meta["pages"][0]) if meta["pages"] else 0
            
            chunks = [(chunk_text, {
                "page": first_page, 
                "section": meta["section"], 
                "pages": meta["pages"], 
                "bboxes": meta["bboxes"],
                "token_count": meta["token_count"],
                "type": meta["type"]
            })]

        # 2-6) ìµœì¢… ê²€ì‚¬ ë° ì •ë¦¬ (ê°•í™”ëœ ë²„ì „)
        if chunks:
            print(f"[CHUNK] Pre-cleanup: {len(chunks)} chunks")
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬ ê°•í™”
            cleaned_chunks = []
            for chunk_text, chunk_meta in chunks:
                # "ì¸ì ‘í–‰ ë¬¶ìŒ" ë“± ì´ìƒí•œ ë¼ë²¨ ì œê±°
                clean_text = re.sub(r'\bì¸ì ‘í–‰\s*ë¬¶ìŒ\b', '', chunk_text)
                clean_text = re.sub(r'\b[ê°€-í£]*í–‰\s*ë¬¶ìŒ\b', '', clean_text)  
                clean_text = re.sub(r'\b\w*\s*ë¬¶ìŒ\b', '', clean_text)
                
                # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
                clean_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', clean_text)
                clean_text = re.sub(r'[ \t]+', ' ', clean_text)
                
                # ë¹ˆ ì²­í¬ ê±´ë„ˆë›°ê¸°
                if clean_text.strip() and len(clean_text.strip()) > 10:
                    cleaned_chunks.append((clean_text.strip(), chunk_meta))
            
            chunks = cleaned_chunks
            print(f"[CHUNK] Post-cleanup: {len(chunks)} chunks")

        chunks = _coerce_chunks_for_milvus(chunks)
        if not chunks:
            raise RuntimeError("ìµœì¢… ì²­í‚¹ ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")

        print(f"[CHUNK] Final result: {len(chunks)} chunks ready for embedding")
        job_state.update(job_id, status="chunking", step="chunk:done", chunks=len(chunks), progress=50)

        # ---------- 3) doc_id í™•ì • ----------
        if not doc_id:
            base_from_obj = os.path.splitext(os.path.basename(minio_object or ""))[0] if minio_object else None
            doc_id = base_from_obj or (os.path.splitext(os.path.basename(file_path))[0] if file_path else None)
            if not doc_id:
                import uuid
                doc_id = uuid.uuid4().hex

        REPLACE_BEFORE_INSERT = os.getenv("RAG_REPLACE_BEFORE_INSERT", "0") == "1"
        RETRY_AFTER_DELETE_ON_DUP = os.getenv("RAG_RETRY_AFTER_DELETE", "1") == "1"

        st = job_state.get(job_id) or {}
        mode = st.get("mode")  # 'replace' | 'version' | 'skip' ë“±

        from app.services.milvus_store_v2 import MilvusStoreV2
        from app.services.embedding_model import embed, get_sentence_embedding_dimension
        store = MilvusStoreV2(dim=get_sentence_embedding_dimension())

        if mode == "replace" or REPLACE_BEFORE_INSERT:
            try:
                if hasattr(store, "delete_by_doc_id"):
                    deleted = store.delete_by_doc_id(doc_id)  # type: ignore
                else:
                    deleted = store._delete_by_doc_id(doc_id)  # type: ignore
                print(f"[INDEX] pre-delete for replace: doc_id={doc_id}, deleted={deleted}")
            except Exception as e:
                print(f"[INDEX] pre-delete warn: {e}")

        # ---------- 4) Milvus upsert ----------
        job_state.update(job_id, status="embedding", step="embed:start", progress=60)
        res = store.insert(doc_id, chunks, embed_fn=embed)  # {inserted, skipped, reason, doc_id}
        real_doc_id = res.get("doc_id", doc_id)

        if res.get("skipped") and (mode == "replace" or RETRY_AFTER_DELETE_ON_DUP):
            reason = (res.get("reason") or "").lower()
            if any(k in reason for k in ["duplicate", "exists", "doc_id"]):
                try:
                    if hasattr(store, "delete_by_doc_id"):
                        deleted = store.delete_by_doc_id(real_doc_id)  # type: ignore
                    else:
                        deleted = store._delete_by_doc_id(real_doc_id)  # type: ignore
                    print(f"[INDEX] retry-after-delete: deleted={deleted}, doc_id={real_doc_id}")
                    res = store.insert(doc_id, chunks, embed_fn=embed)
                    real_doc_id = res.get("doc_id", doc_id)
                except Exception as e:
                    print(f"[INDEX] retry-after-delete failed: {e}")

        if res.get("skipped"):
            job_state.update(job_id, status="indexing", step=f"milvus:skipped:{res.get('reason')}",
                             progress=90, doc_id=real_doc_id)
            print(f"[INDEX] skipped: doc_id={real_doc_id}, reason={res.get('reason')}")
        else:
            job_state.update(job_id, status="indexing", step=f"milvus:inserted:{res.get('inserted',0)}",
                             progress=90, doc_id=real_doc_id)
            print(f"[INDEX] done: {minio_object or file_path} (doc_id={real_doc_id}, chunks={len(chunks)}, "
                  f"inserted={res.get('inserted',0)})")

        # ---------- 5) MinIO ì›ë³¸ ì‚­ì œ(ì˜µì…˜) ----------
        if os.getenv("RAG_DELETE_AFTER_INDEX", "0") == "1" and minio_object and uploaded:
            try:
                from app.services.minio_store import MinIOStore
                MinIOStore().delete(minio_object)
                print(f"[CLEANUP] deleted from MinIO: {minio_object}")
                job_state.update(job_id, status="cleanup", step="minio:deleted",
                                 minio_object=minio_object, progress=95)
            except Exception as e:
                print(f"[CLEANUP] delete failed: {e}")
                job_state.update(job_id, status="cleanup", step=f"minio:delete_failed:{e!s}")

        # ---------- 6) ë¡œì»¬ íŒŒì¼ ì •ë¦¬ ----------
        if remove_local and file_path and not use_bytes_path:
            try:
                os.remove(file_path)
            except Exception:
                pass
        # TOTAL ì²­í¬ ìˆ˜ ì§‘ê³„    
        total = len(chunks) if isinstance(chunks, list) else None

        if total is None:
            try:
                total = milvus_store.count_by_doc(doc_id) 
            except Exception:
                total = None
    
        # ë©”íƒ€ ê°±ì‹ 
        try:
            mstore = MinIOStore()
            meta = {}
            try:
                if mstore.exists(meta_key(doc_id)):
                    meta = mstore.get_json(meta_key(doc_id))
                elif mstore.exists(legacy_meta_key(doc_id)):
                    meta = mstore.get_json(legacy_meta_key(doc_id))
            except Exception:
                meta = {}

            meta = dict(meta or {})
            if total is not None:
                meta["chunk_count"] = int(total)
            meta["indexed"] = True
            meta["last_indexed_at"] = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

            mstore.put_json(meta_key(doc_id), meta)
        except Exception as e:
            print(f"[INDEXER] warn: failed to update meta chunk_count: {e}")
    
        # ---------- 7) ì™„ë£Œ ----------
        job_state.complete(
            job_id,
            pages=len(pages_std or []),
            chunks=len(chunks or []),
            doc_id=real_doc_id,
            inserted=int(res.get("inserted", 0)),
            skipped=bool(res.get("skipped", False)),
            reason=res.get("reason"),
        )

    except Exception as e:
        job_state.fail(job_id, str(e))
        raise
def _get_clean_ocr_text(pdf_bytes: bytes) -> str:
    """OCR + ì›Œí„°ë§ˆí¬ ì œê±°"""
    try:
        import fitz
        import easyocr
        from app.services.ocr_service import filter_watermarks
        
        # PyMuPDFë¡œ ë Œë”ë§
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        if doc.page_count == 0:
            return None
            
        # ì²« í˜ì´ì§€ì—ì„œ í¬ê¸° ì •ë³´ íšë“
        page = doc[0]
        page_w, page_h = page.rect.width, page.rect.height
        
        # EasyOCR ì‹¤í–‰
        reader = easyocr.Reader(['ko', 'en'], gpu=False)
        all_results = []
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))
            img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, 3)
            
            results = reader.readtext(img, detail=1)  # bbox, text, confidence
            all_results.append(results)
        
        # ì›Œí„°ë§ˆí¬ í•„í„°ë§
        filtered_results = filter_watermarks(all_results, page_w, page_h)
        
        # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        clean_texts = []
        for page_results in filtered_results:
            page_text = []
            for bbox, text, conf in page_results:
                if conf > 0.5:  # ì‹ ë¢°ë„ í•„í„°
                    page_text.append(text)
            if page_text:
                clean_texts.append('\n'.join(page_text))
        
        return '\n\n'.join(clean_texts)
        
    except Exception as e:
        print(f"[OCR] Clean OCR failed: {e}")
        return None

def _content_disposition(disposition: str, filename: str) -> str:
    """
    latin-1 ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´:
    - ASCII fallback: íŒŒì¼ëª…ì—ì„œ ë¹„ASCIIë¥¼ _ ë¡œ ëŒ€ì²´
    - filename*: UTF-8''<percent-encoded> í•¨ê»˜ ì œê³µ
    """
    # fallback: ASCIIë§Œ ë‚¨ê¸°ê¸°
    ascii_fallback = re.sub(r'[^A-Za-z0-9._-]+', '_', filename) or 'file'
    utf8_quoted = quote(filename)  # UTF-8 percent-encode
    return f"{disposition}; filename=\"{ascii_fallback}\"; filename*=UTF-8''{utf8_quoted}"

def _strip_meta_line(chunk_text: str) -> str:
    """ì²­í¬ ë§¨ ìœ„ META: ë¼ì¸ì„ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ë°˜í™˜"""
    t = chunk_text or ""
    if t.startswith("META:"):
        nl = t.find("\n")
        t = t[nl+1:] if nl != -1 else ""
    return t.strip()

_DEF_PATTS = ("ë­ì•¼", "ë¬´ì—‡", "ë­”ê°€", "ì˜ë¯¸", "ì •ì˜", "ì„¤ëª…", "ì–´ë–¤", "ë¬´ì—‡ì¸ê°€", "ë¬´ì—‡ì¸ì§€")

def normalize_query(q: str) -> str:
    """
    ì •ì˜/ì„¤ëª…í˜• ì§ˆë¬¸ì„ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ë³´ê°•:
    - '... ë­ì•¼/ë¬´ì—‡/ì˜ë¯¸' ë“±ì„ '... ë‚´ìš©'ìœ¼ë¡œ ë³´ê°•
    - ë„ˆë¬´ ê³¼í•˜ê²Œ ë°”ê¾¸ì§€ ì•Šê³  ì›ë¬¸ì„ ìœ ì§€í•˜ë˜ 'ë‚´ìš©', 'ì •ì˜' í† í°ì„ ì¶”ê°€
    """
    base = q.strip()
    lowered = base.lower()
    if any(p in base for p in _DEF_PATTS):
        # í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´ + ë‚´ìš©/ì •ì˜ë¥¼ ë§ë¶™ì—¬ ë²¡í„° ê²€ìƒ‰ ì¹œí™”í™”
        return f"{base} ë‚´ìš© ì •ì˜"
    return base

_KW_TOKEN_RE = re.compile(r"[A-Za-zê°€-í£0-9\.#\-]+")  # '57bí•­', 'Â§57(b)', 'AEA-57b' ë¥˜ ë³´ì¡´

def extract_keywords(q: str) -> list[str]:
    """
    ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ(ì§§ì€ ì¡°ì‚¬ë¥˜/í•œ ê¸€ì í† í° ì œê±°)
    """
    toks = [t for t in _KW_TOKEN_RE.findall(q) if len(t) >= 2]
    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´)
    seen, out = set(), []
    for t in toks:
        tl = t.lower()
        if tl not in seen:
            seen.add(tl); out.append(t)
    return out

# ---------- Routes ----------
@router.get("/test")
def test():
    return {"status": "LLaMA router is working"}

@router.post("/generate")
def generate(body: GenerateReq):
    try:
        result = generate_answer_unified(body.prompt, body.model_name)
        return {"response": result}
    except Exception as e:
        raise HTTPException(500, f"ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")

@router.post("/upload", response_model=UploadResp)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    mode: str = Query("version", regex="^(skip|version|replace)$"),
):
    # 0) ì—…ë¡œë“œ ì›ë³¸ì„ ë©”ëª¨ë¦¬ë¡œ ì½ìŒ
    safe_name = os.path.basename(file.filename or "upload.bin")
    orig_ct = file.content_type or mimetypes.guess_type(safe_name)[0] or "application/octet-stream"
    content = await file.read()
    if not content:
        raise HTTPException(400, "ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤.")

    m = MinIOStore()

    # 1) ë¹„-PDF â†’ PDF bytes ë³€í™˜ (DOC_CONVERTER_URL ìˆìœ¼ë©´ ìŠ¤íŠ¸ë¦¼ ë³€í™˜, ì—†ìœ¼ë©´ ì„ì‹œíŒŒì¼-í´ë°±)
    src_ext = os.path.splitext(safe_name)[1].lower()
    # 1ìˆœìœ„: Gotenberg ë°”ì´íŠ¸ ë³€í™˜ (ì™„ì „ ë¬´-ë””ìŠ¤í¬)
    pdf_bytes: bytes | None = None
    if src_ext == ".pdf":
        pdf_bytes = content
    else:
        pdf_bytes = convert_bytes_to_pdf_bytes(content, src_ext)
    
    # 2ìˆœìœ„: ì‚¬ë‚´/ì™¸ë¶€ ì»¨ë²„í„° (DOC_CONVERTER_URL), ìˆìœ¼ë©´ ì‚¬ìš©
    if pdf_bytes is None and src_ext != ".pdf":
        try:
            pdf_bytes = convert_stream_to_pdf_bytes(content, src_ext)
        except Exception:
            pdf_bytes = None
    
    # 3ìˆœìœ„: ì„ì‹œí´ë” í´ë°± (convert_to_pdf) â†’ ë³€í™˜ í›„ ì¦‰ì‹œ ì‚­ì œ
    if pdf_bytes is None:
        with tempfile.TemporaryDirectory() as td:
            src_path = os.path.join(td, safe_name)
            with open(src_path, "wb") as f:
                f.write(content)
            out_path = convert_to_pdf(src_path)
            with open(out_path, "rb") as f:
                pdf_bytes = f.read()
    # 2) í•´ì‹œ/ì¤‘ë³µíŒì •
    pdf_filename = safe_name if src_ext == ".pdf" else (os.path.splitext(safe_name)[0] + ".pdf")
    pdf_sha = _sha256_bytes(pdf_bytes)
    hash_flag_key = f"uploaded/__hash__/sha256/{pdf_sha}.flag"
    object_pdf = f"uploaded/{pdf_filename}"
    uploaded = True
    duplicate_reason = None

    if m.exists(hash_flag_key):
        uploaded = False
        duplicate_reason = "same_content_hash"

    if uploaded and m.exists(object_pdf):
        try:
            remote_size = m.size(object_pdf)
        except Exception:
            remote_size = -1
        local_size = len(pdf_bytes)
        if remote_size == local_size and remote_size > -1:
            uploaded = False
            duplicate_reason = (duplicate_reason or "same_name_and_size")
        else:
            if mode == "replace":
                m.upload_bytes(pdf_bytes, object_name=object_pdf, content_type="application/pdf", length=len(pdf_bytes))
            else:
                object_pdf = f"uploaded/{uuid.uuid4().hex}_{pdf_filename}"
                m.upload_bytes(pdf_bytes, object_name=object_pdf, content_type="application/pdf", length=len(pdf_bytes))
    elif uploaded:
        m.upload_bytes(pdf_bytes, object_name=object_pdf, content_type="application/pdf", length=len(pdf_bytes))

    # 2-1) í•´ì‹œ í”Œë˜ê·¸(ë°”ì´íŠ¸ ì§ì—…ë¡œë“œ: ë¡œì»¬ íŒŒì¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    try:
        if uploaded and not m.exists(hash_flag_key):
            m.upload_bytes(b"1", object_name=hash_flag_key, content_type="text/plain", length=1)
    except Exception as e:
        # ì¹˜ëª…ì  ì•„ë‹˜
        print(f"[UPLOAD] warn: failed to write hash flag: {e}")

    # 3) doc_id ê²°ì • ë° â€˜ì›ë³¸â€™ ë°”ì´íŠ¸ ì—…ë¡œë“œ (ë¬¸ì„œë³„ í´ë”ë¡œ)
    doc_id = os.path.splitext(os.path.basename(object_pdf))[0]
    object_orig = f"uploaded/originals/{doc_id}/{safe_name}"
    if m.exists(object_orig):
        try:
            rsize = m.size(object_orig)
        except Exception:
            rsize = -1
        if rsize != len(content):
            object_orig = f"uploaded/originals/{doc_id}/{uuid.uuid4().hex}_{safe_name}"

    m.upload_bytes(content, object_name=object_orig, content_type=orig_ct, length=len(content))
    is_pdf_original = (src_ext == ".pdf")
    # 4) ë§¤í•‘ ë©”íƒ€ JSON
    try:
        meta = {
            "doc_id": doc_id,
            "title": safe_name,                # ë³´ê¸°ìš©
            "pdf_key": object_pdf,             # â† í‚¤ ì´ë¦„ì„ pdf_keyë¡œ í†µì¼
            "original_key": object_orig,       # â† original_key í†µì¼
            "original_name": safe_name,
            "is_pdf_original": is_pdf_original,
            "sha256": pdf_sha,
            # ì—…ë¡œë“œ ì‹œê°„ì€ UTCë¡œ ì €ì¥(í”„ëŸ°íŠ¸ì—ì„œ KSTë¡œ ë Œë” ì¶”ì²œ)
            "uploaded_at": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
            "mode": mode,
            # ì•„ì§ ëª¨ë¦„ â†’ ë‚˜ì¤‘ì— ì¸ë±ì„œê°€ ë®ì–´ì”€
            # "chunk_count": null
        }
        m.put_json(meta_key(doc_id), meta)
    except Exception as e:
        print(f"[UPLOAD] warn: failed to write meta json: {e}")

    # 5) ë°±ê·¸ë¼ìš´ë“œ ì¸ë±ì‹± (MinIO bytes ê²½ë¡œë§Œ ë„˜ê¹€)
    job_id = uuid.uuid4().hex
    job_state.start(job_id, doc_id=doc_id, minio_object=object_pdf)
    job_state.update(
        job_id,
        status="uploaded",
        step="minio:ok",
        filename=safe_name,
        progress=10,
        mode=mode,
        content_sha256=pdf_sha,
        duplicate_reason=duplicate_reason,
        uploaded=uploaded,
    )
    background_tasks.add_task(index_pdf_to_milvus, job_id, None, object_pdf, uploaded, False, doc_id)

    return UploadResp(filename=safe_name, minio_object=object_pdf, indexed="background", job_id=job_id)

@router.post("/ask", response_model=AskResp)
def ask_question(req: AskReq):
    try:
        # 0) ëª¨ë¸/ìŠ¤í† ì–´ ì¤€ë¹„
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())

        # 1) ì§ˆë¬¸ ì „ì²˜ë¦¬(ì¿¼ë¦¬ ë³´ê°•) + ì´ˆê¸° ë„‰ë„‰íˆ ê²€ìƒ‰
        query_for_search = normalize_query(req.question)
        raw_topk = max(20, req.top_k * 5)
        cands = store.search(query_for_search, embed_fn=embed, topk=raw_topk)

        if not cands:
            return AskResp(
                answer="ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œê°€ ì˜¬ë°”ë¥´ê²Œ ì¸ë±ì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
                used_chunks=0,
                sources=[]
            )

        # 2) í‚¤ì›Œë“œ ë¶€ìŠ¤íŠ¸
        kws = extract_keywords(req.question)
        def _kw_boost_score(c: dict) -> int:
            txt = _strip_meta_line(c.get("chunk", "")).lower()
            return sum(1 for k in kws if k.lower() in txt)
        for c in cands:
            c["kw_boost"] = _kw_boost_score(c)

        ARTICLE_BOOST = float(os.getenv("RAG_ARTICLE_BOOST", "2.5"))
        m = re.search(r"ì œ\s*(\d+)\s*ì¡°", req.question)
        if m:
            art = m.group(1)
            patt = re.compile(rf"ì œ\s*{art}\s*ì¡°")
            for c in cands:
                sec = c.get("section") or ""
                txt = c.get("chunk") or ""
                if patt.search(sec) or patt.search(txt):
                    c["kw_boost"] = c.get("kw_boost", 0.0) + ARTICLE_BOOST

        cands.sort(key=lambda x: (x.get("kw_boost", 0), x.get("score", 0.0)), reverse=True)

        # 3) ë¦¬ë­í¬
        topk = rerank(req.question, cands, top_k=req.top_k)
        if not topk:
            return AskResp(
                answer="ë¬¸ì„œì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                used_chunks=0,
                sources=[]
            )

        # 4) ìŠ¤ì½”ì–´ ì»·ì˜¤í”„
        THRESH = float(os.getenv("RAG_SCORE_THRESHOLD", "0.3"))
        if "re_score" in topk[0] and topk[0]["re_score"] < THRESH:
            return AskResp(
                answer="ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ í™•ì‹¤í•œ ë‹µë³€ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
                used_chunks=0,
                sources=[]
            )

        # 5) ì»¨í…ìŠ¤íŠ¸/ì¶œì²˜ êµ¬ì„±
        context_lines = []
        sources = []
        for i, c in enumerate(topk, 1):
            sec = (c.get("section") or "").strip()
            chunk_body = _strip_meta_line(c.get("chunk", ""))
            body_only = f"{sec}\n{chunk_body}" if sec and not chunk_body.startswith(sec) else chunk_body
            context_lines.append(f"[{i}] (doc:{c['doc_id']} p.{c['page']})\n{chunk_body}")
            sources.append({
                "id": i,
                "doc_id": c.get("doc_id"),
                "page": c.get("page"),
                "section": c.get("section"),
                "chunk": c.get("chunk"),
                "score": c.get("re_score", c.get("score")),
            })
        context = "\n\n".join(context_lines)

        # 6) í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ì¤‘ìš” ê·œì¹™]
- ë¬¸ì„œì— ëª…í™•í•œ ê·¼ê±°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”
- ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µí•˜ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
- ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{req.question}

[ë‹µë³€ í˜•ì‹]
1. ì •ì˜: [ë¬¸ì„œì—ì„œ ì°¾ì€ ì •ì˜]
2. ì£¼ìš” ë‚´ìš©: [êµ¬ì²´ì  ì ˆì°¨ë‚˜ ê·œì •]
3. ê´€ë ¨ ì¡°í•­: [í•´ë‹¹ë˜ëŠ” ê²½ìš°]

[ë‹µë³€]"""

        answer = generate_answer_unified(prompt, req.model_name)
        answer = _clean_repetitive_answer(answer)
        return AskResp(answer=answer, used_chunks=len(topk), sources=sources)

    except HTTPException:
        raise
    except RuntimeError as milvus_error:
        raise HTTPException(503, f"Milvus ì—°ê²° ëŒ€ê¸°/ê²€ìƒ‰ ì‹¤íŒ¨: {milvus_error}")
    except Exception as e:
        raise HTTPException(500, f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


def _clean_repetitive_answer(answer: str) -> str:
    """ë°˜ë³µë˜ëŠ” ë‹µë³€ íŒ¨í„´ì„ ì •ë¦¬"""
    if not answer:
        return answer
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = answer.split('.')
    unique_sentences = []
    seen_content = set()
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:
            continue
            
        # ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•´ì„œ ì¤‘ë³µ í™•ì¸
        keywords = set(re.findall(r'[ê°€-í£]{2,}|[A-Za-z]{3,}', sentence))
        content_hash = frozenset(keywords)
        
        # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        is_duplicate = False
        for seen in seen_content:
            overlap = len(content_hash & seen)
            similarity = overlap / max(len(content_hash), len(seen)) if content_hash and seen else 0
            if similarity > 0.7:
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_sentences.append(sentence)
            seen_content.add(content_hash)
    
    return '. '.join(unique_sentences[:5]) + '.' if unique_sentences else answer
# ---------- Job State Management ----------
@router.get("/job/{job_id}")
def get_job(job_id: str):
    st = job_state.get(job_id)
    if not st:
        raise HTTPException(404, "í•´ë‹¹ job_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return st

@router.get("/jobs")
def list_jobs(status: Optional[str] = Query(None), limit: int = Query(50, ge=1, le=500)):
    return {"jobs": job_state.list_jobs(status=status, limit=limit)}

@router.get("/doc/{doc_id}")
def doc_status(doc_id: str):
    s = MinIOStore()

    # 1) ë©”íƒ€ ìš°ì„  (ì‹ ê·œ ê²½ë¡œ)
    try:
        if s.exists(meta_key(doc_id)):
            meta = s.get_json(meta_key(doc_id))
        elif s.exists(legacy_meta_key(doc_id)):  # êµ¬ë²„ì „ íŒŒì¼í˜• í´ë°±
            meta = s.get_json(legacy_meta_key(doc_id))
        else:
            meta = None
    except Exception:
        meta = None

    if isinstance(meta, dict):
        # í‚¤ í˜¸í™˜(pdf/pdf_key, original/original_key ë“±) ì²˜ë¦¬
        chunk_count = meta.get("chunk_count")
        if isinstance(chunk_count, int):
            return {"doc_id": doc_id, "chunks": chunk_count, "indexed": chunk_count > 0}

    # 2) í´ë°±: Milvusì—ì„œ ì„¸ê³  ë©”íƒ€ì— ìºì‹œ
    try:
        model = get_embedding_model()
        m = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        total = m.count_by_doc(doc_id)

        # ë©”íƒ€ì— ìºì‹œ
        try:
            meta = (s.get_json(meta_key(doc_id)) if s.exists(meta_key(doc_id)) else {}) or {}
            meta["doc_id"] = doc_id
            meta["chunk_count"] = int(total)
            s.put_json(meta_key(doc_id), meta)
        except Exception:
            pass

        return {"doc_id": doc_id, "chunks": total, "indexed": total > 0}
    except Exception as e:
        raise HTTPException(500, f"doc status ì‹¤íŒ¨: {e}")


# ========== SSE Stream for Job Status =========
@router.get("/job/{job_id}/stream")
async def stream_job(job_id: str):
    async def event_gen():
        last_serialized = None
        while True:
            st = job_state.get(job_id)
            if not st:
                yield {"event": "error", "data": json.dumps({"error": "not found"}, ensure_ascii=False)}
                break

            data = json.dumps(st, ensure_ascii=False)
            if data != last_serialized:
                yield {"event": "update", "data": data}
                last_serialized = data

                if st.get("status") in ("done", "error"):
                    break

            await asyncio.sleep(1)

    return EventSourceResponse(event_gen())

# ---------- MinIO Utilities ----------

    
@router.get("/files")
def list_files(prefix: str = "uploaded/", include_internal: bool = False, only_pdf: bool = False):
    m = MinIOStore()
    try:
        keys = m.list_files(prefix=prefix) 
    except Exception as e:
        raise HTTPException(500, f"MinIO íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # ë‚´ë¶€ ê´€ë¦¬ ì˜¤ë¸Œì íŠ¸ ìˆ¨ê¸°ê¸° (ì›í•˜ë©´ include_internal=Trueë¡œ ë…¸ì¶œ)
    if not include_internal:
        keys = [k for k in keys if not (k.endswith(".flag") or "/__hash__/" in k or "/__meta__/" in k)]

    if only_pdf:
        keys = [k for k in keys if k.lower().endswith(".pdf")]

    return {"files": keys}


@router.get("/file/{object_name:path}")
def get_file_presigned(
    object_name: str,
    minutes: int = Query(60, ge=1, le=7*24*60),
    download_name: Optional[str] = None,
    inline: bool = False,  # trueë©´ inline, falseë©´ attachment
):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    try:
        if inline:
            url = m.presign_view(key, filename=download_name, ttl_seconds=minutes * 60)
        else:
            url = m.presign_download(key, filename=download_name, ttl_seconds=minutes * 60)
        return {"url": url}
    except Exception as e:
        raise HTTPException(500, f"presign failed: {e}")

# ê¸°ì¡´
# @router.get("/docs")
# def list_docs():

@router.get("/rag/docs")
def list_docs():
    m = MinIOStore()
    try:
        all_keys = m.list_files("uploaded/")
    except Exception as e:
        raise HTTPException(500, f"minio list failed: {e}")

    def is_internal(k: str) -> bool:
        return (
            k.endswith(".flag")
            or "/__hash__/" in k
            or "/__meta__/" in k
            or k.startswith("uploaded/originals/")
        )

    pdf_keys = [k for k in all_keys if not is_internal(k) and k.lower().endswith(".pdf")]

    items = []
    for k in pdf_keys:
        base = os.path.basename(k)
        doc_id = os.path.splitext(base)[0]

        meta = None
        try:
            if m.exists(meta_key(doc_id)):
                meta = m.get_json(meta_key(doc_id))
            elif m.exists(legacy_meta_key(doc_id)):
                meta = m.get_json(legacy_meta_key(doc_id))
        except Exception:
            meta = None

        # ë©”íƒ€ì—ì„œ ì›ë³¸ ì •ë³´ êº¼ë‚¼ ë•Œ í‚¤ í˜¸í™˜
        original_key = None
        original_name = None
        uploaded_at = None
        if isinstance(meta, dict):
            original_key = meta.get("original_key") or meta.get("original")
            original_name = meta.get("original_name")
            uploaded_at = meta.get("uploaded_at")

        def _resolve_original() -> Optional[str]:
            if original_key and m.exists(original_key):
                return original_key
            if original_name:
                cand1 = f"uploaded/originals/{doc_id}/{original_name}"
                if m.exists(cand1):
                    return cand1
                cand2 = f"uploaded/originals/{original_name}"
                if m.exists(cand2):
                    return cand2
            cands = m.list_files(f"uploaded/originals/{doc_id}/")
            if cands:
                return cands[0]
            return None

        resolved_orig = _resolve_original()
        if resolved_orig:
            original_key = resolved_orig
            if not original_name:
                original_name = os.path.basename(resolved_orig)

        title = (isinstance(meta, dict) and meta.get("title")) or original_name or base
        is_pdf_original = bool(original_key and original_key.lower().endswith(".pdf"))

        items.append({
            "doc_id": doc_id,
            "title": title,
            "object_key": k,                  # ë³€í™˜ PDF
            "original_key": original_key,     # ì›ë³¸ (ìˆìœ¼ë©´)
            "original_name": original_name,
            "is_pdf_original": is_pdf_original,
            "uploaded_at": uploaded_at,
        })

    return {"docs": items}

@router.get("/rag/meta/{doc_id}")
def get_meta(doc_id: str):
    m = MinIOStore()
    if m.exists(meta_key(doc_id)):
        return m.get_json(meta_key(doc_id))
    if m.exists(legacy_meta_key(doc_id)):
        return m.get_json(legacy_meta_key(doc_id))
    raise HTTPException(404, f"meta not found for {doc_id}")

@router.get("/status")
def status():
    m = MinIOStore()
    try:
        keys = m.list_files("uploaded/")
        pdfs = [k for k in keys if k.lower().endswith(".pdf") and "/__hash__/" not in k and "/__meta__/" not in k]
        return {"has_data": len(pdfs) > 0, "doc_count": len(pdfs)}
    except Exception:
        return {"has_data": False, "doc_count": 0}


@router.delete("/file/{object_name}")
def delete_file(object_name: str):
    try:
        minio = MinIOStore()
        if not minio.exists(object_name):
            raise HTTPException(404, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        minio.delete(object_name)
        return {"status": "ok", "deleted": object_name}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
@router.get("/view/{object_name:path}")
def view_object(object_name: str, name: Optional[str] = None):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    disp_name = name or os.path.basename(key)
    ext = os.path.splitext(key)[1].lower()
    media = "application/pdf" if ext == ".pdf" else (mimetypes.guess_type(disp_name)[0] or "application/octet-stream")

    try:
        obj = m.client.get_object(m.bucket, key)
    except Exception as e:
        raise HTTPException(500, f"MinIO get_object failed: {e}")

    headers = {
        # ğŸ‘‡ latin-1 ì•ˆì „í•˜ê²Œ
        "Content-Disposition": _content_disposition("inline", disp_name)
    }

    def _iter():
        try:
            for chunk in obj.stream(32 * 1024):
                yield chunk
        finally:
            obj.close()
            obj.release_conn()

    return StreamingResponse(_iter(), media_type=media, headers=headers)


@router.get("/download/{object_name:path}")
def download_object(object_name: str, name: Optional[str] = None):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    disp_name = name or os.path.basename(key)
    media = mimetypes.guess_type(disp_name)[0] or "application/octet-stream"

    try:
        obj = m.client.get_object(m.bucket, key)
    except Exception as e:
        raise HTTPException(500, f"MinIO get_object failed: {e}")

    headers = {
        # ğŸ‘‡ latin-1 ì•ˆì „í•˜ê²Œ
        "Content-Disposition": _content_disposition("attachment", disp_name)
    }

    def _iter():
        try:
            for chunk in obj.stream(32 * 1024):
                yield chunk
        finally:
            obj.close()
            obj.release_conn()

    return StreamingResponse(_iter(), media_type=media, headers=headers)

# ---------- Bulk delete MinIO files under a prefix ----------
@router.delete("/files/purge", tags=["llama"])
def purge_files(
    prefix: str = Query("uploaded/", description="ì§€ìš¸ ê²½ë¡œ prefix (ë°˜ë“œì‹œ 'uploaded/'ë¡œ ì‹œì‘)"),
    dry_run: bool = Query(False, description="trueë©´ ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ëª©ë¡ë§Œ ë°˜í™˜"),
    limit_preview: int = Query(50, ge=1, le=500, description="dry_run ë•Œ ë¯¸ë¦¬ë³´ê¸° ìµœëŒ€ ê°œìˆ˜"),
):
    """
    MinIOì—ì„œ íŠ¹ì • prefix í•˜ìœ„ ê°ì²´ë“¤ì„ ì¼ê´„ ì‚­ì œ.
    - ì•ˆì „ì¥ì¹˜: prefixê°€ 'uploaded/'ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ 400 ì—ëŸ¬
    - dry_run=True ë©´ ì‚­ì œ ì—†ì´ ëª©ë¡ ë¯¸ë¦¬ë³´ê¸°ë§Œ
    """
    if not prefix or not prefix.startswith("uploaded/"):
        raise HTTPException(400, "prefixëŠ” ë°˜ë“œì‹œ 'uploaded/'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")

    try:
        minio = MinIOStore()
        files = minio.list_files(prefix=prefix)
    except Exception as e:
        raise HTTPException(500, f"MinIO ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    matched = len(files)
    if dry_run:
        preview = files[:limit_preview]
        more = max(0, matched - len(preview))
        return {"status": "dry-run", "prefix": prefix, "matched": matched, "preview": preview, "more": more}

    deleted = 0
    failed = 0
    errors = []
    for obj in files:
        try:
            minio.delete(obj)
            deleted += 1
        except Exception as e:
            failed += 1
            errors.append({"object": obj, "error": str(e)})

    return {"status": "ok", "prefix": prefix, "matched": matched, "deleted": deleted, "failed": failed, "errors": errors}

# ========= Debug / Inspection =========
@router.get("/debug/milvus/info")
def debug_milvus_info():
    """ Milvus ìƒíƒœ ì •ë³´ ì¡°íšŒ"""
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        return store.stats()
    except Exception as e:
        raise HTTPException(500, f"Milvus info ì¡°íšŒ ì‹¤íŒ¨: {e}")

@router.get("/debug/milvus/peek")
def debug_milvus_peek(limit: int = 100, full: bool = True, max_chars:int|None = None):
    """ Milvus ì»¬ë ‰ì…˜ì˜ ì¼ë¶€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° """
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        if full:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = "0"
        elif max_chars is not None:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = str(max_chars)
        return {"items": store.peek(limit=limit)}
    except Exception as e:
        raise HTTPException(500, f"Milvus peek ì‹¤íŒ¨: {e}")

@router.get("/debug/milvus/by-doc")
def debug_milvus_by_doc(
    doc_id: str,
    limit: int = 100,
    full: bool = False,
    max_chars: int | None = None
):
    items: list = []            # ë¯¸ë¦¬ ì´ˆê¸°í™” (UnboundLocalError ë°©ì§€)
    total: int | None = None

    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())

        # ê¸¸ì´ íŠ¸ë ì¼€ì´ì…˜ ì œì–´
        if full or max_chars == 0:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = "0"
        elif max_chars is not None:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = str(max_chars)

        # ë°ì´í„° ì¡°íšŒ
        items = store.query_by_doc(doc_id=doc_id, limit=limit)

        # ì´ ê°œìˆ˜(ê°€ëŠ¥í•˜ë©´)
        try:
            total = store.count_by_doc(doc_id)
        except Exception:
            total = None

        # í•­ìƒ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆë¡œ ë°˜í™˜
        return {
            "items": items,
            "total": total,
            "limit": limit,
            "doc_id": doc_id,
        }

    except Exception as e:
        # ì—¬ê¸°ì„œëŠ” ë¡œì»¬ ë³€ìˆ˜ ì°¸ì¡° ê¸ˆì§€!
        raise HTTPException(500, f"Milvus by-doc ì‹¤íŒ¨: {e}")

@router.get("/debug/search")
def debug_vector_search(q: str, k: int = 5):
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        raw = store.debug_search(q, embed_fn=embed, topk=k)
        return {"results": raw}
    except Exception as e:
        raise HTTPException(500, f"ë””ë²„ê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
