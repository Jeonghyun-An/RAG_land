# app/api/llama_router.py
from __future__ import annotations

import mimetypes
import hashlib, tempfile
import os, re
import uuid
from urllib.parse import unquote, quote
from typing import List, Optional
from starlette.responses import FileResponse
from starlette.background import BackgroundTask
from starlette.responses import StreamingResponse

from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks, Query
from pydantic import BaseModel
import asyncio, json
from sse_starlette.sse import EventSourceResponse  # âœ… ìš”êµ¬ì‚¬í•­: sse-starlette
from app.services import job_state
from datetime import datetime, timedelta

from app.services.file_parser import (
    parse_pdf,                    # (local path) -> [(page_no, text)]
    parse_pdf_blocks,             # (local path) -> [(page_no, [ {text,bbox}, ... ])]
    parse_any_bytes,              # (filename, bytes) -> {"kind":"pdf", "pages":[...], "blocks":[...]}
    parse_pdf_blocks_from_bytes,  # (bytes) -> [(page_no, [ {text,bbox}, ... ])]
)
from app.services.llama_model import generate_answer_unified
from app.services.minio_store import MinIOStore
from app.services.pdf_converter import convert_to_pdf, ConvertError
from app.services.chunker import smart_chunk_pages, smart_chunk_pages_plus
from app.services.layout_chunker import layout_aware_chunks
from app.services.milvus_store_v2 import MilvusStoreV2
from app.services.embedding_model import get_embedding_model, embed, get_sentence_embedding_dimension
from app.services.reranker import rerank

router = APIRouter(tags=["llama"])

UPLOAD_DIR = "data"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ---------- Schemas ----------
class GenerateReq(BaseModel):
    prompt: str
    model_name: str = "llama-3.2-3b"

class AskReq(BaseModel):
    question: str
    model_name: str = "llama-3.2-3b"
    top_k: int = 3

class UploadResp(BaseModel):
    filename: str
    minio_object: str
    indexed: str  # "background"
    job_id: Optional[str] = None

class AskResp(BaseModel):
    answer: str
    used_chunks: int
    sources: Optional[List[dict]] = None  # (ì„ íƒ) ì¶œì²˜ ì œê³µ

# --- í´ë°± ì „ìš©: pages ì •ê·œí™” ë„ìš°ë¯¸ ---------------------------------
def _normalize_pages_for_chunkers(pages):
    """
    pagesë¥¼ [(page_no:int, text:str), ...] ë¡œ ê°•ì œ ë³€í™˜.
    í—ˆìš© ì…ë ¥:
      - [(int, str)], [[int, str]]
      - ["page text", ...]  -> enumerate 1-based
      - [{"page":..,"text":..}], [{"page_no":..,"body":..}], [{"index":..,"lines":[..]}]
    ê·¸ ì™¸ëŠ” ë¬¸ìì—´í™”í•´ì„œ ì•ˆì „í•˜ê²Œ ìˆ˜ìš©.
    """
    out = []
    if not pages:
        return out

    for i, item in enumerate(pages, start=1):
        # (int,str) íŠœí”Œ/ë¦¬ìŠ¤íŠ¸
        if isinstance(item, (tuple, list)):
            if len(item) >= 2:
                pno, txt = item[0], item[1]
            else:
                pno, txt = i, (item[0] if item else "")
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, "" if txt is None else str(txt)))
            continue

        # dict
        if isinstance(item, dict):
            pno = item.get("page") or item.get("page_no") or item.get("index") or i
            txt = (
                item.get("text")
                or item.get("body")
                or ("\n".join(item.get("lines") or []) if item.get("lines") else "")
                or ""
            )
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, str(txt)))
            continue

        # ë¬¸ìì—´
        if isinstance(item, str):
            out.append((i, item))
            continue

        # ê¸°íƒ€: ë¬¸ìì—´í™”
        out.append((i, str(item)))

    return out

# ---------- Helpers ----------
def _coerce_chunks_for_milvus(chs):
    """
    (í…ìŠ¤íŠ¸, ë©”íƒ€) ë¦¬ìŠ¤íŠ¸ë¥¼ Milvus insert í˜•íƒœë¡œ ì •ê·œí™”:
    - ë©”íƒ€ íƒ€ì… ë³´ì •(dict ê°•ì œ), page=int, section<=512ì
    - ë‹¤ì¤‘ í˜ì´ì§€ ì§€ì›: meta.pagesê°€ ìˆìœ¼ë©´ pageëŠ” ì²« í˜ì´ì§€ë¡œ
    - ë¹ˆ í…ìŠ¤íŠ¸/ì—°ì† ì¤‘ë³µ ì œê±°
    """
    safe = []
    for t in chs or []:
        if not isinstance(t, (list, tuple)) or len(t) < 2:
            continue
        text, meta = t[0], t[1]
        text = "" if text is None else str(text)
        if not isinstance(meta, dict):
            meta = {}

        # section ìš°ì„  ê²°ì •
        section = str(meta.get("section", ""))[:512]
        # page ì •ê·œí™”: pagesê°€ ìˆìœ¼ë©´ ì²« í˜ì´ì§€
        pages = meta.get("pages")
        if isinstance(pages, (list, tuple)) and len(pages) > 0:
            try:
                page = int(pages[0])
            except Exception:
                page = int(meta.get("page", 0))
        else:
            try:
                page = int(meta.get("page", 0))
            except Exception:
                page = 0

        safe.append((text, {"page": page, "section": section, "pages": pages or [], "bboxes": meta.get("bboxes", {})}))

    out = []
    last = None
    for it in safe:
        if it[0] and it != last:
            out.append(it)
            last = it
    return out

def _make_encoder():
    m = get_embedding_model()
    tok = getattr(m, "tokenizer", None)
    max_len = int(getattr(m, "max_seq_length", 128))
    def enc(s: str):
        if tok is None:
            return []
        return tok.encode(s, add_special_tokens=False) or []
    return enc, max_len

def index_pdf_to_milvus(
    job_id: str,
    file_path: str | None = None,
    minio_object: str | None = None,
    uploaded: bool = True,
    remove_local: bool = True,
    doc_id: str | None = None,
) -> None:
    try:
        job_state.update(job_id, status="parsing", step="parse_pdf:start")
        print(f"[INDEX] start: {file_path}")

        NO_LOCAL = os.getenv("RAG_NO_LOCAL", "0") == "1"

        SKIP_IF_ALREADY_UPLOADED = os.getenv("RAG_SKIP_IF_UPLOADED", "1") == "1"
        if not uploaded and SKIP_IF_ALREADY_UPLOADED:
            job_state.update(job_id, status="done", step="skipped:already_uploaded", progress=100)
            print(f"[INDEX] skip: uploaded=False (already uploaded), job_id={job_id}")
            return

        # 1) PDF â†’ í˜ì´ì§€ í…ìŠ¤íŠ¸ (+ ë ˆì´ì•„ì›ƒ ë¸”ë¡)
        pages = None
        layout_map = {}

        use_bytes_path = (NO_LOCAL or file_path is None) and bool(minio_object)
        if use_bytes_path:
            # MinIO â†’ bytes â†’ bytes íŒŒì„œ
            from app.services.minio_store import MinIOStore
            mstore = MinIOStore()
            pdf_bytes = mstore.get_bytes(minio_object)

            try:
                from app.services.file_parser import parse_any_bytes, parse_pdf_blocks_from_bytes
                parsed = parse_any_bytes(os.path.basename(minio_object), pdf_bytes)
                if parsed.get("kind") != "pdf":
                    raise RuntimeError("PDF íŒŒì´í”„ë¼ì¸ë§Œ ì¸ë±ì‹±í•©ë‹ˆë‹¤. (ë³€í™˜ ë‹¨ê³„ í™•ì¸)")
                pages = parsed.get("pages") or []

                # í•µì‹¬: BBoxë¥¼ bytes ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                blocks_by_page_list = parsed.get("blocks")
                if not blocks_by_page_list:
                    # parse_any_bytesê°€ blocksë¥¼ ì•ˆ ì±„ì› ë‹¤ë©´, ì „ìš© í•¨ìˆ˜ë¡œ ë³´ì™„
                    blocks_by_page_list = parse_pdf_blocks_from_bytes(pdf_bytes)

                # blocksê°€ dictë¡œ ì˜¬ ìˆ˜ë„ ìˆê³ (list of tuplesë¡œ ì˜¬ ìˆ˜ë„ ìˆìŒ)
                if isinstance(blocks_by_page_list, dict):
                    layout_map = {int(k): v for k, v in blocks_by_page_list.items()}
                else:
                    layout_map = {int(p): blks for p, blks in (blocks_by_page_list or [])}
            except Exception as ee:
                raise RuntimeError(f"bytes parsing unavailable or failed: {ee}") from ee
        else:
            # ê¸°ì¡´ ë¡œì»¬ ê²½ë¡œ íŒŒì„œ ìœ ì§€
            pages = parse_pdf(file_path, by_page=True)
            if not pages:
                raise RuntimeError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            blocks_by_page_list = parse_pdf_blocks(file_path)
            layout_map = {int(p): blks for p, blks in blocks_by_page_list}  # ğŸ”¹int ìºìŠ¤íŒ…

        # ì—¬ê¸°ì„œ í‘œì¤€í™”: ì´í›„ ëª¨ë“  ì²­í‚¹ í•¨ìˆ˜ëŠ” ì´ ë³€ìˆ˜ë§Œ ì‚¬ìš©
        pages_std = _normalize_pages_for_chunkers(pages)

        job_state.update(job_id, status="parsing", step="parse_pdf:done", progress=25)

        # 2) ì²­í‚¹
        job_state.update(job_id, status="chunking", step="chunk:start", progress=35)
        model = get_embedding_model()
        enc, max_len = _make_encoder()
        default_target = max(64, max_len - 16)
        default_overlap = min(96, default_target // 3)
        target_tokens = int(os.getenv("RAG_CHUNK_TOKENS", str(default_target)))
        overlap_tokens = int(os.getenv("RAG_CHUNK_OVERLAP", str(default_overlap)))

        chunks = None
        try:
            from app.services.layout_chunker import layout_aware_chunks  # type: ignore
            chunks = layout_aware_chunks(
                pages_std, enc, target_tokens, overlap_tokens,
                slide_rows=4, layout_blocks=layout_map
            )
            if not chunks:
                raise RuntimeError("layout-aware ê²°ê³¼ ë¹„ì–´ìˆìŒ")
        except Exception as e1:
            try:
                chunks = smart_chunk_pages_plus(
                    pages_std, enc,
                    target_tokens=target_tokens, overlap_tokens=overlap_tokens,
                    layout_blocks=layout_map
                )
                if not chunks:
                    raise RuntimeError("plus ê²°ê³¼ ë¹„ì–´ìˆìŒ")
            except Exception as e2:
                print(f"[CHUNK] layout-aware/plus failed ({e1}); fallback to smart ({e2})")
                chunks = smart_chunk_pages(
                    pages_std, enc,
                    target_tokens=target_tokens, overlap_tokens=overlap_tokens
                )

        chunks = _coerce_chunks_for_milvus(chunks)
        if not chunks:
            raise RuntimeError("Chunking ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")

        job_state.update(job_id, status="chunking", step="chunk:done", chunks=len(chunks), progress=50)

        # 3) doc_id í™•ì • (ë„˜ê²¨ë°›ì€ ê°’ > MinIO ê°ì²´ëª… > íŒŒì¼ëª…)
        if not doc_id:
            base_from_obj = os.path.splitext(os.path.basename(minio_object or ""))[0] if minio_object else None
            doc_id = base_from_obj or (os.path.splitext(os.path.basename(file_path))[0] if file_path else None)
            if not doc_id:
                import uuid
                doc_id = uuid.uuid4().hex

        REPLACE_BEFORE_INSERT = os.getenv("RAG_REPLACE_BEFORE_INSERT", "0") == "1"
        RETRY_AFTER_DELETE_ON_DUP = os.getenv("RAG_RETRY_AFTER_DELETE", "1") == "1"

        st = job_state.get(job_id) or {}
        mode = st.get("mode")  # 'replace' | 'version' | 'skip' ë“±ì´ ìˆìœ¼ë©´ í™œìš©

        store = MilvusStoreV2(dim=get_sentence_embedding_dimension())

        if mode == "replace" or REPLACE_BEFORE_INSERT:
            try:
                if hasattr(store, "delete_by_doc_id"):
                    deleted = store.delete_by_doc_id(doc_id)  # type: ignore
                else:
                    deleted = store._delete_by_doc_id(doc_id)  # type: ignore
                print(f"[INDEX] pre-delete for replace: doc_id={doc_id}, deleted={deleted}")
            except Exception as e:
                print(f"[INDEX] pre-delete warn: {e}")

        # 4) Milvus upsert
        job_state.update(job_id, status="embedding", step="embed:start", progress=60)
        res = store.insert(doc_id, chunks, embed_fn=embed)  # {inserted, skipped, reason, doc_id}
        real_doc_id = res.get("doc_id", doc_id)

        if res.get("skipped") and (mode == "replace" or RETRY_AFTER_DELETE_ON_DUP):
            reason = (res.get("reason") or "").lower()
            if any(k in reason for k in ["duplicate", "exists", "doc_id"]):
                try:
                    if hasattr(store, "delete_by_doc_id"):
                        deleted = store.delete_by_doc_id(real_doc_id)  # type: ignore
                    else:
                        deleted = store._delete_by_doc_id(real_doc_id)  # type: ignore
                    print(f"[INDEX] retry-after-delete: deleted={deleted}, doc_id={real_doc_id}")
                    res = store.insert(doc_id, chunks, embed_fn=embed)
                    real_doc_id = res.get("doc_id", doc_id)
                except Exception as e:
                    print(f"[INDEX] retry-after-delete failed: {e}")

        if res.get("skipped"):
            job_state.update(job_id, status="indexing", step=f"milvus:skipped:{res.get('reason')}",
                             progress=90, doc_id=real_doc_id)
            print(f"[INDEX] skipped: doc_id={real_doc_id}, reason={res.get('reason')}")
        else:
            job_state.update(job_id, status="indexing", step=f"milvus:inserted:{res.get('inserted',0)}",
                             progress=90, doc_id=real_doc_id)
            print(f"[INDEX] done: {minio_object} (doc_id={real_doc_id}, chunks={len(chunks)}, "
                  f"inserted={res.get('inserted',0)})")

        # 5) MinIO ì›ë³¸ ì‚­ì œ(ì˜µì…˜)
        if os.getenv("RAG_DELETE_AFTER_INDEX", "0") == "1" and minio_object and uploaded:
            try:
                MinIOStore().delete(minio_object)
                print(f"[CLEANUP] deleted from MinIO: {minio_object}")
                job_state.update(job_id, status="cleanup", step="minio:deleted",
                                 minio_object=minio_object, progress=95)
            except Exception as e:
                print(f"[CLEANUP] delete failed: {e}")
                job_state.update(job_id, status="cleanup", step=f"minio:delete_failed:{e!s}")

        # 6) ë¡œì»¬ íŒŒì¼ ì •ë¦¬(ì˜µì…˜) â€” ë¬´ë””ìŠ¤í¬ë©´ ì•„ë¬´ê²ƒë„ ì•ˆ í•¨
        if remove_local and file_path and not use_bytes_path:
            try:
                os.remove(file_path)
            except Exception:
                pass

        # 7) ì™„ë£Œ
        job_state.complete(
            job_id,
            pages=len(pages_std or []),
            chunks=len(chunks or []),
            doc_id=real_doc_id,
            inserted=int(res.get("inserted", 0)),
            skipped=bool(res.get("skipped", False)),
            reason=res.get("reason"),
        )

    except Exception as e:
        job_state.fail(job_id, str(e))
        raise
def _content_disposition(disposition: str, filename: str) -> str:
    """
    latin-1 ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´:
    - ASCII fallback: íŒŒì¼ëª…ì—ì„œ ë¹„ASCIIë¥¼ _ ë¡œ ëŒ€ì²´
    - filename*: UTF-8''<percent-encoded> í•¨ê»˜ ì œê³µ
    """
    # fallback: ASCIIë§Œ ë‚¨ê¸°ê¸°
    ascii_fallback = re.sub(r'[^A-Za-z0-9._-]+', '_', filename) or 'file'
    utf8_quoted = quote(filename)  # UTF-8 percent-encode
    return f"{disposition}; filename=\"{ascii_fallback}\"; filename*=UTF-8''{utf8_quoted}"



def _strip_meta_line(chunk_text: str) -> str:
    """ì²­í¬ ë§¨ ìœ„ META: ë¼ì¸ì„ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ë°˜í™˜"""
    t = chunk_text or ""
    if t.startswith("META:"):
        nl = t.find("\n")
        t = t[nl+1:] if nl != -1 else ""
    return t.strip()

_DEF_PATTS = ("ë­ì•¼", "ë¬´ì—‡", "ë­”ê°€", "ì˜ë¯¸", "ì •ì˜", "ì„¤ëª…", "ì–´ë–¤", "ë¬´ì—‡ì¸ê°€", "ë¬´ì—‡ì¸ì§€")

def normalize_query(q: str) -> str:
    """
    ì •ì˜/ì„¤ëª…í˜• ì§ˆë¬¸ì„ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ë³´ê°•:
    - '... ë­ì•¼/ë¬´ì—‡/ì˜ë¯¸' ë“±ì„ '... ë‚´ìš©'ìœ¼ë¡œ ë³´ê°•
    - ë„ˆë¬´ ê³¼í•˜ê²Œ ë°”ê¾¸ì§€ ì•Šê³  ì›ë¬¸ì„ ìœ ì§€í•˜ë˜ 'ë‚´ìš©', 'ì •ì˜' í† í°ì„ ì¶”ê°€
    """
    base = q.strip()
    lowered = base.lower()
    if any(p in base for p in _DEF_PATTS):
        # í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´ + ë‚´ìš©/ì •ì˜ë¥¼ ë§ë¶™ì—¬ ë²¡í„° ê²€ìƒ‰ ì¹œí™”í™”
        return f"{base} ë‚´ìš© ì •ì˜"
    return base

_KW_TOKEN_RE = re.compile(r"[A-Za-zê°€-í£0-9\.#\-]+")  # '57bí•­', 'Â§57(b)', 'AEA-57b' ë¥˜ ë³´ì¡´

def extract_keywords(q: str) -> list[str]:
    """
    ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ(ì§§ì€ ì¡°ì‚¬ë¥˜/í•œ ê¸€ì í† í° ì œê±°)
    """
    toks = [t for t in _KW_TOKEN_RE.findall(q) if len(t) >= 2]
    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´)
    seen, out = set(), []
    for t in toks:
        tl = t.lower()
        if tl not in seen:
            seen.add(tl); out.append(t)
    return out

# ---------- Routes ----------
@router.get("/test")
def test():
    return {"status": "LLaMA router is working"}

@router.post("/generate")
def generate(body: GenerateReq):
    try:
        result = generate_answer_unified(body.prompt, body.model_name)
        return {"response": result}
    except Exception as e:
        raise HTTPException(500, f"ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")

@router.post("/upload", response_model=UploadResp)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    mode: str = Query("version", regex="^(skip|version|replace)$"),  
):
    # 1) ë¡œì»¬ ì €ì¥
    safe_name = os.path.basename(file.filename)
    local_path = os.path.join(UPLOAD_DIR, safe_name)
    content = await file.read()
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    with open(local_path, "wb") as f:
        f.write(content)

    # 1-1) ì›ë³¸ MinIO ì—…ë¡œë“œ (ì¤‘ë³µ/ì¶©ëŒ ì²˜ë¦¬)
    minio = MinIOStore()
    object_orig = f"uploaded/originals/{safe_name}"
    if minio.exists(object_orig):
        try:
            rsize = minio.size(object_orig)
        except Exception:
            rsize = -1
        lsize = os.path.getsize(local_path)
        if rsize != lsize:
            object_orig = f"uploaded/originals/{uuid.uuid4().hex}_{safe_name}"
    orig_ct = file.content_type or mimetypes.guess_type(local_path)[0] or "application/octet-stream"
    # ì›ë³¸ ì—…ë¡œë“œ (í•­ìƒ ë³´ê´€)
    minio.upload(local_path, object_name=object_orig, content_type=orig_ct)

    # 2) ë¹„-PDFë©´ PDFë¡œ ë³€í™˜
    try:
        pdf_path = convert_to_pdf(local_path)
    except ConvertError as e:
        raise HTTPException(400, f"íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
    except Exception as e:
        raise HTTPException(500, f"íŒŒì¼ ë³€í™˜ ì¤‘ ì˜ˆì™¸: {e}")

    # 2-1) PDF í•´ì‹œ
    def _sha256_file(path: str) -> str:
        h = hashlib.sha256()
        with open(path, "rb") as fp:
            for chunk in iter(lambda: fp.read(1024 * 1024), b""):
                h.update(chunk)
        return h.hexdigest()

    pdf_sha = _sha256_file(pdf_path)
    hash_flag_key = f"uploaded/__hash__/sha256/{pdf_sha}.flag"

    # 3) PDF ì—…ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€ + ì¶©ëŒ ì²˜ë¦¬)
    pdf_name = os.path.basename(pdf_path)
    object_pdf = f"uploaded/{pdf_name}"
    uploaded = True
    duplicate_reason = None

    if minio.exists(hash_flag_key):
        uploaded = False
        duplicate_reason = "same_content_hash"
        print(f"[UPLOAD] dedup by hash: {hash_flag_key}")
        # mode=skip ì´ë©´ ì—¬ê¸°ì„œ ë°”ë¡œ 'ìŠ¤í‚µ'ìœ¼ë¡œ í‘œì‹œ(ìƒ‰ì¸ ë‹¨ê³„ì—ì„œ ì°¸ì¡°)

    # ê¸°ì¡´: ì´ë¦„ ê°™ê³ , ì‚¬ì´ì¦ˆ ê°™ìœ¼ë©´ ìŠ¤í‚µ 
    if uploaded and minio.exists(object_pdf):
        try:
            remote_size = minio.size(object_pdf)
        except Exception:
            remote_size = -1
        local_size = os.path.getsize(pdf_path)

        if remote_size == local_size and remote_size > -1:
            uploaded = False
            duplicate_reason = duplicate_reason or "same_name_and_size"
            print(f"[UPLOAD] dedup hit: {object_pdf} (same name & size)")
        else:
            # ì´ë¦„ì€ ê°™ì§€ë§Œ ì‚¬ì´ì¦ˆ ë‹¤ë¥´ë©´
            if mode == "replace":  # ë™ì¼ í‚¤ë¡œ ë®ì–´ì“°ê¸° (=êµì²´)
                minio.upload(pdf_path, object_name=object_pdf, content_type="application/pdf")
                print(f"[UPLOAD] replaced existing: {object_pdf}")
            else:
                # ê¸°ì¡´ ë™ì‘: ì¶©ëŒ íšŒí”¼ìš© ìƒˆ í‚¤ë¡œ ì €ì¥(ë²„ì „ ê´€ë¦¬)
                object_pdf = f"uploaded/{uuid.uuid4().hex}_{pdf_name}"
                minio.upload(pdf_path, object_name=object_pdf, content_type="application/pdf")
                print(f"[UPLOAD] name match but size differs -> stored as: {object_pdf}")
    elif uploaded:
        # ìµœì´ˆ ì—…ë¡œë“œ
        minio.upload(pdf_path, object_name=object_pdf, content_type="application/pdf")
        print(f"[UPLOAD] stored: {object_pdf}")

    # 3-1) í•´ì‹œ í”Œë˜ê·¸ ê¸°ë¡ (ê¸°ì¡´ ìœ ì§€)
    try:
        if uploaded and not minio.exists(hash_flag_key):
            flag_local = os.path.join(UPLOAD_DIR, "__hashflags__", f"{pdf_sha}.flag")
            os.makedirs(os.path.dirname(flag_local), exist_ok=True)
            with open(flag_local, "wb") as ff:
                ff.write(b"1")
            minio.upload(flag_local, object_name=hash_flag_key, content_type="text/plain")
            print(f"[UPLOAD] hash flag written: {hash_flag_key}")
    except Exception as e:
        print(f"[UPLOAD] warn: failed to write hash flag: {e}")

    # === NEW: PDFâ†”ì›ë³¸ ë§¤í•‘ ë©”íƒ€ ì €ì¥ (sidecar JSON) ===
    try:
        doc_id = os.path.splitext(os.path.basename(object_pdf))[0]
        meta = {
            "pdf": object_pdf,
            "original": object_orig,
            "original_name": safe_name,
            "original_mime": orig_ct,
            "sha256": pdf_sha,
            "uploaded_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
            "mode": mode,
        }
        meta_key = f"uploaded/__meta__/{doc_id}.json"
        minio.put_json(meta_key, meta)
    except Exception as e:
        print(f"[UPLOAD] warn: failed to write meta json: {e}")
        # ë©”íƒ€ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì  ì•„ë‹˜

    # ì¸ë±ì‹±(ë°±ê·¸ë¼ìš´ë“œ) ì´í•˜ ê¸°ì¡´ ìœ ì§€
    doc_id = os.path.splitext(os.path.basename(object_pdf))[0]
    job_id = uuid.uuid4().hex
    job_state.start(job_id, doc_id=doc_id, minio_object=object_pdf)
    job_state.update(
        job_id,
        status="uploaded",
        step="minio:ok",
        filename=safe_name,
        progress=10,
        mode=mode,
        content_sha256=pdf_sha,
        duplicate_reason=duplicate_reason,
        uploaded=uploaded,
    )

    background_tasks.add_task(index_pdf_to_milvus, job_id, pdf_path, object_pdf, uploaded, False, doc_id)

    return UploadResp(filename=safe_name, minio_object=object_pdf, indexed="background", job_id=job_id)

@router.post("/ask", response_model=AskResp)
def ask_question(body: AskReq):
    try:
        # 0) ëª¨ë¸/ìŠ¤í† ì–´ ì¤€ë¹„
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())

        # 1) ì§ˆë¬¸ ì „ì²˜ë¦¬(ì¿¼ë¦¬ ë³´ê°•) + ì´ˆê¸° ë„‰ë„‰íˆ ê²€ìƒ‰
        query_for_search = normalize_query(body.question)
        raw_topk = max(20, body.top_k * 5)
        cands = store.search(query_for_search, embed_fn=embed, topk=raw_topk)
        
        # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì²˜ë¦¬ ê°œì„ 
        if not cands:
            return AskResp(
                answer="ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œê°€ ì˜¬ë°”ë¥´ê²Œ ì¸ë±ì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
                used_chunks=0,
                sources=[]
            )

        # 2) í‚¤ì›Œë“œ ë¶€ìŠ¤íŠ¸(ê°„ë‹¨ ê°€ì‚°ì ) â€” rerank ì „ì— ìƒìœ„ê¶Œìœ¼ë¡œ ëŒì–´ì˜¬ë¦¼
        kws = extract_keywords(body.question)
        def _kw_boost_score(c: dict) -> int:
            txt = _strip_meta_line(c.get("chunk", "")).lower()
            return sum(1 for k in kws if k.lower() in txt)

        for c in cands:
            c["kw_boost"] = _kw_boost_score(c)

        ARTICLE_BOOST = float(os.getenv("RAG_ARTICLE_BOOST", "2.5"))

        m = re.search(r"ì œ\s*(\d+)\s*ì¡°", body.question)
        if m:
            art = m.group(1)
            patt = re.compile(rf"ì œ\s*{art}\s*ì¡°")
            for c in cands:
                sec = c.get("section") or ""
                txt = c.get("chunk") or ""
                # META ì¤„ ì œê±°í•œ ë³¸ë¬¸ì— ëŒ€í•´ì„œë„ ì²´í¬í•˜ê³  ì‹¶ìœ¼ë©´ _strip_meta_line(txt) ì‚¬ìš©
                if patt.search(sec) or patt.search(txt):
                    c["kw_boost"] = c.get("kw_boost", 0.0) + ARTICLE_BOOST
        # kw_boost ìš°ì„  â†’ ë™ì  ì‹œ ì›ë˜ score ìœ ì§€
        cands.sort(key=lambda x: (x.get("kw_boost", 0), x.get("score", 0.0)), reverse=True)

        # 3) ë¦¬ë­í¬ í›„ ìƒìœ„ K
        topk = rerank(body.question, cands, top_k=body.top_k)
        if not topk:
            return AskResp(
                answer="ë¬¸ì„œì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                used_chunks=0,
                sources=[]
            )

        # 4) ì„ê³„ê°’ ì»·ì˜¤í”„(ë¦¬ë­ì»¤ ìŠ¤ì½”ì–´ ê¸°ì¤€) - ë” ì—„ê²©í•˜ê²Œ
        THRESH = float(os.getenv("RAG_SCORE_THRESHOLD", "0.3"))  # 0.2 â†’ 0.3ìœ¼ë¡œ ìƒí–¥
        if "re_score" in topk[0] and topk[0]["re_score"] < THRESH:
            return AskResp(
                answer="ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ í™•ì‹¤í•œ ë‹µë³€ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
                used_chunks=0,
                sources=[]
            )

        # 5) ì»¨í…ìŠ¤íŠ¸ + ì¶œì²˜ êµ¬ì„± (ë³¸ë¬¸ë§Œ, META ì œê±°)
        context_lines = []
        sources = []
        for i, c in enumerate(topk, 1):
            sec = (c.get("section") or "").strip()
            body = _strip_meta_line(c.get("chunk",""))
            body_only = f"{sec}\n{body}" if sec and not body.startswith(sec) else body
            context_lines.append(f"[{i}] (doc:{c['doc_id']} p.{c['page']} {c.get('section','')})\n{body_only}")
            sources.append({
                "id": i,
                "doc_id": c.get("doc_id"),
                "page": c.get("page"),
                "section": c.get("section"),
                "chunk": c.get("chunk"),
                "score": c.get("re_score", c.get("score")),
            })
        context = "\n\n".join(context_lines)

        # 6) ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (ë°˜ë³µ ë°©ì§€ + ê°„ê²°í•¨ ê°•ì¡°)
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ì¤‘ìš” ê·œì¹™]
- ë¬¸ì„œì— ëª…í™•í•œ ê·¼ê±°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”
- ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µí•˜ì§€ ë§ˆì„¸ìš”  
- ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
- ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{body.question}

[ë‹µë³€]"""

        # 7) ëª¨ë¸ í˜¸ì¶œ ì‹œ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë¡œ ë°˜ë³µ ë°©ì§€
        answer = generate_answer_unified(prompt, body.model_name)
        
        # 8) ë‹µë³€ í›„ì²˜ë¦¬ - ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±°
        answer = _clean_repetitive_answer(answer)
        
        return AskResp(answer=answer, used_chunks=len(topk), sources=sources)

    except HTTPException:
        raise
    except RuntimeError as milvus_error:
        raise HTTPException(503, f"Milvus ì—°ê²° ëŒ€ê¸°/ê²€ìƒ‰ ì‹¤íŒ¨: {milvus_error}")
    except Exception as e:
        raise HTTPException(500, f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


def _clean_repetitive_answer(answer: str) -> str:
    """ë°˜ë³µë˜ëŠ” ë‹µë³€ íŒ¨í„´ì„ ì •ë¦¬"""
    if not answer:
        return answer
    
    # ë§¤ìš° ê¸´ ë‹µë³€ ì˜ë¼ë‚´ê¸° (1000ì ì´ˆê³¼ ì‹œ)
    if len(answer) > 1000:
        sentences = answer.split('.')
        clean_sentences = []
        for sentence in sentences[:5]:  # ìµœëŒ€ 5ë¬¸ì¥ë§Œ
            if sentence.strip() and len(sentence.strip()) > 10:
                clean_sentences.append(sentence.strip())
        answer = '. '.join(clean_sentences) + '.'
    
    # ë°˜ë³µë˜ëŠ” êµ¬ë¬¸ ì œê±° (ê°™ì€ êµ¬ë¬¸ì´ 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ë©´ ì œê±°)
    lines = answer.split('\n')
    seen_lines = {}
    filtered_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line in seen_lines:
            seen_lines[line] += 1
            if seen_lines[line] <= 2:  # ìµœëŒ€ 2ë²ˆê¹Œì§€ë§Œ í—ˆìš©
                filtered_lines.append(line)
        else:
            seen_lines[line] = 1
            filtered_lines.append(line)
    
    return '\n'.join(filtered_lines).strip()

# ---------- Job State Management ----------
@router.get("/job/{job_id}")
def get_job(job_id: str):
    st = job_state.get(job_id)
    if not st:
        raise HTTPException(404, "í•´ë‹¹ job_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return st

@router.get("/jobs")
def list_jobs(status: Optional[str] = Query(None), limit: int = Query(50, ge=1, le=500)):
    return {"jobs": job_state.list_jobs(status=status, limit=limit)}

@router.get("/doc/{doc_id}")
def doc_status(doc_id: str):
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        cnt = store.count_by_doc(doc_id)
        return {"doc_id": doc_id, "chunks": cnt, "indexed": cnt > 0}
    except Exception as e:
        raise HTTPException(500, f"Milvus ì¡°íšŒ ì‹¤íŒ¨: {e}")
    


# ========== SSE Stream for Job Status =========
@router.get("/job/{job_id}/stream")
async def stream_job(job_id: str):
    async def event_gen():
        last_serialized = None
        while True:
            st = job_state.get(job_id)
            if not st:
                yield {"event": "error", "data": json.dumps({"error": "not found"}, ensure_ascii=False)}
                break

            data = json.dumps(st, ensure_ascii=False)
            if data != last_serialized:
                yield {"event": "update", "data": data}
                last_serialized = data

                if st.get("status") in ("done", "error"):
                    break

            await asyncio.sleep(1)

    return EventSourceResponse(event_gen())

# ---------- MinIO Utilities ----------

    
@router.get("/files")
def list_files(prefix: str = "uploaded/", include_internal: bool = False, only_pdf: bool = False):
    m = MinIOStore()
    try:
        keys = m.list_files(prefix=prefix) 
    except Exception as e:
        raise HTTPException(500, f"MinIO íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # ë‚´ë¶€ ê´€ë¦¬ ì˜¤ë¸Œì íŠ¸ ìˆ¨ê¸°ê¸° (ì›í•˜ë©´ include_internal=Trueë¡œ ë…¸ì¶œ)
    if not include_internal:
        keys = [k for k in keys if not (k.endswith(".flag") or "/__hash__/" in k or "/__meta__/" in k)]

    if only_pdf:
        keys = [k for k in keys if k.lower().endswith(".pdf")]

    return {"files": keys}


@router.get("/file/{object_name:path}")
def get_file_presigned(
    object_name: str,
    minutes: int = Query(60, ge=1, le=7*24*60),
    download_name: Optional[str] = None,
    inline: bool = False,  # trueë©´ inline, falseë©´ attachment
):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    headers = None
    if download_name:
        disp = "inline" if inline else "attachment"
        headers = {"response-content-disposition": f'{disp}; filename="{download_name}"'}

    try:
        url = m.presigned_url(key, method="GET", expires=timedelta(minutes=minutes), response_headers=headers)
        return {"url": url}
    except Exception as e:
        raise HTTPException(500, f"presign failed: {e}")


@router.get("/docs")
def list_docs(limit: int = 200):
    m = MinIOStore()
    try:
        keys = m.list_files("uploaded/")
    except Exception as e:
        raise HTTPException(500, f"minio list failed: {e}")

    def is_internal(k: str) -> bool:
        return k.endswith(".flag") or "/__hash__/" in k or "/__meta__/" in k

    pdf_keys = [k for k in keys if k.lower().endswith(".pdf") and not is_internal(k)]
    out = []

    for k in pdf_keys:
        base = os.path.splitext(os.path.basename(k))[0]
        # ë©”íƒ€ JSONì—ì„œ ì›ë³¸ ì°¾ê¸°
        meta_key = f"uploaded/__meta__/{base}.json"
        orig_key = None
        title = os.path.basename(k)

        try:
            if m.exists(meta_key):
                # ì„ì‹œ íŒŒì¼ë¡œ ë°›ì•„ì„œ ë¡œë“œ
                tmp = os.path.join(tempfile.gettempdir(), f"{uuid.uuid4().hex}.json")
                m.download(meta_key, tmp)
                with open(tmp, "r", encoding="utf-8") as fp:
                    meta = json.load(fp)
                os.remove(tmp)
                orig_key = meta.get("original") or meta.get("orig_object")
                title = meta.get("original_name") or title
        except Exception:
            pass

        # í´ë°±: originals/ ì—ì„œ ê°™ì€ base ì´ë¦„ì„ ê°€ì§„ ì›ë³¸ íƒìƒ‰
        if not orig_key:
            for ext in (".pdf", ".docx", ".hwpx", ".hwp"):
                cand = f"uploaded/originals/{base}{ext}"
                if m.exists(cand):
                    orig_key = cand
                    break

        # presigned URL ìƒì„±
        try:
            pdf_url = m.presigned_url(k, method="GET", expires=timedelta(minutes=60))
        except Exception as e:
            raise HTTPException(500, f"presign(pdf) failed: {e}")

        if orig_key and m.exists(orig_key):
            try:
                download_url = m.presigned_url(
                    orig_key, method="GET", expires=timedelta(minutes=60),
                    response_headers={"response-content-disposition": f'attachment; filename="{title}"'}
                )
            except Exception:
                download_url = pdf_url
        else:
            download_url = pdf_url  # ì›ë³¸ ì—†ìœ¼ë©´ PDFë¡œ í´ë°±

        out.append({
            "doc_id": base,
            "title": title,
            "object_key": k,        # PDF object
            "url": pdf_url,         # PDF ë³´ê¸°ìš©
            "download_url": download_url,  # ì›ë³¸ ë‹¤ìš´ë¡œë“œìš©
            "uploaded_at": None,
        })

        if len(out) >= limit:
            break

    return out

@router.get("/status")
def status():
    m = MinIOStore()
    try:
        keys = m.list_files("uploaded/")
        pdfs = [k for k in keys if k.lower().endswith(".pdf") and "/__hash__/" not in k and "/__meta__/" not in k]
        return {"has_data": len(pdfs) > 0, "doc_count": len(pdfs)}
    except Exception:
        return {"has_data": False, "doc_count": 0}


@router.delete("/file/{object_name}")
def delete_file(object_name: str):
    try:
        minio = MinIOStore()
        if not minio.exists(object_name):
            raise HTTPException(404, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        minio.delete(object_name)
        return {"status": "ok", "deleted": object_name}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
@router.get("/view/{object_name:path}")
def view_object(object_name: str, name: Optional[str] = None):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    disp_name = name or os.path.basename(key)
    ext = os.path.splitext(key)[1].lower()
    media = "application/pdf" if ext == ".pdf" else (mimetypes.guess_type(disp_name)[0] or "application/octet-stream")

    try:
        obj = m.client.get_object(m.bucket, key)
    except Exception as e:
        raise HTTPException(500, f"MinIO get_object failed: {e}")

    headers = {
        # ğŸ‘‡ latin-1 ì•ˆì „í•˜ê²Œ
        "Content-Disposition": _content_disposition("inline", disp_name)
    }

    def _iter():
        try:
            for chunk in obj.stream(32 * 1024):
                yield chunk
        finally:
            obj.close()
            obj.release_conn()

    return StreamingResponse(_iter(), media_type=media, headers=headers)


@router.get("/download/{object_name:path}")
def download_object(object_name: str, name: Optional[str] = None):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    disp_name = name or os.path.basename(key)
    media = mimetypes.guess_type(disp_name)[0] or "application/octet-stream"

    try:
        obj = m.client.get_object(m.bucket, key)
    except Exception as e:
        raise HTTPException(500, f"MinIO get_object failed: {e}")

    headers = {
        # ğŸ‘‡ latin-1 ì•ˆì „í•˜ê²Œ
        "Content-Disposition": _content_disposition("attachment", disp_name)
    }

    def _iter():
        try:
            for chunk in obj.stream(32 * 1024):
                yield chunk
        finally:
            obj.close()
            obj.release_conn()

    return StreamingResponse(_iter(), media_type=media, headers=headers)

# ---------- Bulk delete MinIO files under a prefix ----------
@router.delete("/files/purge", tags=["llama"])
def purge_files(
    prefix: str = Query("uploaded/", description="ì§€ìš¸ ê²½ë¡œ prefix (ë°˜ë“œì‹œ 'uploaded/'ë¡œ ì‹œì‘)"),
    dry_run: bool = Query(False, description="trueë©´ ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ëª©ë¡ë§Œ ë°˜í™˜"),
    limit_preview: int = Query(50, ge=1, le=500, description="dry_run ë•Œ ë¯¸ë¦¬ë³´ê¸° ìµœëŒ€ ê°œìˆ˜"),
):
    """
    MinIOì—ì„œ íŠ¹ì • prefix í•˜ìœ„ ê°ì²´ë“¤ì„ ì¼ê´„ ì‚­ì œ.
    - ì•ˆì „ì¥ì¹˜: prefixê°€ 'uploaded/'ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ 400 ì—ëŸ¬
    - dry_run=True ë©´ ì‚­ì œ ì—†ì´ ëª©ë¡ ë¯¸ë¦¬ë³´ê¸°ë§Œ
    """
    if not prefix or not prefix.startswith("uploaded/"):
        raise HTTPException(400, "prefixëŠ” ë°˜ë“œì‹œ 'uploaded/'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")

    try:
        minio = MinIOStore()
        files = minio.list_files(prefix=prefix)
    except Exception as e:
        raise HTTPException(500, f"MinIO ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    matched = len(files)
    if dry_run:
        preview = files[:limit_preview]
        more = max(0, matched - len(preview))
        return {"status": "dry-run", "prefix": prefix, "matched": matched, "preview": preview, "more": more}

    deleted = 0
    failed = 0
    errors = []
    for obj in files:
        try:
            minio.delete(obj)
            deleted += 1
        except Exception as e:
            failed += 1
            errors.append({"object": obj, "error": str(e)})

    return {"status": "ok", "prefix": prefix, "matched": matched, "deleted": deleted, "failed": failed, "errors": errors}

# ========= Debug / Inspection =========
@router.get("/debug/milvus/info")
def debug_milvus_info():
    """ Milvus ìƒíƒœ ì •ë³´ ì¡°íšŒ"""
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        return store.stats()
    except Exception as e:
        raise HTTPException(500, f"Milvus info ì¡°íšŒ ì‹¤íŒ¨: {e}")

@router.get("/debug/milvus/peek")
def debug_milvus_peek(limit: int = 100, full: bool = True, max_chars:int|None = None):
    """ Milvus ì»¬ë ‰ì…˜ì˜ ì¼ë¶€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° """
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        if full:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = "0"
        elif max_chars is not None:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = str(max_chars)
        return {"items": store.peek(limit=limit)}
    except Exception as e:
        raise HTTPException(500, f"Milvus peek ì‹¤íŒ¨: {e}")

@router.get("/debug/milvus/by-doc")
def debug_milvus_by_doc(doc_id: str, limit: int = 10, full: bool = False, max_chars:int|None = None):
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        if full:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = "0"
        elif max_chars is not None:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = str(max_chars)
        return {"items": store.query_by_doc(doc_id=doc_id, limit=limit)}
    except Exception as e:
        raise HTTPException(500, f"Milvus by-doc ì‹¤íŒ¨: {e}")

@router.get("/debug/search")
def debug_vector_search(q: str, k: int = 5):
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        raw = store.debug_search(q, embed_fn=embed, topk=k)
        return {"results": raw}
    except Exception as e:
        raise HTTPException(500, f"ë””ë²„ê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
