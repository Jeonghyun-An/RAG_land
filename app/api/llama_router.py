# app/api/llama_router.py
from __future__ import annotations

import functools
import mimetypes
import hashlib, tempfile
import os, re
import uuid
from urllib.parse import unquote, quote
from typing import List, Optional,Literal
from starlette.responses import StreamingResponse

from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks, Query
from pydantic import BaseModel
import asyncio, json
import numpy as np
import logging
logger = logging.getLogger("uvicorn.error")  # uvicorn ì¶œë ¥ì— ì„ê¸°

from sse_starlette.sse import EventSourceResponse  # âœ… ìš”êµ¬ì‚¬í•­: sse-starlette

from datetime import datetime, timedelta, timezone
import time as pytime

from app.services.file_parser import (
    parse_pdf,                    # (local path) -> [(page_no, text)]
    parse_pdf_blocks,             # (local path) -> [(page_no, [ {text,bbox}, ... ])]
    parse_any_bytes,              # (filename, bytes) -> {"kind":"pdf", "pages":[...], "blocks":[...]}
    parse_pdf_blocks_from_bytes,  # (bytes) -> [(page_no, [ {text,bbox}, ... ])]
)
from app.services import job_state
from app.services.llama_model import generate_answer_unified
from app.services.minio_store import MinIOStore
from app.services.pdf_converter import convert_stream_to_pdf_bytes, convert_to_pdf,convert_bytes_to_pdf_bytes, ConvertError
from app.services.milvus_store_v2 import MilvusStoreV2
from app.services.embedding_model import get_embedding_model, embed
from app.services.reranker import rerank
from app.services.llm_client import get_openai_client
from app.services.db_connector import DBConnector

router = APIRouter(tags=["llama"])
logger.info("[ask] router loaded v2025-10-29a")

UPLOAD_DIR = "data"
os.makedirs(UPLOAD_DIR, exist_ok=True)
# ---------- Schemas ----------
class GenerateReq(BaseModel):
    prompt: str
    model_name: str = "qwen2.5-14b"

class AskReq(BaseModel):
    question: str
    model_name: str = "qwen2.5-14b"
    top_k: int = 3  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì§€ì • ê°€ëŠ¥í•˜ì§€ë§Œ, response_typeìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ
    history: Optional[List[dict]] = []
    doc_ids: Optional[List[str]] = None
    response_type: Literal["short", "long"] = "short"  # short(ë‹¨ë¬¸í˜•) | long(ì¥ë¬¸í˜•)

class UploadResp(BaseModel):
    filename: str
    minio_object: str
    indexed: str  # "background"
    job_id: Optional[str] = None

class AskResp(BaseModel):
    answer: str
    used_chunks: int
    sources: Optional[List[dict]] = None  # (ì„ íƒ) ì¶œì²˜ ì œê³µ
    
# ========== ë‹µë³€ ëª¨ë“œë³„ ì„¤ì • ==========
RESPONSE_MODE_CONFIG = {
    "short": {
        "top_k": 3,
        "max_tokens": 320,
        "top_p": 0.9,
        "temperature": 0.0,
        "context_style": "concise",  # ê°„ê²°í•œ ì»¨í…ìŠ¤íŠ¸
    },
    "long": {
        "top_k": 8,
        "max_tokens": 1024,
        "top_p": 0.92,
        "temperature": 0.1,  # ì•½ê°„ì˜ ë‹¤ì–‘ì„±
        "context_style": "detailed",  # ìƒì„¸í•œ ì»¨í…ìŠ¤íŠ¸
    }
}

# --- í´ë°± ì „ìš©: pages ì •ê·œí™” ë„ìš°ë¯¸ ---------------------------------
def _normalize_pages_for_chunkers(pages):
    """
    pagesë¥¼ [(page_no:int, text:str), ...] ë¡œ ê°•ì œ ë³€í™˜.
    í—ˆìš© ì…ë ¥:
      - [(int, str)], [[int, str]]
      - ["page text", ...]  -> enumerate 1-based
      - [{"page":..,"text":..}], [{"page_no":..,"body":..}], [{"index":..,"lines":[..]}]
    ê·¸ ì™¸ëŠ” ë¬¸ìì—´í™”í•´ì„œ ì•ˆì „í•˜ê²Œ ìˆ˜ìš©.
    """
    out = []
    if not pages:
        return out

    for i, item in enumerate(pages, start=1):
        # (int,str) íŠœí”Œ/ë¦¬ìŠ¤íŠ¸
        if isinstance(item, (tuple, list)):
            if len(item) >= 2:
                pno, txt = item[0], item[1]
            else:
                pno, txt = i, (item[0] if item else "")
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, "" if txt is None else str(txt)))
            continue

        # dict
        if isinstance(item, dict):
            pno = item.get("page") or item.get("page_no") or item.get("index") or i
            txt = (
                item.get("text")
                or item.get("body")
                or ("\n".join(item.get("lines") or []) if item.get("lines") else "")
                or ""
            )
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, str(txt)))
            continue

        # ë¬¸ìì—´
        if isinstance(item, str):
            out.append((i, item))
            continue

        # ê¸°íƒ€: ë¬¸ìì—´í™”
        out.append((i, str(item)))

    return out

# ---------- Helpers ----------
def _coerce_chunks_for_milvus(chs):
    """
    (í…ìŠ¤íŠ¸, ë©”íƒ€) ë¦¬ìŠ¤íŠ¸ë¥¼ Milvus insert í˜•íƒœë¡œ ì •ê·œí™”:
    - ë©”íƒ€ íƒ€ì… ë³´ì •(dict ê°•ì œ), page=int, section<=512ì
    - ë‹¤ì¤‘ í˜ì´ì§€ ì§€ì›: meta.pagesê°€ ìˆìœ¼ë©´ pageëŠ” ì²« í˜ì´ì§€ë¡œ
    - ë¹ˆ í…ìŠ¤íŠ¸/ì—°ì† ì¤‘ë³µ ì œê±°
    """
    safe = []
    for t in chs or []:
        if not isinstance(t, (list, tuple)) or len(t) < 2:
            continue
        text, meta = t[0], t[1]
        text = "" if text is None else str(text)
        if not isinstance(meta, dict):
            meta = {}

        # section ìš°ì„  ê²°ì •
        section = str(meta.get("section", ""))[:512]
        # page ì •ê·œí™”: pagesê°€ ìˆìœ¼ë©´ ì²« í˜ì´ì§€
        pages = meta.get("pages")
        if isinstance(pages, (list, tuple)) and len(pages) > 0:
            try:
                page = int(pages[0])
            except Exception:
                page = int(meta.get("page", 0))
        else:
            try:
                page = int(meta.get("page", 0))
            except Exception:
                page = 0

        safe.append((text, {"page": page, "section": section, "pages": pages or [], "bboxes": meta.get("bboxes", {})}))

    out = []
    last = None
    for it in safe:
        if it[0] and it != last:
            out.append(it)
            last = it
    return out

def _make_encoder():
    m = get_embedding_model()
    tok = getattr(m, "tokenizer", None)
    max_len = int(getattr(m, "max_seq_length", 128))
    def enc(s: str):
        if tok is None:
            return []
        return tok.encode(s, add_special_tokens=False) or []
    return enc, max_len

def _sha256_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()

def meta_key(doc_id: str) -> str:
    return f"uploaded/__meta__/{doc_id}/meta.json"

def legacy_meta_key(doc_id: str) -> str:
    return f"uploaded/__meta__/{doc_id}.json"

def index_pdf_to_milvus(
    job_id: str,
    file_path: str | None = None,
    minio_object: str | None = None,
    uploaded: bool = True,
    remove_local: bool = True,
    doc_id: str | None = None,
) -> None:
    """
    ì—…ë¡œë“œëœ(ë˜ëŠ” MinIO ìƒì˜) PDFë¥¼ íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ Milvus upsert.
    """
    try:
        job_state.update(job_id, status="parsing", step="parse_pdf:start")
        print(f"[INDEX] start: {file_path or minio_object}")

        NO_LOCAL = os.getenv("RAG_NO_LOCAL", "0") == "1"
        SKIP_IF_ALREADY_UPLOADED = os.getenv("RAG_SKIP_IF_UPLOADED", "1") == "1"

        if not uploaded and SKIP_IF_ALREADY_UPLOADED:
            job_state.update(job_id, status="done", step="skipped:already_uploaded", progress=100)
            print(f"[INDEX] skip: uploaded=False (already uploaded), job_id={job_id}")
            return

        # ---------- 1) PDF â†’ í…ìŠ¤íŠ¸/ë ˆì´ì•„ì›ƒ ----------
        pages: list | None = None
        layout_map: dict[int, list[dict]] = {}
        pdf_bytes: bytes | None = None

        use_bytes_path = (NO_LOCAL or file_path is None) and bool(minio_object)
        
        # [í•µì‹¬] pdf_fusion ëª¨ë“ˆ ìš°ì„  ì‚¬ìš© (OCR + layout_blocks í†µí•©)
        try:
            if use_bytes_path:
                # MinIO â†’ bytes
                from app.services.minio_store import MinIOStore
                mstore = MinIOStore()
                pdf_bytes = mstore.get_bytes(minio_object)
                
                # pdf_fusionìœ¼ë¡œ í†µí•© ì¶”ì¶œ
                from app.services.pdf_fusion import extract_pdf_fused_from_bytes
                print("[PARSE] Using pdf_fusion (bytes mode) with OCR+layout integration")
                pages_tuples, layout_map = extract_pdf_fused_from_bytes(pdf_bytes)
                pages = list(pages_tuples)
                
            else:
                # ë¡œì»¬ íŒŒì¼
                from app.services.pdf_fusion import extract_pdf_fused
                print("[PARSE] Using pdf_fusion (file mode) with OCR+layout integration")
                pages_tuples, layout_map = extract_pdf_fused(file_path)
                pages = list(pages_tuples)
                
            print(f"[PARSE] Extracted {len(pages)} pages with layout_blocks for {len(layout_map)} pages")
            
        except Exception as fusion_err:
            print(f"[PARSE] pdf_fusion failed: {fusion_err}, falling back to legacy parser")
            
            # í´ë°±: ê¸°ì¡´ file_parser ì‚¬ìš©
            if use_bytes_path:
                from app.services.file_parser import parse_any_bytes, parse_pdf_blocks_from_bytes
                parsed = parse_any_bytes(os.path.basename(minio_object), pdf_bytes)
                if parsed.get("kind") != "pdf":
                    raise RuntimeError("PDF íŒŒì´í”„ë¼ì¸ë§Œ ì¸ë±ì‹±í•©ë‹ˆë‹¤.")
                pages = parsed.get("pages") or []
                blocks_by_page_list = parsed.get("blocks")
                
                if isinstance(blocks_by_page_list, dict):
                    layout_map = {int(k): v for k, v in blocks_by_page_list.items()}
                else:
                    layout_map = {int(p): blks for p, blks in (blocks_by_page_list or [])}
            else:
                from app.services.file_parser import parse_pdf, parse_pdf_blocks
                pages = parse_pdf(file_path, by_page=True)
                if not pages:
                    raise RuntimeError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                blocks_by_page_list = parse_pdf_blocks(file_path)
                layout_map = {int(p): blks for p, blks in (blocks_by_page_list or [])}

        # ---------- 1-1) í˜ì´ì§€ í‘œì¤€í™” ----------
        pages_std = _normalize_pages_for_chunkers(pages)
        if not any((t or "").strip() for _, t in pages_std):
            print("[PARSE] no textual content after parsing/OCR; will use fallback if chunkers return empty")
        
        job_state.update(job_id, status="parsing", step="parse_pdf:done", progress=25)

        # ---------- 2) ê³ ë„í™”ëœ ì²­í‚¹ ì‹œìŠ¤í…œ ----------
        job_state.update(job_id, status="chunking", step="chunk:start", progress=35)

        # ì¸ì½”ë”/ê¸¸ì´
        enc, max_len = _make_encoder()
        default_target = max(64, max_len - 16)
        default_overlap = min(50, default_target // 4)

        target_tokens = int(os.getenv("RAG_TARGET_TOKENS", str(default_target)))
        overlap_tokens = int(os.getenv("RAG_OVERLAP_TOKENS", str(default_overlap)))
        min_chunk_tokens = int(os.getenv("RAG_MIN_CHUNK_TOKENS", "100"))

        chunks: list[tuple[str, dict]] | None = None

        # 2-1) ì˜ì–´ ê¸°ìˆ  ë¬¸ì„œ ì²­ì»¤ (english_technical_chunker) ìµœìš°ì„ 
        ENABLE_EN_TECH_CHUNKER = os.getenv("RAG_ENABLE_EN_TECH_CHUNKER", "1") == "1"
        
        if ENABLE_EN_TECH_CHUNKER:
            try:
                from app.services.english_technical_chunker import english_technical_chunk_pages
                print("[CHUNK] Trying English technical chunker (IAEA/standards optimized)...")
                
                # ì˜ì–´ ë¬¸ì„œëŠ” ë” í° íƒ€ê²Ÿ í† í° ì‚¬ìš©
                en_target_tokens = int(os.getenv("RAG_EN_TARGET_TOKENS", "800"))
                
                chunks = english_technical_chunk_pages(
                    pages_std, enc, en_target_tokens, overlap_tokens, layout_map
                )
                
                if chunks and len(chunks) > 0:
                    print(f"[CHUNK] English technical chunker: {len(chunks)} chunks")
                else:
                    print("[CHUNK] English technical chunker returned empty, falling back")
                    chunks = None
            except Exception as e:
                print(f"[CHUNK] English technical chunker error: {e}")
                chunks = None

        # 2-2) ë²•ë ¹ ì²­ì»¤ (law_chunker)
        if chunks is None:
            ENABLE_LAW_CHUNKER = os.getenv("RAG_ENABLE_LAW_CHUNKER", "1") == "1"
            
            if ENABLE_LAW_CHUNKER:
                try:
                    from app.services.law_chunker import law_chunk_pages
                    print("[CHUNK] Trying law chunker (nuclear/legal optimized)...")
                    
                    chunks = law_chunk_pages(
                        pages_std, enc, target_tokens, overlap_tokens,
                        layout_blocks=layout_map, min_chunk_tokens=min_chunk_tokens
                    )
                    
                    if chunks and len(chunks) > 0:
                        print(f"[CHUNK] Law chunker: {len(chunks)} chunks")
                    else:
                        print("[CHUNK] Law chunker returned empty, falling back")
                        chunks = None
                except Exception as e:
                    print(f"[CHUNK] Law chunker error: {e}")
                    chunks = None

        # 2-3) Smart chunker Plus (layout í™œìš©)
        if chunks is None:
            ENABLE_LAYOUT_CHUNKER = os.getenv("RAG_ENABLE_LAYOUT_CHUNKER", "1") == "1"
            
            if ENABLE_LAYOUT_CHUNKER and layout_map:
                try:
                    from app.services.chunker import smart_chunk_pages_plus
                    print("[CHUNK] Using layout-aware chunker (SmartChunkerPlus)...")
                    
                    chunks = smart_chunk_pages_plus(
                        pages_std, enc, target_tokens, overlap_tokens, layout_map
                    )
                    
                    if chunks and len(chunks) > 0:
                        print(f"[CHUNK] Layout chunker: {len(chunks)} chunks")
                    else:
                        print("[CHUNK] Layout chunker returned empty, falling back")
                        chunks = None
                except Exception as e:
                    print(f"[CHUNK] Layout chunker error: {e}")
                    chunks = None

        # 2-4) ê¸°ë³¸ Smart chunker
        if chunks is None:
            try:
                from app.services.chunker import smart_chunk_pages
                print("[CHUNK] Using basic smart chunker...")
                
                chunks = smart_chunk_pages(
                    pages_std, enc, target_tokens, overlap_tokens, layout_map
                )
                
                if chunks and len(chunks) > 0:
                    print(f"[CHUNK] Basic chunker: {len(chunks)} chunks")
                else:
                    raise RuntimeError("Basic chunker returned empty")
            except Exception as e:
                print(f"[CHUNK] Basic chunker error: {e}")
                raise RuntimeError(f"ëª¨ë“  ì²­í‚¹ ë°©ë²• ì‹¤íŒ¨: {e}")

        # 2-5) ìµœí›„ ë³´í˜¸ë§‰ (í´ë°±)
        if not chunks or len(chunks) == 0:
            print("[CHUNK] All chunkers failed - using fallback protection")
            
            flat_texts = []
            for _, t in pages_std or []:
                tt = (t or "").strip()
                if tt:
                    # ì´ìƒí•œ ë¼ë²¨ ì œê±°
                    tt = re.sub(r'\bì¸ì ‘í–‰\s*ë¬¶ìŒ\b', '', tt)
                    tt = re.sub(r'\b[ê°€-í£]*\s*ë¬¶ìŒ\b', '', tt)
                    tt = re.sub(r'[\r\n\s]+', ' ', tt)
                    if tt.strip():
                        flat_texts.append(tt.strip())

            fallback_text = "\n\n".join(flat_texts).strip()
            
            if not fallback_text:
                # ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
                try:
                    if pdf_bytes:
                        import fitz
                        doc_dbg = fitz.open(stream=pdf_bytes, filetype="pdf")
                        placeholders = [f"[page {i+1}: image or low-text content]" for i in range(doc_dbg.page_count)]
                        fallback_text = "\n".join(placeholders).strip()
                except Exception:
                    pass

            # ìµœì¢… í´ë°±
            if not fallback_text:
                if os.getenv("RAG_ALLOW_EMPTY_FALLBACK", "1") == "1":
                    fallback_text = "[Document processed but no readable text content found]"
                else:
                    raise RuntimeError("ëª¨ë“  ì²­í‚¹ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

            # í´ë°± ì²­í¬ ìƒì„±
            tokens = len(enc(fallback_text))
            chunks = [(fallback_text, {"page": 1, "pages": [1], "section": "", "token_count": tokens, "bboxes": {}})]
            print(f"[CHUNK] Fallback chunk created: {tokens} tokens")

        print(f"[CHUNK] Final result: {len(chunks)} chunks ready for embedding")
        job_state.update(job_id, status="chunking", step="chunk:done", chunks=len(chunks), progress=50)

        # ---------- 3) doc_id í™•ì • ----------
        if not doc_id:
            base_from_obj = os.path.splitext(os.path.basename(minio_object or ""))[0] if minio_object else None
            doc_id = base_from_obj or (os.path.splitext(os.path.basename(file_path))[0] if file_path else None)
            if not doc_id:
                import uuid
                doc_id = uuid.uuid4().hex

        REPLACE_BEFORE_INSERT = os.getenv("RAG_REPLACE_BEFORE_INSERT", "0") == "1"
        RETRY_AFTER_DELETE_ON_DUP = os.getenv("RAG_RETRY_AFTER_DELETE", "1") == "1"

        st = job_state.get(job_id) or {}
        mode = st.get("mode")  # 'replace' | 'version' | 'skip' ë“±

        from app.services.milvus_store_v2 import MilvusStoreV2
        from app.services.embedding_model import embed, get_sentence_embedding_dimension
        store = MilvusStoreV2(dim=get_sentence_embedding_dimension())

        if mode == "replace" or REPLACE_BEFORE_INSERT:
            try:
                if hasattr(store, "delete_by_doc_id"):
                    deleted = store.delete_by_doc_id(doc_id)
                else:
                    deleted = store._delete_by_doc_id(doc_id)
                print(f"[INDEX] pre-delete for replace: doc_id={doc_id}, deleted={deleted}")
            except Exception as e:
                print(f"[INDEX] pre-delete warn: {e}")

        # ---------- 4) Milvus upsert ----------
        job_state.update(job_id, status="embedding", step="embed:start", progress=60)
        res = store.insert(doc_id, chunks, embed_fn=embed)  # {inserted, skipped, reason, doc_id}
        real_doc_id = res.get("doc_id", doc_id)

        if res.get("skipped") and (mode == "replace" or RETRY_AFTER_DELETE_ON_DUP):
            reason = (res.get("reason") or "").lower()
            if any(k in reason for k in ["duplicate", "exists", "doc_id"]):
                try:
                    if hasattr(store, "delete_by_doc_id"):
                        deleted = store.delete_by_doc_id(real_doc_id)
                    else:
                        deleted = store._delete_by_doc_id(real_doc_id)
                    print(f"[INDEX] retry-after-delete: deleted={deleted}, doc_id={real_doc_id}")
                    res = store.insert(doc_id, chunks, embed_fn=embed)
                    real_doc_id = res.get("doc_id", doc_id)
                except Exception as e:
                    print(f"[INDEX] retry-after-delete failed: {e}")

        if res.get("skipped"):
            job_state.update(job_id, status="indexing", step=f"milvus:skipped:{res.get('reason')}",
                             progress=90, doc_id=real_doc_id)
            print(f"[INDEX] skipped: doc_id={real_doc_id}, reason={res.get('reason')}")
        else:
            job_state.update(job_id, status="indexing", step=f"milvus:inserted:{res.get('inserted',0)}",
                             progress=90, doc_id=real_doc_id)
            print(f"[INDEX] done: {minio_object or file_path} (doc_id={real_doc_id}, chunks={len(chunks)}, "
                  f"inserted={res.get('inserted',0)})")

        # ---------- 5) MinIO ì›ë³¸ ì‚­ì œ(ì˜µì…˜) ----------
        if os.getenv("RAG_DELETE_AFTER_INDEX", "0") == "1" and minio_object and uploaded:
            try:
                from app.services.minio_store import MinIOStore
                MinIOStore().delete(minio_object)
                print(f"[CLEANUP] deleted from MinIO: {minio_object}")
                job_state.update(job_id, status="cleanup", step="minio:deleted",
                                 minio_object=minio_object, progress=95)
            except Exception as e:
                print(f"[CLEANUP] delete failed: {e}")
                job_state.update(job_id, status="cleanup", step=f"minio:delete_failed:{e!s}")

        # ---------- 6) ë¡œì»¬ íŒŒì¼ ì •ë¦¬ ----------
        if remove_local and file_path and not use_bytes_path:
            try:
                os.remove(file_path)
            except Exception:
                pass
        total = len(chunks) if isinstance(chunks, list) else None

        if total is None:
            try:
                total = store.count_by_doc(real_doc_id)
            except Exception:
                total = None
    
        # ë©”íƒ€ ê°±ì‹ 
        try:
            from app.services.minio_store import MinIOStore
            mstore = MinIOStore()
            meta = {}
            try:
                if mstore.exists(meta_key(real_doc_id)):
                    meta = mstore.get_json(meta_key(real_doc_id))
                elif mstore.exists(legacy_meta_key(real_doc_id)):
                    meta = mstore.get_json(legacy_meta_key(real_doc_id))
            except Exception:
                meta = {}

            meta = dict(meta or {})
            if total is not None:
                meta["chunk_count"] = int(total)
            meta["indexed"] = True
            meta["last_indexed_at"] = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

            mstore.put_json(meta_key(real_doc_id), meta)
        except Exception as e:
            print(f"[INDEXER] warn: failed to update meta chunk_count: {e}")
    
        # ---------- 7) ì™„ë£Œ ----------
        job_state.complete(
            job_id,
            pages=len(pages_std or []),
            chunks=len(chunks or []),
            doc_id=real_doc_id,
            inserted=int(res.get("inserted", 0)),
            skipped=bool(res.get("skipped", False)),
            reason=res.get("reason"),
        )

    except Exception as e:
        job_state.fail(job_id, str(e))
        raise
    
def _content_disposition(disposition: str, filename: str) -> str:
    """
    latin-1 ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´:
    - ASCII fallback: íŒŒì¼ëª…ì—ì„œ ë¹„ASCIIë¥¼ _ ë¡œ ëŒ€ì²´
    - filename*: UTF-8''<percent-encoded> í•¨ê»˜ ì œê³µ
    """
    # fallback: ASCIIë§Œ ë‚¨ê¸°ê¸°
    ascii_fallback = re.sub(r'[^A-Za-z0-9._-]+', '_', filename) or 'file'
    utf8_quoted = quote(filename)  # UTF-8 percent-encode
    return f"{disposition}; filename=\"{ascii_fallback}\"; filename*=UTF-8''{utf8_quoted}"

def _strip_meta_line(chunk_text: str) -> str:
    """ì²­í¬ ë§¨ ìœ„ META: ë¼ì¸ì„ ì œê±°í•˜ê³  ë³¸ë¬¸ë§Œ ë°˜í™˜"""
    t = chunk_text or ""
    if t.startswith("META:"):
        nl = t.find("\n")
        t = t[nl+1:] if nl != -1 else ""
    return t.strip()

_DEF_PATTS = ("ë­ì•¼", "ë¬´ì—‡", "ë­”ê°€", "ì˜ë¯¸", "ì •ì˜", "ì„¤ëª…", "ì–´ë–¤", "ë¬´ì—‡ì¸ê°€", "ë¬´ì—‡ì¸ì§€")

def normalize_query(q: str) -> str:
    """
    ì •ì˜/ì„¤ëª…í˜• ì§ˆë¬¸ì„ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ë³´ê°•:
    - '... ë­ì•¼/ë¬´ì—‡/ì˜ë¯¸' ë“±ì„ '... ë‚´ìš©'ìœ¼ë¡œ ë³´ê°•
    - ë„ˆë¬´ ê³¼í•˜ê²Œ ë°”ê¾¸ì§€ ì•Šê³  ì›ë¬¸ì„ ìœ ì§€í•˜ë˜ 'ë‚´ìš©', 'ì •ì˜' í† í°ì„ ì¶”ê°€
    """
    base = q.strip()
    lowered = base.lower()
    if any(p in base for p in _DEF_PATTS):
        # í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´ + ë‚´ìš©/ì •ì˜ë¥¼ ë§ë¶™ì—¬ ë²¡í„° ê²€ìƒ‰ ì¹œí™”í™”
        return f"{base} ë‚´ìš© ì •ì˜"
    return base

_KW_TOKEN_RE = re.compile(r"[A-Za-zê°€-í£0-9\.#\-]+")  # '57bí•­', 'Â§57(b)', 'AEA-57b' ë¥˜ ë³´ì¡´

def extract_keywords(q: str) -> list[str]:
    """
    ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ(ì§§ì€ ì¡°ì‚¬ë¥˜/í•œ ê¸€ì í† í° ì œê±°)
    """
    toks = [t for t in _KW_TOKEN_RE.findall(q) if len(t) >= 2]
    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´)
    seen, out = set(), []
    for t in toks:
        tl = t.lower()
        if tl not in seen:
            seen.add(tl); out.append(t)
    return out

def _detect_lang(text: str) -> str:
    """ì•„ì£¼ ë‹¨ìˆœí•œ í•œê¸€ ê°ì§€. í•œê¸€ í¬í•¨ë˜ë©´ 'ko', ì•„ë‹ˆë©´ 'en'."""
    if any('\uac00' <= ch <= '\ud7a3' for ch in (text or "")):
        return "ko"
    return "en"

def _t(lang: str, ko: str, en: str) -> str:
    return ko if lang == "ko" else en

# def _build_prompt(context: str, question: str, lang: str) -> str:
#     if lang == "ko":
#         return f"""ë‹¹ì‹ ì€ í•œêµ­ì›ìë ¥í†µì œê¸°ìˆ ì›(KINAC)ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'í‚¤ë‚˜ê¸°AI'ì…ë‹ˆë‹¤.

# # ë‹µë³€ ì›ì¹™
# 1. í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (~ìŠµë‹ˆë‹¤, ~í•˜ì„¸ìš” ì²´).
# 2. ì´ëª¨ì§€, ì€ì–´, ì¸í„°ë„· ìŠ¬ë­ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# 3. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.

# # ë‹µë³€ ë°©ì‹
# ì§ˆë¬¸ì´ ì¸ì‚¬, ì•ˆë¶€, ê²©ë ¤, ì¡ë‹´, ì¼ìƒ ì¡°ì–¸ì´ë©´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³  1~3ë¬¸ì¥ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

# ì§ˆë¬¸ì´ ì •ì˜, ì ˆì°¨, ì •ì±…, ê·œì •, ìš©ì–´ ì„¤ëª…ì´ë©´ ì•„ë˜ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
# - ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê³ , ì™¸ë¶€ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
# - 2~4ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
# - ë¶ˆë¦¿(-)ì€ ì—¬ëŸ¬ ì¡°ì¹˜ë‚˜ ì ˆì°¨ë¥¼ ë‚˜ì—´í•  ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
# - ë²ˆí˜¸(1), 2), â‘ , â‘¡ ë“±ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
# - ì›ë¬¸ ìš©ì–´(Source material, Safeguards, PIV, PIT ë“±)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
# - í˜ì´ì§€ ë²ˆí˜¸, ì¸ìš© ë²ˆí˜¸, URLì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
# - ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´:
#   "KINACì˜ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# # ì»¨í…ìŠ¤íŠ¸
# {context}

# # ì§ˆë¬¸
# {question}

# # ë‹µë³€
# ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”. ì§ˆë¬¸ ìœ í˜•ì´ë‚˜ íŒë‹¨ ê³¼ì •ì€ ì¶œë ¥ ê¸ˆì§€."""


#     else:
#         return f"""You are "Kinagi AI", an AI assistant for KINAC (Korea Institute of Nuclear Nonproliferation And Control).

# # Answer Principles
# 1. Always use polite, professional language.
# 2. Never use emojis, slang, or internet jargon.
# 3. Be concise and clear.

# # How to Answer
# If the question is a greeting, small talk, encouragement, or everyday advice, do NOT use the context. Answer naturally and kindly in 1-3 sentences.

# If the question asks for definitions, procedures, policies, regulations, or terminology, follow these rules:
# - Use ONLY the provided context. No external knowledge.
# - Write 2-4 sentences in natural paragraphs.
# - Use bullet points (dash -) only when listing multiple procedures or steps.
# - Avoid numbered formatting like 1), 2), â‘ , â‘¡.
# - Keep original technical terms (Source material, Safeguards, PIV, PIT, etc.) as-is.
# - Do NOT include page numbers, citation numbers, or URLs.
# - If the answer cannot be found in the context:
#   "I cannot find this information in KINAC's documents."

# # Context
# {context}

# # Question
# {question}

# # Answer
# Provide only the answer. Do not mention the question type or reasoning process."""

def _build_prompt(
    context: str, 
    question: str, 
    lang: str,
    response_type: str = "short"
) -> str:
    """
    ë‹µë³€ ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        context: RAG ì»¨í…ìŠ¤íŠ¸
        question: ì‚¬ìš©ì ì§ˆë¬¸
        lang: ì–¸ì–´ (ko/en)
        response_type: short(ë‹¨ë¬¸í˜•) | long(ì¥ë¬¸í˜•)
    
    Returns:
        ëª¨ë¸ ì…ë ¥ í”„ë¡¬í”„íŠ¸
    """
    if lang == "ko":
        if response_type == "long":
            return f"""
ë‹¹ì‹ ì€ "í‚¤ë‚˜ê¸° AI"ì´ë©°, KINAC(í•œêµ­ì›ìë ¥í†µì œê¸°ìˆ ì›)ì˜ ê³µì‹ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  êµ¬ì¡°ì ì¸ ê¸°ìˆ  í•´ì„¤ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

# ë‹µë³€ í†¤ & ìŠ¤íƒ€ì¼
- KINACÂ·IAEA ë¬¸ì„œ ìŠ¤íƒ€ì¼ì„ ë”°ë¥´ëŠ” **ì •í™•í•˜ê³  ê³µì‹ì ì¸ ë¬¸ì²´**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë¬¸ì„œ í˜•ì‹ì„ ëª¨ë°©í•˜ë˜, ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ **ì¡°ì§ì Â·ì²´ê³„ì **ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
- ì›ìë ¥ ë¹„í™•ì‚°, êµ­ì œí˜‘ë ¥, Safeguards, ì ˆì°¨ ë¬¸ì„œ, ê³µì‹ ì„œì‹ ì— ì í•©í•œ ì „ë¬¸ ìš©ì–´ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
- ì´ëª¨ì§€, ì€ì–´, ê°€ë²¼ìš´ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

# ë‹µë³€ êµ¬ì„± ê·œì¹™ (í•„ìˆ˜)
ì§ˆë¬¸ì´ ì¸ì‚¬, ì•ˆë¶€, ê²©ë ¤, ì¡ë‹´, ì¼ìƒ ì¡°ì–¸ì´ë©´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³  1~3ë¬¸ì¥ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. 
ì´ë•ŒëŠ” ì•„ë˜ì˜ 4ë‹¨ êµ¬ì„± í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ì§§ì€ ëŒ€í™”ì²´ ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”.

ì§ˆë¬¸ì´ ì •ì˜, ì ˆì°¨, ì •ì±…, ê·œì •, ìš©ì–´ ì„¤ëª…ì´ë©´ ì•„ë˜ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
ì•„ë˜ì˜ 4ë‹¨ êµ¬ì„±ìœ¼ë¡œ **ìƒì„¸í•˜ê³  ì™„ê²°ëœ ë¬¸ì„œí˜• ë‹µë³€**ì„ ì‘ì„±í•˜ì„¸ìš”:

### 1) ê°œìš”(Overview)
- ì§ˆë¬¸ì˜ ì£¼ì œê°€ ë¬´ì—‡ì¸ì§€ ê°„ëµíˆ ìš”ì•½í•©ë‹ˆë‹¤.
- í•µì‹¬ ê°œë… ë˜ëŠ” ì œë„ì˜ ì·¨ì§€ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

### 2) ì£¼ìš” ë‚´ìš©(Detailed Explanation)
- ë¬¸ë§¥(Context)ì— ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ìš”ì†Œë¥¼ **5~7ë¬¸ì¥ ì´ìƒ** ìƒì„¸í•˜ê²Œ ê¸°ìˆ í•©ë‹ˆë‹¤.
- ì •ì±…Â·ê·œì •Â·ì ˆì°¨ê°€ í¬í•¨ëœ ê²½ìš°:
  - ë‹¨ê³„í˜• ì ˆì°¨ëŠ” ë²ˆí˜¸(1, 2, 3â€¦)ë¡œ ê¸°ìˆ 
  - ì¡°ê±´Â·ìš”ê±´ì€ ë¶ˆë¦¿(-)ë¡œ ì •ë¦¬
- ë¬¸ì„œ ë‚´ í‘œí˜„(Source material, Safeguards, Facility, Reporting ë“±)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
- ë™ì¼í•œ ì˜ë¯¸ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , ë…ë¦½ì  ì •ë³´ ë‹¨ìœ„ë¥¼ ì œê³µí•˜ì„¸ìš”.

### 3) ë°°ê²½ ë˜ëŠ” ê´€ë ¨ ê·œì •(Background / Relevant Provisions)
- í•„ìš”í•  ê²½ìš°, í•´ë‹¹ ì œë„ ë˜ëŠ” ì ˆì°¨ê°€ ë“±ì¥í•œ ì´ìœ (ëª©ì Â·ê·¼ê±°)ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
- ë¬¸ë§¥ ë‚´ì—ì„œ ì—°ê²°ë˜ëŠ” ë‹¤ë¥¸ ê°œë…ì´ ìˆë‹¤ë©´ í•¨ê»˜ ì–¸ê¸‰í•©ë‹ˆë‹¤.

### 4) ê²°ë¡ (Conclusion)
- í•µì‹¬ ë‚´ìš©ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°íˆ ì •ë¦¬í•©ë‹ˆë‹¤.
- ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ë¡ í•˜ì§€ ì•Šê³  ë‹¤ìŒ ë¬¸ì¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
  â€œì œê³µëœ KINAC ë¬¸ì„œì˜ ë²”ìœ„ ë‚´ì—ì„œ í™•ì¸ëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.â€

# ì •ë³´ ì‚¬ìš© ì œí•œ
- ë‹µë³€ì€ ë°˜ë“œì‹œ **ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©**í•©ë‹ˆë‹¤.
- ì™¸ë¶€ ì§€ì‹, ì¶”ì •, ë˜ëŠ” ì¼ë°˜ì ì¸ ìƒì‹ ê¸°ë°˜ í•´ì„¤ì€ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- í˜ì´ì§€ ë²ˆí˜¸, í‘œ ë²ˆí˜¸, ì¸ìš© ë²ˆí˜¸, URLì€ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ìƒê°í•˜ëŠ” ê³¼ì •ì´ë‚˜ íŒë‹¨ ì ˆì°¨ë¥¼ ì„¤ëª…í•˜ì§€ ë§ê³ , ìµœì¢… ì •ë¦¬ëœ ë‹µë³€ë§Œ ì‘ì„±í•©ë‹ˆë‹¤.

# ì»¨í…ìŠ¤íŠ¸
{context}

# ì§ˆë¬¸
{question}

# ë‹µë³€
ì•„ë˜ì˜ 4ë‹¨ êµ¬ì„± í˜•ì‹ì„ ë”°ë¼, ìƒì„¸í•˜ê³  ì²´ê³„ì ì¸ ê¸°ìˆ  ë¬¸ì„œ ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""


        else:  # short (ê¸°ì¡´ ë¡œì§)
            return f"""ë‹¹ì‹ ì€ í•œêµ­ì›ìë ¥í†µì œê¸°ìˆ ì›(KINAC)ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'í‚¤ë‚˜ê¸°AI'ì…ë‹ˆë‹¤.

# ë‹µë³€ ì›ì¹™
1. í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (~ìŠµë‹ˆë‹¤, ~í•˜ì„¸ìš” ì²´).
2. ì´ëª¨ì§€, ì€ì–´, ì¸í„°ë„· ìŠ¬ë­ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.

# ë‹µë³€ ë°©ì‹
ì§ˆë¬¸ì´ ì¸ì‚¬, ì•ˆë¶€, ê²©ë ¤, ì¡ë‹´, ì¼ìƒ ì¡°ì–¸ì´ë©´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³  1~3ë¬¸ì¥ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸ì´ ì •ì˜, ì ˆì°¨, ì •ì±…, ê·œì •, ìš©ì–´ ì„¤ëª…ì´ë©´ ì•„ë˜ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
- ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê³ , ì™¸ë¶€ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- 2~4ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ë¶ˆë¦¿(-)ì€ ì—¬ëŸ¬ ì¡°ì¹˜ë‚˜ ì ˆì°¨ë¥¼ ë‚˜ì—´í•  ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ë²ˆí˜¸(1), 2), â‘ , â‘¡ ë“±ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì›ë¬¸ ìš©ì–´(Source material, Safeguards, PIV, PIT ë“±)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
- í˜ì´ì§€ ë²ˆí˜¸, ì¸ìš© ë²ˆí˜¸, URLì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´:
  "KINACì˜ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ì»¨í…ìŠ¤íŠ¸
{context}

# ì§ˆë¬¸
{question}

# ë‹µë³€
ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”. ì§ˆë¬¸ ìœ í˜•ì´ë‚˜ íŒë‹¨ ê³¼ì •ì€ ì¶œë ¥ ê¸ˆì§€."""

    else:  # English
        if response_type == "long":
            return f"""
You are "Kinagi AI", an AI assistant for KINAC (Korea Institute of Nuclear Nonproliferation and Control). 
Your role is to provide technically accurate, well-structured explanations based strictly on the provided context.

# How to Answer
If the question is a greeting, small talk, encouragement, or everyday advice, do NOT use the context. Answer naturally and kindly in 1-3 sentences.

If the question asks for definitions, procedures, policies, regulations, or terminology, follow these rules:
- Provide a **detailed, document-style answer** structured into four clear sections as outlined below.
# Tone & Style Requirements
- Use **formal, professional English** similar to IAEA reports, safeguards technical manuals, and official correspondence.
- Maintain an objective and neutral tone appropriate for nuclear regulation and international safeguards.
- Do not use emojis, slang, conversational fillers, or overly casual expressions.

# Mandatory Answer Structure (4 Sections)
Your answer must follow the four-part structure below:

### 1) Overview
- Provide a concise summary of the topic in 2â€“3 sentences.
- Describe the purpose or relevance of the concept as presented in the context.

### 2) Detailed Explanation
- Using only the provided context, elaborate key elements in **at least 5â€“7 well-developed sentences**.
- If the document involves procedures, regulatory steps, or operational requirements:
  - Use numbered lists (â€œ1. â€¦â€, â€œ2. â€¦â€) with a space.
  - Use bullet lists (â€œ- â€¦â€) with a space for components, conditions, or parallel items.
- Preserve original technical terminology (e.g., Safeguards, Source material, Facility, PIV, PIT, Reporting obligations).
- Avoid redundancy; each sentence must provide unique information.

### 3) Background or Relevant Provisions
- Briefly explain the underlying rationale, regulatory basis, or contextual significance (2â€“3 sentences).
- Connect related concepts from the provided document when relevant.

### 4) Conclusion
- Summarize the essential points in 1â€“2 sentences.
- If information is missing from the context, explicitly state:
  â€œThis explanation is based solely on the information provided in the KINAC documents.â€

# Information Restrictions
- **Use only the provided context**. No external knowledge, assumptions, or inferred facts.
- Do not cite page numbers, URLs, figure numbers, or external references.
- Do not restructure content beyond what the context supports.
- Do not output your internal reasoning, deliberation, or step-by-step analysis; provide only the final formatted answer.

# Context
{context}

# Question
{question}

# Answer
Provide a detailed, well-structured answer following the four-part format above.
"""

        else:  # short (ê¸°ì¡´ ë¡œì§)
            return f"""You are "Kinagi AI", an AI assistant for KINAC (Korea Institute of Nuclear Nonproliferation And Control).

# Answer Principles
1. Always use polite, professional language.
2. Never use emojis, slang, or internet jargon.
3. Be concise and clear.

# How to Answer
If the question is a greeting, small talk, encouragement, or everyday advice, do NOT use the context. Answer naturally and kindly in 1-3 sentences.

If the question asks for definitions, procedures, policies, regulations, or terminology, follow these rules:
- Use ONLY the provided context. No external knowledge.
- Write 2-4 sentences in natural paragraphs.
- Use bullet points (dash -) only when listing multiple procedures or steps.
- Avoid numbered formatting like 1), 2), â‘ , â‘¡.
- Keep original technical terms (Source material, Safeguards, PIV, PIT, etc.) as-is.
- Do NOT include page numbers, citation numbers, or URLs.
- If the answer cannot be found in the context:
  "I cannot find this information in KINAC's documents."

# Context
{context}

# Question
{question}

# Answer
Provide only the answer. Do not mention the question type or reasoning process."""

# ---- ko -> en ë²ˆì—­ê¸° (ë¡œì»¬ vLLM ì‚¬ìš©) ---------------------------------------
# ON/OFF í† ê¸€: RAG_TRANSLATE_QUERY=1 (default 1)
USE_Q_TRANSL = os.getenv("RAG_TRANSLATE_QUERY", "1").strip() != "0"
# íƒ€ì„ì•„ì›ƒ(ì´ˆ): ë¬´í•œ ëŒ€ê¸° ë°©ì§€
TRANSLATE_TIMEOUT = float(os.getenv("RAG_TRANSLATE_TIMEOUT", "8.0"))


def _has_hangul(s: str) -> bool:
    return any('\uac00' <= ch <= '\ud7a3' for ch in s or "")

@functools.lru_cache(maxsize=512)
def _cached_ko_to_en(text: str) -> str:
    return _ko_to_en_call(text)

def _ko_to_en_call(text: str) -> str:
    """
    í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­ (RAG ê²€ìƒ‰ìš©)
    - ì§ˆë¬¸ì˜ ì˜ë„ì™€ í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´
    - êµ¬ì–´ì²´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€í™˜
    - ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì˜ë¯¸ ì¤‘ì‹¬ ë²ˆì—­
    """
    try:
        client = get_openai_client()
        
        # ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        sys = """You are a professional translator specialized in converting Korean queries to English for document retrieval.

CRITICAL RULES:
1. Preserve the INTENT of the question (interrogative â†’ interrogative, statement â†’ statement)
2. Convert colloquial/informal Korean to formal search-friendly English
3. Keep technical terms and proper nouns intact
4. Output ONLY the English translation - no quotes, no explanations
5. For questions: MUST use question words (What, How, Why, When, Where, Which, etc.)
6. For incomplete/casual speech: infer the complete meaning

Examples:
- "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?" â†’ "What is the weather today?"
- "ì™œ ë°˜ë§í•´?" â†’ "Why are you speaking informally?"
- "ì™œ ì¤‘êµ­ì–´ í•´ ë„ˆ ì¤‘êµ­ì–´ ì˜í•´?" â†’ "Why are you speaking Chinese? Are you good at Chinese?"
- "ì œ57ì¡°ê°€ ë­ì•¼?" â†’ "What is Article 57?"
- "PIV ì ˆì°¨ ì•Œë ¤ì¤˜" â†’ "What is the PIV procedure?"
- "ì´ê±° ì–´ë–»ê²Œ í•´?" â†’ "How do I do this?"
"""
        
        st = pytime.time()
        resp = client.chat.completions.create(
            model=os.getenv("DEFAULT_MODEL_ALIAS", "qwen2.5-14b"),
            messages=[
                {"role": "system", "content": sys},
                {"role": "user", "content": text}
            ],
            temperature=0.1,  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš© (0.0 â†’ 0.1)
            max_tokens=256,
        )
        
        if pytime.time() - st > TRANSLATE_TIMEOUT:
            raise TimeoutError("translate timeout")
        
        out = (resp.choices[0].message.content or "").strip()
        
        # ë”°ì˜´í‘œ ì œê±°
        out = out.strip('"\'`')
        
        # í•œê¸€ì´ ì„ì—¬ ìˆìœ¼ë©´ í•œ ë²ˆ ë” ì‹œë„
        if _has_hangul(out):
            sys2 = """Translate to English for document search. 
Output ONLY pure ASCII English. 
NO Korean letters. NO quotes. NO explanations.
Preserve question format if input is a question."""
            
            resp2 = client.chat.completions.create(
                model=os.getenv("DEFAULT_MODEL_ALIAS", "qwen2.5-14b"),
                messages=[
                    {"role": "system", "content": sys2},
                    {"role": "user", "content": f"Translate this Korean to English: {text}"}
                ],
                temperature=0.0,
                max_tokens=256,
            )
            out = (resp2.choices[0].message.content or "").strip().strip('"\'`')
        
        # ì—¬ì „íˆ í•œê¸€ì´ ìˆê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ fallback
        if not out or _has_hangul(out):
            logger.warning("[ask] translation contains Hangul or empty; fallback to original")
            return text
        
        return out
        
    except Exception as e:
        logger.warning(f"[ask] translate failed: {e}")
        return text


def _maybe_translate_query_for_search(question: str, lang: str) -> str:
    """
    ê²€ìƒ‰ìš© ì¿¼ë¦¬ ì „ì²˜ë¦¬:
    1. normalize_queryë¡œ ì •ì˜í˜• ì§ˆë¬¸ ë³´ê°•
    2. í•œêµ­ ë²•ì‹ í‘œê¸° ë³´ì • (ì œ N ì¡° â†’ Article N)
    3. í•œêµ­ì–´ë©´ ì˜ì–´ë¡œ ë²ˆì—­
    """
    q = normalize_query(question)
    
    if not USE_Q_TRANSL or lang != "ko":
        return q
    
    # í•œêµ­ ë²•ì‹ "ì œ 12 ì¡°" â†’ ì˜ì–´ "Article 12" ë³´ì • (ë²ˆì—­ ì „ ì ìš©)
    q = re.sub(r"ì œ\s*(\d+)\s*ì¡°", r"Article \1", q)
    
    # ìºì‹œëœ ë²ˆì—­ ì‚¬ìš©
    translated = _cached_ko_to_en(q)
    
    # ë²ˆì—­ ê²°ê³¼ ë¡œê¹… (ë””ë²„ê¹…ìš©)
    if translated != q:
        logger.debug(f"[translate] {q[:50]} â†’ {translated[:50]}")
    
    return translated
# ---------- Routes ----------
@router.get("/test")
def test():
    return {"status": "LLaMA router is working"}

@router.post("/generate")
def generate(body: GenerateReq):
    try:
        result = generate_answer_unified(body.prompt, body.model_name)
        return {"response": result}
    except Exception as e:
        raise HTTPException(500, f"ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")

@router.post("/upload", response_model=UploadResp)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    mode: str = Query("version", regex="^(skip|version|replace)$"),
):
    # 0) ì—…ë¡œë“œ ì›ë³¸ì„ ë©”ëª¨ë¦¬ë¡œ ì½ìŒ
    safe_name = os.path.basename(file.filename or "upload.bin")
    orig_ct = file.content_type or mimetypes.guess_type(safe_name)[0] or "application/octet-stream"
    content = await file.read()
    if not content:
        raise HTTPException(400, "ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤.")

    m = MinIOStore()

    # 1) ë¹„-PDF â†’ PDF bytes ë³€í™˜ (DOC_CONVERTER_URL ìˆìœ¼ë©´ ìŠ¤íŠ¸ë¦¼ ë³€í™˜, ì—†ìœ¼ë©´ ì„ì‹œíŒŒì¼-í´ë°±)
    src_ext = os.path.splitext(safe_name)[1].lower()
    # 1ìˆœìœ„: Gotenberg ë°”ì´íŠ¸ ë³€í™˜ (ì™„ì „ ë¬´-ë””ìŠ¤í¬)
    pdf_bytes: bytes | None = None
    if src_ext == ".pdf":
        pdf_bytes = content
    else:
        pdf_bytes = convert_bytes_to_pdf_bytes(content, src_ext)
    
    # 2ìˆœìœ„: ì‚¬ë‚´/ì™¸ë¶€ ì»¨ë²„í„° (DOC_CONVERTER_URL), ìˆìœ¼ë©´ ì‚¬ìš©
    if pdf_bytes is None and src_ext != ".pdf":
        try:
            pdf_bytes = convert_stream_to_pdf_bytes(content, src_ext)
        except Exception:
            pdf_bytes = None
    
    # 3ìˆœìœ„: ì„ì‹œí´ë” í´ë°± (convert_to_pdf) â†’ ë³€í™˜ í›„ ì¦‰ì‹œ ì‚­ì œ
    if pdf_bytes is None:
        with tempfile.TemporaryDirectory() as td:
            src_path = os.path.join(td, safe_name)
            with open(src_path, "wb") as f:
                f.write(content)
            out_path = convert_to_pdf(src_path)
            with open(out_path, "rb") as f:
                pdf_bytes = f.read()
    # 2) í•´ì‹œ/ì¤‘ë³µíŒì •
    pdf_filename = safe_name if src_ext == ".pdf" else (os.path.splitext(safe_name)[0] + ".pdf")
    pdf_sha = _sha256_bytes(pdf_bytes)
    hash_flag_key = f"uploaded/__hash__/sha256/{pdf_sha}.flag"
    object_pdf = f"uploaded/{pdf_filename}"
    uploaded = True
    duplicate_reason = None

    if m.exists(hash_flag_key):
        uploaded = False
        duplicate_reason = "same_content_hash"

    if uploaded and m.exists(object_pdf):
        try:
            remote_size = m.size(object_pdf)
        except Exception:
            remote_size = -1
        local_size = len(pdf_bytes)
        if remote_size == local_size and remote_size > -1:
            uploaded = False
            duplicate_reason = (duplicate_reason or "same_name_and_size")
        else:
            if mode == "replace":
                m.upload_bytes(pdf_bytes, object_name=object_pdf, content_type="application/pdf", length=len(pdf_bytes))
            else:
                object_pdf = f"uploaded/{uuid.uuid4().hex}_{pdf_filename}"
                m.upload_bytes(pdf_bytes, object_name=object_pdf, content_type="application/pdf", length=len(pdf_bytes))
    elif uploaded:
        m.upload_bytes(pdf_bytes, object_name=object_pdf, content_type="application/pdf", length=len(pdf_bytes))

    # 2-1) í•´ì‹œ í”Œë˜ê·¸(ë°”ì´íŠ¸ ì§ì—…ë¡œë“œ: ë¡œì»¬ íŒŒì¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    try:
        if uploaded and not m.exists(hash_flag_key):
            m.upload_bytes(b"1", object_name=hash_flag_key, content_type="text/plain", length=1)
    except Exception as e:
        # ì¹˜ëª…ì  ì•„ë‹˜
        print(f"[UPLOAD] warn: failed to write hash flag: {e}")

    # 3) doc_id ê²°ì • ë° â€˜ì›ë³¸â€™ ë°”ì´íŠ¸ ì—…ë¡œë“œ (ë¬¸ì„œë³„ í´ë”ë¡œ)
    doc_id = os.path.splitext(os.path.basename(object_pdf))[0]
    object_orig = f"uploaded/originals/{doc_id}/{safe_name}"
    if m.exists(object_orig):
        try:
            rsize = m.size(object_orig)
        except Exception:
            rsize = -1
        if rsize != len(content):
            object_orig = f"uploaded/originals/{doc_id}/{uuid.uuid4().hex}_{safe_name}"

    m.upload_bytes(content, object_name=object_orig, content_type=orig_ct, length=len(content))
    is_pdf_original = (src_ext == ".pdf")
    # 4) ë§¤í•‘ ë©”íƒ€ JSON
    try:
        meta = {
            "doc_id": doc_id,
            "title": safe_name,                # ë³´ê¸°ìš©
            "pdf_key": object_pdf,             # â† í‚¤ ì´ë¦„ì„ pdf_keyë¡œ í†µì¼
            "original_key": object_orig,       # â† original_key í†µì¼
            "original_name": safe_name,
            "is_pdf_original": is_pdf_original,
            "sha256": pdf_sha,
            # ì—…ë¡œë“œ ì‹œê°„ì€ UTCë¡œ ì €ì¥(í”„ëŸ°íŠ¸ì—ì„œ KSTë¡œ ë Œë” ì¶”ì²œ)
            "uploaded_at": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
            "mode": mode,
            # ì•„ì§ ëª¨ë¦„ â†’ ë‚˜ì¤‘ì— ì¸ë±ì„œê°€ ë®ì–´ì”€
            # "chunk_count": null
        }
        m.put_json(meta_key(doc_id), meta)
    except Exception as e:
        print(f"[UPLOAD] warn: failed to write meta json: {e}")

    # 5) ë°±ê·¸ë¼ìš´ë“œ ì¸ë±ì‹± (MinIO bytes ê²½ë¡œë§Œ ë„˜ê¹€)
    job_id = uuid.uuid4().hex
    job_state.start(job_id, doc_id=doc_id, minio_object=object_pdf)
    job_state.update(
        job_id,
        status="uploaded",
        step="minio:ok",
        filename=safe_name,
        progress=10,
        mode=mode,
        content_sha256=pdf_sha,
        duplicate_reason=duplicate_reason,
        uploaded=uploaded,
    )
    background_tasks.add_task(index_pdf_to_milvus, job_id, None, object_pdf, uploaded, False, doc_id)

    return UploadResp(filename=safe_name, minio_object=object_pdf, indexed="background", job_id=job_id)

@router.post("/ask", response_model=AskResp)
def ask_question(req: AskReq):
    try:
        # ğŸ†• response_typeì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        mode_config = RESPONSE_MODE_CONFIG.get(req.response_type, RESPONSE_MODE_CONFIG["short"])
        max_tokens = mode_config["max_tokens"]
        temperature = mode_config["temperature"]
        top_p = mode_config["top_p"]
        
        logger.info(f"[ask] response_type={req.response_type}, max_tokens={max_tokens}")

        # ========== ê¸°ì¡´ ë¡œì§ ì‹œì‘ (ìˆ˜ì • ì—†ìŒ) ==========
        
        # 0) ëª¨ë¸/ìŠ¤í† ì–´ ì¤€ë¹„
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())

        # ì–¸ì–´ ê°ì§€(ko/en)
        lang = _detect_lang(req.question)

        # 1) ê²€ìƒ‰ìš© ì§ˆì˜ ì¤€ë¹„(í•œêµ­ì–´ë©´ ko->en ë³€í™˜)
        query_for_search = _maybe_translate_query_for_search(req.question, lang)
        logger.info("[ask] lang=%s | q_before=%s", lang, req.question[:120])
        logger.info("[ask] q_search=%s", query_for_search[:120])

        # ì´ˆê¸° ë„‰ë„‰íˆ ê²€ìƒ‰
        raw_topk = max(40, req.top_k * 6)
        
        # ì„ íƒ ë¬¸ì„œê°€ ìˆì„ ê²½ìš° doc í•„í„°ë¥¼ ê±¸ì–´ì„œ ê²€ìƒ‰í•˜ëŠ” í—¬í¼
        def _search_with_optional_filter(q: str, topk: int):
            if not q:
                return []
            if req.doc_ids:
                logger.info("[ask] doc filter enabled: %d docs", len(req.doc_ids))
                return store.search_in_docs(
                    query=q,
                    embed_fn=embed,
                    doc_ids=req.doc_ids,
                    topk=topk,
                )
            return store.search(
                query=q,
                embed_fn=embed,
                topk=topk,
            )
        
        def _dedup_key(c):
            return (c.get("doc_id"), c.get("page"), _strip_meta_line(c.get("chunk",""))[:80])

        def _merge_dedup(a, b):
            seen, out = set(), []
            for x in (a + b):
                k = _dedup_key(x)
                if k in seen: 
                    continue
                seen.add(k); out.append(x)
            return out

        if lang == "ko" and _has_hangul(query_for_search):
            # ë²ˆì—­ ì‹¤íŒ¨ë¡œ íŒë‹¨ â†’ í•œ/ì˜ ì–‘ë°©í–¥ ê²€ìƒ‰
            logger.warning(
                "[ask] q_search still contains Hangul; doing bilingual search fallback"
            )
            cands_ko = _search_with_optional_filter(
                normalize_query(req.question),
                raw_topk // 2,
            )
            forced_en = _cached_ko_to_en(normalize_query(req.question))
            cands_en = _search_with_optional_filter(
                forced_en,
                raw_topk // 2,
            )
            cands = _merge_dedup(cands_ko, cands_en)
        else:
            cands = _search_with_optional_filter(query_for_search, raw_topk)

        logger.info("[ask] cands=%d (raw_topk=%d)", len(cands), raw_topk)

        if not cands:
            return AskResp(
                answer=_t(lang,
                          "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œê°€ ì˜¬ë°”ë¥´ê²Œ ì¸ë±ì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
                          "No relevant content was found in the uploaded documents. Please verify the documents were indexed correctly."),
                used_chunks=0,
                sources=[]
            )

        # 2) í‚¤ì›Œë“œ ë¶€ìŠ¤íŠ¸ (ì§ˆë¬¸ ì›ë¬¸ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        kws = extract_keywords(req.question)
        def _kw_boost_score(c: dict) -> int:
            txt = _strip_meta_line(c.get("chunk", "")).lower()
            return sum(1 for k in kws if k.lower() in txt)
        for c in cands:
            c["kw_boost"] = _kw_boost_score(c)

        # ì¡°í•­ ê²€ìƒ‰ ë¶€ìŠ¤íŠ¸
        ARTICLE_BOOST = float(os.getenv("RAG_ARTICLE_BOOST", "2.5"))
        if lang == "ko":
            m = re.search(r"ì œ\s*(\d+)\s*ì¡°", req.question)
            if m:
                art = m.group(1)
                patt = re.compile(rf"ì œ\s*{art}\s*ì¡°")
                for c in cands:
                    sec = c.get("section") or ""
                    txt = c.get("chunk") or ""
                    if patt.search(sec) or patt.search(txt):
                        c["kw_boost"] = c.get("kw_boost", 0.0) + ARTICLE_BOOST

        cands.sort(key=lambda x: (x.get("kw_boost", 0), x.get("score", 0.0)), reverse=True)
        rerank_pool = cands[:max(30, req.top_k * 6)]

        # ì„ê³„ê°’ ì„¤ì •
        THRESH = float(os.getenv("RAG_SCORE_THRESHOLD", "0.1"))

        def _is_confident(hit: dict, thr: float) -> bool:
            re_s = hit.get("re_score")
            emb_s = hit.get("score", 0.0)
            if re_s is not None:
                return (re_s >= thr) or (emb_s >= float(os.getenv("RAG_EMB_BACKUP_THR", "0.28")))
            return emb_s >= float(os.getenv("RAG_EMB_BACKUP_THR", "0.28"))

        # Rerank
        topk = rerank(query_for_search, rerank_pool, top_k=req.top_k)
        if not topk:
            return AskResp(
                answer=_t(lang,
                          "ë¬¸ì„œì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                          "Could not find sufficiently reliable supporting content in the documents."),
                used_chunks=0,
                sources=[]
            )

        # ì„ê³„ê°’ í•„í„°ë§
        filtered_topk = []
        for c in topk:
            if _is_confident(c, THRESH):
                filtered_topk.append(c)
        
        # ë¡œê·¸
        try:
            dbg = [(x.get("re_score"), x.get("score")) for x in topk[:3]]
            logger.info("[ask] rerank-top3 (re,emb)=%s | filtered=%d/%d | THRESH=%.3f",
                        dbg, len(filtered_topk), len(topk), THRESH)
        except Exception as e:
            logger.warning("[ask] debug summarize failed: %s", e)
        
        if not filtered_topk:
            try:
                dbg = [(x.get("re_score"), x.get("score")) for x in topk[:3]]
                logger.info("[ask] LOW-CONF; all chunks filtered | top3 (re,emb)=%s | q_search=%s",
                    dbg, query_for_search[:120])
            except Exception as e:
                logger.warning(f"[ask] debug print failed: {e}")
            return AskResp(
                answer=_t(lang,
                          "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ í™•ì‹¤í•œ ë‹µë³€ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
                          "It is hard to find a definitive answer to this question in the documents."),
                used_chunks=0,
                sources=[]
            )

        # 5) ì»¨í…ìŠ¤íŠ¸/ì¶œì²˜ êµ¬ì„±
        context_lines = []
        sources = []
        for i, c in enumerate(filtered_topk, 1):
            sec = (c.get("section") or "").strip()
            chunk_body = _strip_meta_line(c.get("chunk", ""))
            context_lines.append(f"[{i}] (doc:{c['doc_id']} p.{c['page']})\n{chunk_body}")
            sources.append({
                "id": i,
                "doc_id": c.get("doc_id"),
                "page": c.get("page"),
                "section": c.get("section"),
                "chunk": c.get("chunk"),
                "score": c.get("re_score", c.get("score")),
            })
        context = "\n\n".join(context_lines)

        # ========== ê¸°ì¡´ ë¡œì§ ë ==========

        # 6) í”„ë¡¬í”„íŠ¸ ìƒì„± (ğŸ†• response_type ë°˜ì˜)
        prompt = _build_prompt(
            context=context,
            question=req.question,
            lang=lang,
            response_type=req.response_type
        )

        # 7) ëª¨ë¸ í˜¸ì¶œ (ğŸ†• íŒŒë¼ë¯¸í„° ì „ë‹¬)
        answer = generate_answer_unified(
            prompt=prompt,
            name_or_id=req.model_name,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p          
        )
        
        answer = _clean_repetitive_answer(answer)

        return AskResp(
            answer=answer,
            used_chunks=len(filtered_topk),
            sources=sources
        )

    except HTTPException:
        raise
    except RuntimeError as milvus_error:
        lang = _detect_lang(getattr(req, "question", "") or "")
        raise HTTPException(503, _t(lang,
                                    f"Milvus ì—°ê²° ëŒ€ê¸°/ê²€ìƒ‰ ì‹¤íŒ¨: {milvus_error}",
                                    f"Milvus connection/search failed: {milvus_error}"))
    except Exception as e:
        lang = _detect_lang(getattr(req, "question", "") or "")
        logger.error(f"[ask] Error: {e}", exc_info=True)
        raise HTTPException(500, _t(lang,
                                    f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}",
                                    f"Error while processing the query: {e}"))


def _clean_repetitive_answer(answer: str) -> str:
    """ë°˜ë³µë˜ëŠ” ë‹µë³€ íŒ¨í„´ì„ ì •ë¦¬"""
    if not answer:
        return answer
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = answer.split('.')
    unique_sentences = []
    seen_content = set()
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:
            continue
            
        # ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•´ì„œ ì¤‘ë³µ í™•ì¸
        keywords = set(re.findall(r'[ê°€-í£]{2,}|[A-Za-z]{3,}', sentence))
        content_hash = frozenset(keywords)
        
        # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        is_duplicate = False
        for seen in seen_content:
            overlap = len(content_hash & seen)
            similarity = overlap / max(len(content_hash), len(seen)) if content_hash and seen else 0
            if similarity > 0.7:
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_sentences.append(sentence)
            seen_content.add(content_hash)
    
    return '. '.join(unique_sentences[:5]) + '.' if unique_sentences else answer
# ---------- Job State Management ----------
@router.get("/job/{job_id}")
def get_job(job_id: str):
    st = job_state.get(job_id)
    if not st:
        raise HTTPException(404, "í•´ë‹¹ job_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return st

@router.get("/jobs")
def list_jobs(status: Optional[str] = Query(None), limit: int = Query(50, ge=1, le=500)):
    return {"jobs": job_state.list_jobs(status=status, limit=limit)}

@router.get("/doc/{doc_id}")
def doc_status(doc_id: str):
    s = MinIOStore()

    # 1) ë©”íƒ€ ìš°ì„  (ì‹ ê·œ ê²½ë¡œ)
    try:
        if s.exists(meta_key(doc_id)):
            meta = s.get_json(meta_key(doc_id))
        elif s.exists(legacy_meta_key(doc_id)):  # êµ¬ë²„ì „ íŒŒì¼í˜• í´ë°±
            meta = s.get_json(legacy_meta_key(doc_id))
        else:
            meta = None
    except Exception:
        meta = None

    if isinstance(meta, dict):
        # í‚¤ í˜¸í™˜(pdf/pdf_key, original/original_key ë“±) ì²˜ë¦¬
        chunk_count = meta.get("chunk_count")
        if isinstance(chunk_count, int):
            return {"doc_id": doc_id, "chunks": chunk_count, "indexed": chunk_count > 0}

    # 2) í´ë°±: Milvusì—ì„œ ì„¸ê³  ë©”íƒ€ì— ìºì‹œ
    try:
        model = get_embedding_model()
        m = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        total = m.count_by_doc(doc_id)

        # ë©”íƒ€ì— ìºì‹œ
        try:
            meta = (s.get_json(meta_key(doc_id)) if s.exists(meta_key(doc_id)) else {}) or {}
            meta["doc_id"] = doc_id
            meta["chunk_count"] = int(total)
            s.put_json(meta_key(doc_id), meta)
        except Exception:
            pass

        return {"doc_id": doc_id, "chunks": total, "indexed": total > 0}
    except Exception as e:
        raise HTTPException(500, f"doc status ì‹¤íŒ¨: {e}")


# ========== SSE Stream for Job Status =========
@router.get("/job/{job_id}/stream")
async def stream_job(job_id: str):
    async def event_gen():
        last_serialized = None
        while True:
            st = job_state.get(job_id)
            if not st:
                yield {"event": "error", "data": json.dumps({"error": "not found"}, ensure_ascii=False)}
                break

            data = json.dumps(st, ensure_ascii=False)
            if data != last_serialized:
                yield {"event": "update", "data": data}
                last_serialized = data

                if st.get("status") in ("done", "error"):
                    break

            await asyncio.sleep(1)

    return EventSourceResponse(event_gen())

# ---------- MinIO Utilities ----------

    
@router.get("/files")
def list_files(prefix: str = "uploaded/", include_internal: bool = False, only_pdf: bool = False):
    m = MinIOStore()
    try:
        keys = m.list_files(prefix=prefix) 
    except Exception as e:
        raise HTTPException(500, f"MinIO íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # ë‚´ë¶€ ê´€ë¦¬ ì˜¤ë¸Œì íŠ¸ ìˆ¨ê¸°ê¸° (ì›í•˜ë©´ include_internal=Trueë¡œ ë…¸ì¶œ)
    if not include_internal:
        keys = [k for k in keys if not (k.endswith(".flag") or "/__hash__/" in k or "/__meta__/" in k)]

    if only_pdf:
        keys = [k for k in keys if k.lower().endswith(".pdf")]

    return {"files": keys}


@router.get("/file/{object_name:path}")
def get_file_presigned(
    object_name: str,
    minutes: int = Query(60, ge=1, le=7*24*60),
    download_name: Optional[str] = None,
    inline: bool = False,  # trueë©´ inline, falseë©´ attachment
):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    try:
        if inline:
            url = m.presign_view(key, filename=download_name, ttl_seconds=minutes * 60)
        else:
            url = m.presign_download(key, filename=download_name, ttl_seconds=minutes * 60)
        return {"url": url}
    except Exception as e:
        raise HTTPException(500, f"presign failed: {e}")

# ê¸°ì¡´
# @router.get("/docs")
# def list_docs():

@router.get("/rag/docs")
def list_docs():
    m = MinIOStore()
    try:
        all_keys = m.list_files("uploaded/")
    except Exception as e:
        raise HTTPException(500, f"minio list failed: {e}")

    def is_internal(k: str) -> bool:
        return (
            k.endswith(".flag")
            or "/__hash__/" in k
            or "/__meta__/" in k
            or k.startswith("uploaded/originals/")
        )

    pdf_keys = [k for k in all_keys if not is_internal(k) and k.lower().endswith(".pdf")]

    items = []
    for k in pdf_keys:
        base = os.path.basename(k)
        doc_id = os.path.splitext(base)[0]

        meta = None
        try:
            if m.exists(meta_key(doc_id)):
                meta = m.get_json(meta_key(doc_id))
            elif m.exists(legacy_meta_key(doc_id)):
                meta = m.get_json(legacy_meta_key(doc_id))
        except Exception:
            meta = None

        # ë©”íƒ€ì—ì„œ ì›ë³¸ ì •ë³´ êº¼ë‚¼ ë•Œ í‚¤ í˜¸í™˜
        original_key = None
        original_name = None
        uploaded_at = None
        if isinstance(meta, dict):
            original_key = meta.get("original_key") or meta.get("original")
            original_name = meta.get("original_name")
            uploaded_at = meta.get("uploaded_at")

        def _resolve_original() -> Optional[str]:
            if original_key and m.exists(original_key):
                return original_key
            if original_name:
                cand1 = f"uploaded/originals/{doc_id}/{original_name}"
                if m.exists(cand1):
                    return cand1
                cand2 = f"uploaded/originals/{original_name}"
                if m.exists(cand2):
                    return cand2
            cands = m.list_files(f"uploaded/originals/{doc_id}/")
            if cands:
                return cands[0]
            return None

        resolved_orig = _resolve_original()
        if resolved_orig:
            original_key = resolved_orig
            if not original_name:
                original_name = os.path.basename(resolved_orig)

        title = (isinstance(meta, dict) and meta.get("title")) or original_name or base
        is_pdf_original = bool(original_key and original_key.lower().endswith(".pdf"))

        items.append({
            "doc_id": doc_id,
            "title": title,
            "object_key": k,                  # ë³€í™˜ PDF
            "original_key": original_key,     # ì›ë³¸ (ìˆìœ¼ë©´)
            "original_name": original_name,
            "is_pdf_original": is_pdf_original,
            "uploaded_at": uploaded_at,
        })

    return {"docs": items}

@router.get("/rag/meta/{doc_id}")
def get_meta(doc_id: str):
    m = MinIOStore()
    if m.exists(meta_key(doc_id)):
        return m.get_json(meta_key(doc_id))
    if m.exists(legacy_meta_key(doc_id)):
        return m.get_json(legacy_meta_key(doc_id))
    raise HTTPException(404, f"meta not found for {doc_id}")

@router.get("/status")
def status():
    m = MinIOStore()
    try:
        keys = m.list_files("uploaded/")
        pdfs = [k for k in keys if k.lower().endswith(".pdf") and "/__hash__/" not in k and "/__meta__/" not in k]
        return {"has_data": len(pdfs) > 0, "doc_count": len(pdfs)}
    except Exception:
        return {"has_data": False, "doc_count": 0}


@router.get("/view/alias/{filename:path}")
def view_object_alias(filename: str, src: str):
    """
    URLì´ ì›í•˜ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ëë‚˜ë„ë¡ ë§Œë“œëŠ” alias ë·°ì–´ ì—”ë“œí¬ì¸íŠ¸.
    ì˜ˆ: /view/alias/ì›í•˜ëŠ”ì´ë¦„.pdf?src=uploaded/53.pdf
    """
    key = unquote(src)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    # í‘œì‹œ/ë‹¤ìš´ë¡œë“œ ëª¨ë‘ ë™ì¼í•˜ê²Œ ë³´ì´ë„ë¡ inline + filename ì§€ì •
    media = "application/pdf"
    try:
        obj = m.client.get_object(m.bucket, key)
    except Exception as e:
        raise HTTPException(500, f"MinIO get_object failed: {e}")

    headers = {
        "Content-Disposition": _content_disposition("inline", filename),
        "Content-Type": media,
    }

    def _iter():
        try:
            for chunk in obj.stream(32 * 1024):
                yield chunk
        finally:
            obj.close()
            obj.release_conn()

    return StreamingResponse(_iter(), media_type=media, headers=headers)

@router.delete("/file/{object_name}")
def delete_file(object_name: str):
    try:
        minio = MinIOStore()
        if not minio.exists(object_name):
            raise HTTPException(404, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        minio.delete(object_name)
        return {"status": "ok", "deleted": object_name}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
@router.get("/view/{object_name:path}")
def view_object(object_name: str, name: Optional[str] = None):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    disp_name = name or os.path.basename(key)
    ext = os.path.splitext(key)[1].lower()
    media = "application/pdf" if ext == ".pdf" else (mimetypes.guess_type(disp_name)[0] or "application/octet-stream")

    try:
        obj = m.client.get_object(m.bucket, key)
    except Exception as e:
        raise HTTPException(500, f"MinIO get_object failed: {e}")

    headers = {
        # ğŸ‘‡ latin-1 ì•ˆì „í•˜ê²Œ
        "Content-Disposition": _content_disposition("inline", disp_name)
    }

    def _iter():
        try:
            for chunk in obj.stream(32 * 1024):
                yield chunk
        finally:
            obj.close()
            obj.release_conn()

    return StreamingResponse(_iter(), media_type=media, headers=headers)


@router.get("/download/{object_name:path}")
def download_object(object_name: str, name: Optional[str] = None):
    key = unquote(object_name)
    m = MinIOStore()
    if not m.exists(key):
        raise HTTPException(404, f"object not found: {key}")

    disp_name = name or os.path.basename(key)
    media = mimetypes.guess_type(disp_name)[0] or "application/octet-stream"

    try:
        obj = m.client.get_object(m.bucket, key)
    except Exception as e:
        raise HTTPException(500, f"MinIO get_object failed: {e}")

    headers = {
        # ğŸ‘‡ latin-1 ì•ˆì „í•˜ê²Œ
        "Content-Disposition": _content_disposition("attachment", disp_name)
    }

    def _iter():
        try:
            for chunk in obj.stream(32 * 1024):
                yield chunk
        finally:
            obj.close()
            obj.release_conn()

    return StreamingResponse(_iter(), media_type=media, headers=headers)

# ---------- Bulk delete MinIO files under a prefix ----------
@router.delete("/files/purge", tags=["llama"])
def purge_files(
    prefix: str = Query("uploaded/", description="ì§€ìš¸ ê²½ë¡œ prefix (ë°˜ë“œì‹œ 'uploaded/'ë¡œ ì‹œì‘)"),
    dry_run: bool = Query(False, description="trueë©´ ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ëª©ë¡ë§Œ ë°˜í™˜"),
    limit_preview: int = Query(50, ge=1, le=500, description="dry_run ë•Œ ë¯¸ë¦¬ë³´ê¸° ìµœëŒ€ ê°œìˆ˜"),
):
    """
    MinIOì—ì„œ íŠ¹ì • prefix í•˜ìœ„ ê°ì²´ë“¤ì„ ì¼ê´„ ì‚­ì œ.
    - ì•ˆì „ì¥ì¹˜: prefixê°€ 'uploaded/'ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ 400 ì—ëŸ¬
    - dry_run=True ë©´ ì‚­ì œ ì—†ì´ ëª©ë¡ ë¯¸ë¦¬ë³´ê¸°ë§Œ
    """
    if not prefix or not prefix.startswith("uploaded/"):
        raise HTTPException(400, "prefixëŠ” ë°˜ë“œì‹œ 'uploaded/'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")

    try:
        minio = MinIOStore()
        files = minio.list_files(prefix=prefix)
    except Exception as e:
        raise HTTPException(500, f"MinIO ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    matched = len(files)
    if dry_run:
        preview = files[:limit_preview]
        more = max(0, matched - len(preview))
        return {"status": "dry-run", "prefix": prefix, "matched": matched, "preview": preview, "more": more}

    deleted = 0
    failed = 0
    errors = []
    for obj in files:
        try:
            minio.delete(obj)
            deleted += 1
        except Exception as e:
            failed += 1
            errors.append({"object": obj, "error": str(e)})

    return {"status": "ok", "prefix": prefix, "matched": matched, "deleted": deleted, "failed": failed, "errors": errors}

# ========= Debug / Inspection =========
from pymilvus import connections, Collection, utility

@router.get("/milvus/info",tags=["milvus"])
def milvus_info():
    try:
        col_name = os.getenv("MILVUS_COLLECTION", "rag_chunks_v2")
        connections.connect("default", host=os.getenv("MILVUS_HOST", "milvus"), port=os.getenv("MILVUS_PORT", "19530"))

        if not utility.has_collection(col_name):
            return {"collection": col_name, "exists": False, "num_entities": 0, "indexes": [], "schema_fields": []}

        col = Collection(col_name)
        col.load()  # âœ… ê°•ì œ ë¡œë“œ (peekì—ì„œ release ë˜ì–´ë„ ë‹¤ì‹œ ë¡œë“œ)
        info = {
            "collection": col_name,
            "exists": True,
            "num_entities": col.num_entities,
            "indexes": col.indexes,
            "schema_fields": [f.name for f in col.schema.fields],
        }
        return info
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Milvus info ì¡°íšŒ ì‹¤íŒ¨: {e}")


@router.get("/debug/milvus/peek",tags=["milvus"])
def debug_milvus_peek(limit: int = 100, full: bool = True, max_chars:int|None = None):
    """ Milvus ì»¬ë ‰ì…˜ì˜ ì¼ë¶€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° """
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        if full:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = "0"
        elif max_chars is not None:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = str(max_chars)
        return {"items": store.peek(limit=limit)}
    except Exception as e:
        raise HTTPException(500, f"Milvus peek ì‹¤íŒ¨: {e}")

@router.get("/debug/milvus/by-doc",tags=["milvus"])
def debug_milvus_by_doc(
    doc_id: str,
    limit: int = 100,
    full: bool = False,
    max_chars: int | None = None
):
    items: list = []            # ë¯¸ë¦¬ ì´ˆê¸°í™” (UnboundLocalError ë°©ì§€)
    total: int | None = None

    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())

        # ê¸¸ì´ íŠ¸ë ì¼€ì´ì…˜ ì œì–´
        if full or max_chars == 0:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = "0"
        elif max_chars is not None:
            os.environ["DEBUG_PEEK_MAX_CHARS"] = str(max_chars)

        # ë°ì´í„° ì¡°íšŒ
        items = store.query_by_doc(doc_id=doc_id, limit=limit)

        # ì´ ê°œìˆ˜(ê°€ëŠ¥í•˜ë©´)
        try:
            total = store.count_by_doc(doc_id)
        except Exception:
            total = None

        # í•­ìƒ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆë¡œ ë°˜í™˜
        return {
            "items": items,
            "total": total,
            "limit": limit,
            "doc_id": doc_id,
        }

    except Exception as e:
        # ì—¬ê¸°ì„œëŠ” ë¡œì»¬ ë³€ìˆ˜ ì°¸ì¡° ê¸ˆì§€!
        raise HTTPException(500, f"Milvus by-doc ì‹¤íŒ¨: {e}")

@router.get("/debug/search",tags=["milvus"])
def debug_vector_search(q: str, k: int = 5):
    try:
        model = get_embedding_model()
        store = MilvusStoreV2(dim=model.get_sentence_embedding_dimension())
        raw = store.debug_search(q, embed_fn=embed, topk=k)
        return {"results": raw}
    except Exception as e:
        raise HTTPException(500, f"ë””ë²„ê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

# ==================== ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ í•„í„°ë§ (ì‹ ê·œ ì—”ë“œí¬ì¸íŠ¸) ====================
class DocsByCodeResponse(BaseModel):
    """ì¹´í…Œê³ ë¦¬ í•„í„° ì‘ë‹µ"""
    doc_ids: List[str]

@router.get("/rag/docs/by-code", response_model=DocsByCodeResponse)
def list_docs_by_code(
    data_code: Optional[str] = Query(None, description="ëŒ€ë¶„ë¥˜ ì½”ë“œ"),
    data_code_detail: Optional[str] = Query(None, description="ì¤‘ë¶„ë¥˜ ì½”ë“œ"),
    data_code_detail_sub: Optional[str] = Query(None, description="ì†Œë¶„ë¥˜ ì½”ë“œ"),
):
    """
    osk_data í…Œì´ë¸”ì—ì„œ data_code / data_code_detail / data_code_detail_sub ê¸°ì¤€ìœ¼ë¡œ
    data_id(doc_id)ë¥¼ ì¡°íšŒí•´ì„œ ë‚´ë ¤ì£¼ëŠ” ì—”ë“œí¬ì¸íŠ¸.
    
    - parse_yn = 'S' (RAG ì¸ë±ì‹± ì™„ë£Œëœ ë¬¸ì„œë§Œ)
    - del_yn != 'Y' (ì‚­ì œë˜ì§€ ì•Šì€ ë¬¸ì„œë§Œ)
    
    Examples:
        - /rag/docs/by-code?data_code=LAW
        - /rag/docs/by-code?data_code=LAW&data_code_detail=NUCLEAR
        - /rag/docs/by-code?data_code=MANUAL&data_code_detail_sub=SAFETY
    """
    db = DBConnector()

    try:
        rows = db.fetch_docs_by_code(
            data_code=data_code,
            data_code_detail=data_code_detail,
            data_code_detail_sub=data_code_detail_sub,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB error: {e}")

    doc_ids = [str(r["data_id"]) for r in rows]
    
    logger.info(
        f"[/rag/docs/by-code] Filtered {len(doc_ids)} docs | "
        f"code={data_code}, detail={data_code_detail}, sub={data_code_detail_sub}"
    )
    
    return DocsByCodeResponse(doc_ids=doc_ids)