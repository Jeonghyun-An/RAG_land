# app/api/java_router.py
"""
Java ì‹œìŠ¤í…œ ì—°ë™ ë¼ìš°í„° (ìš´ì˜ìš©)
- ì„œë²„ íŒŒì¼ì‹œìŠ¤í…œ ì‚¬ìš©
- DB ì™„ì „ ì—°ë™ (osk_data, osk_ocr_data, osk_ocr_hist, osk_data_sc)
- convert-and-index: PDF ë³€í™˜ + OCR + ì²­í‚¹ + ì„ë² ë”©
- manual-ocr-and-index: DB ê¸°ë°˜ ìˆ˜ë™ OCR ì²­í‚¹ + ì„ë² ë”©
- sc-index: SC ë¬¸ì„œ (preface + contents + conclusion) ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬
"""
from __future__ import annotations

import os
import re
import hashlib
import hmac
import uuid
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path

from fastapi import APIRouter, HTTPException, Header, BackgroundTasks
from pydantic import BaseModel
import httpx

from app.services.db_connector import DBConnector
from app.services.pdf_converter import convert_to_pdf, convert_bytes_to_pdf_bytes, ConvertError
from app.services import job_state
from app.services.minio_store import MinIOStore
from datetime import timezone
import mimetypes

router = APIRouter(prefix="/java", tags=["java-production"])

# í™˜ê²½ë³€ìˆ˜
SHARED_SECRET = os.getenv("JAVA_SHARED_SECRET", "")
SERVER_BASE_PATH = os.getenv("SERVER_BASE_PATH", "/mnt/shared")

def META_KEY(doc_id: str) -> str:
    return f"uploaded/__meta__/{doc_id}/meta.json"

# ==================== Schemas ====================
class ConvertAndIndexRequest(BaseModel):
    """ìë°” â†’ AI íŠ¸ë¦¬ê±° ìš”ì²­ (convert-and-index)"""
    data_id: str
    path: str  # ì„œë²„ íŒŒì¼ì‹œìŠ¤í…œ ìƒëŒ€ ê²½ë¡œ
    file_id: str
    callback_url: Optional[str] = None


class ManualOCRAndIndexRequest(BaseModel):
    """ìë°” â†’ AI íŠ¸ë¦¬ê±° ìš”ì²­ (manual-ocr-and-index)"""
    data_id: str
    path: str  # ì„œë²„ íŒŒì¼ì‹œìŠ¤í…œ ìƒëŒ€ ê²½ë¡œ (ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŒ)
    file_id: str
    callback_url: Optional[str] = None
    rag_yn: str = "N"  # "N" (ì‹ ê·œ ì‘ì—…), "Y" (ê¸°ì¡´ ì‘ì—… ìˆ˜ì •)


class SCIndexRequest(BaseModel):
    """ìë°” â†’ AI íŠ¸ë¦¬ê±° ìš”ì²­ (sc-index) - SC ë¬¸ì„œ ì „ìš©"""
    data_id: str
    callback_url: Optional[str] = None


class ConvertAndIndexResponse(BaseModel):
    """ì¦‰ì‹œ ì‘ë‹µ"""
    status: str
    job_id: str
    data_id: str
    message: str


class WebhookPayload(BaseModel):
    """AI â†’ ìë°” ì½œë°± í˜ì´ë¡œë“œ"""
    job_id: str
    data_id: str
    status: str
    converted: bool = False
    metrics: Optional[Dict[str, Any]] = None
    timestamps: Optional[Dict[str, str]] = None
    message: str = ""
    chunk_count: Optional[int] = None


class StatusResponse(BaseModel):
    """ìƒíƒœ ì¡°íšŒ ì‘ë‹µ"""
    data_id: str
    parse_yn: Optional[str] = None
    parse_start_dt: Optional[str] = None
    parse_end_dt: Optional[str] = None

    
class DeleteDocumentRequest(BaseModel):
    """ë¬¸ì„œ ì‚­ì œ ìš”ì²­"""
    data_id: str
    delete_from_minio: bool = True
    callback_url: Optional[str] = None


class DeleteDocumentResponse(BaseModel):
    """ì‚­ì œ ì‘ë‹µ"""
    status: str
    data_id: str
    deleted_chunks: int
    deleted_files: List[str]
    message: str


# ==================== Helper Functions ====================
def verify_internal_token(token: Optional[str]) -> bool:
    """ë‚´ë¶€ í† í° ê²€ì¦"""
    if not SHARED_SECRET:
        return True
    return token == SHARED_SECRET


async def send_webhook(url: str, payload: WebhookPayload, secret: str):
    """AI â†’ ìë°” ì›¹í›… ì „ì†¡"""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            headers = {}
            if secret:
                sig = hmac.new(secret.encode(), payload.model_dump_json().encode(), hashlib.sha256).hexdigest()
                headers["X-Webhook-Signature"] = sig
            
            resp = await client.post(url, json=payload.model_dump(), headers=headers)
            resp.raise_for_status()
            print(f"[WEBHOOK] âœ… Sent to {url}: {payload.status}")
    except Exception as e:
        print(f"[WEBHOOK] âŒ Failed: {e}")


def _normalize_pages_for_chunkers(pages) -> List[Tuple[int, str]]:
    """
    í˜ì´ì§€ ì •ê·œí™” - llama_routerì™€ ë™ì¼í•œ ë¡œì§
    """
    out = []
    if not pages:
        return out

    for i, item in enumerate(pages, start=1):
        # (int,str) íŠœí”Œ/ë¦¬ìŠ¤íŠ¸
        if isinstance(item, (tuple, list)):
            if len(item) >= 2:
                pno, txt = item[0], item[1]
            else:
                pno, txt = i, (item[0] if item else "")
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, "" if txt is None else str(txt)))
            continue

        # dict
        if isinstance(item, dict):
            pno = item.get("page") or item.get("page_no") or item.get("index") or i
            txt = (
                item.get("text")
                or item.get("body")
                or ("\n".join(item.get("lines") or []) if item.get("lines") else "")
                or ""
            )
            try:
                pno = int(pno)
            except Exception:
                pno = i
            out.append((pno, str(txt)))
            continue

        # ë¬¸ìì—´
        if isinstance(item, str):
            out.append((i, item))
            continue

        # ê¸°íƒ€: ë¬¸ìì—´í™”
        out.append((i, str(item)))

    return out


def _coerce_chunks_for_milvus(chs):
    """
    (í…ìŠ¤íŠ¸, ë©”íƒ€) ë¦¬ìŠ¤íŠ¸ë¥¼ Milvus insert í˜•íƒœë¡œ ì •ê·œí™”
    llama_routerì™€ ë™ì¼í•œ ë¡œì§
    """
    safe = []
    for t in chs or []:
        if not isinstance(t, (list, tuple)) or len(t) < 2:
            continue
        text, meta = t[0], t[1]
        text = "" if text is None else str(text)
        if not isinstance(meta, dict):
            meta = {}

        # section ìš°ì„  ê²°ì •
        section = str(meta.get("section", ""))[:512]
        # page ì •ê·œí™”: pagesê°€ ìˆìœ¼ë©´ ì²« í˜ì´ì§€
        pages = meta.get("pages")
        if isinstance(pages, (list, tuple)) and len(pages) > 0:
            try:
                page = int(pages[0])
            except Exception:
                page = int(meta.get("page", 0))
        else:
            try:
                page = int(meta.get("page", 0))
            except Exception:
                page = 0

        safe.append((text, {"page": page, "section": section, "pages": pages or [], "bboxes": meta.get("bboxes", {})}))

    out = []
    last = None
    for it in safe:
        if it[0] and it != last:
            out.append(it)
            last = it
    return out


def perform_advanced_chunking(
    pages_std: List[Tuple[int, str]],
    layout_map: Dict[int, List[Dict]],
    job_id: str
) -> List[Tuple[str, Dict]]:
    """
    ê³µìš© ì²­í‚¹ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (llama_routerì™€ ë™ì¼)
    """
    from app.services.chunking_unified import build_chunks
    job_state.update(job_id, step="chunking:unified")
    return build_chunks(pages_std, layout_map, job_id=job_id)


def _render_text_pdf(text: str, out_path: str) -> str:
    """
    ì£¼ì–´ì§„ textë¥¼ ê°„ë‹¨í•œ PDFë¡œ ë Œë”ë§í•´ out_pathì— ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜.
    reportlabì´ ì—†ìœ¼ë©´ ImportError ë°œìƒ -> ìƒìœ„ì—ì„œ ì²˜ë¦¬.
    """
    from reportlab.lib.pagesizes import A4
    from reportlab.pdfgen import canvas
    from reportlab.lib.units import mm
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont

    # í°íŠ¸ ë“±ë¡ (ì—†ì–´ë„ ê¸°ë³¸ í°íŠ¸ë¡œ ë™ì‘ì€ í•¨)
    try:
        # ì‹œìŠ¤í…œì— NotoSansCJK ê°™ì€ í°íŠ¸ê°€ ìˆìœ¼ë©´ ë“±ë¡ (ì„ íƒ)
        # pdfmetrics.registerFont(TTFont("NotoSans", "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc"))
        pass
    except Exception:
        pass

    c = canvas.Canvas(out_path, pagesize=A4)
    width, height = A4

    margin_x = 20 * mm
    margin_y = 20 * mm
    max_width = width - 2 * margin_x
    y = height - margin_y

    # í°íŠ¸ ì„¤ì •
    try:
        c.setFont("Helvetica", 10)
    except Exception:
        pass

    # ì•„ì£¼ ë‹¨ìˆœí•œ ì›Œë“œë©
    import textwrap
    lines = []
    for para in (text or "").splitlines():
        # ëŒ€ëµì ì¸ í­ ê¸°ì¤€(ì˜ë¬¸ 80~100ì, í•œê¸€ ì„ì´ë©´ ì¤„ ì¡°ë°€)
        # í•„ìš”í•˜ë©´ reportlabì˜ stringWidthë¡œ ì •í™•ë„ í–¥ìƒ ê°€ëŠ¥
        wrap = textwrap.wrap(para, width=95) or [""]
        lines.extend(wrap)

    line_height = 12  # pt
    for line in lines:
        if y <= margin_y:
            c.showPage()
            try:
                c.setFont("Helvetica", 10)
            except Exception:
                pass
            y = height - margin_y
        c.drawString(margin_x, y, line)
        y -= line_height

    c.showPage()
    c.save()
    return out_path


# ==================== Endpoints ====================

@router.post("/convert-and-index", response_model=ConvertAndIndexResponse)
async def convert_and_index(
    request: ConvertAndIndexRequest,
    background_tasks: BackgroundTasks,
    x_internal_token: Optional[str] = Header(None)
):
    """ìë°” â†’ AI íŠ¸ë¦¬ê±° API (ìš´ì˜ìš©) - convert-and-index"""
    if not verify_internal_token(x_internal_token):
        raise HTTPException(401, "Unauthorized - Invalid token")
    
    db = DBConnector()
    existing = db.get_file_by_id(request.data_id)
    
    # ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìŠ¤í‚µ ë¡œì§ (í•„ìš” ì‹œ)
    # if existing and existing.get('parse_yn') == 'S':
    #     return ConvertAndIndexResponse(...)
    
    job_id = str(uuid.uuid4())[:8]
    
    background_tasks.add_task(
        process_convert_and_index_prod,
        job_id=job_id,
        data_id=request.data_id,
        path=request.path,
        file_id=request.file_id,
        callback_url=request.callback_url
    )
    
    return ConvertAndIndexResponse(
        status="accepted",
        job_id=job_id,
        data_id=request.data_id,
        message="processing (advanced chunking)"
    )


@router.post("/manual-ocr-and-index", response_model=ConvertAndIndexResponse)
async def manual_ocr_and_index(
    request: ManualOCRAndIndexRequest,
    background_tasks: BackgroundTasks,
    x_internal_token: Optional[str] = Header(None)
):
    """
    ìë°” â†’ AI íŠ¸ë¦¬ê±° API (ìš´ì˜ìš©) - manual-ocr-and-index
    
    í”„ë¡œì„¸ìŠ¤:
    1. rag_yn='N': ì‹ ê·œ OCR ì‘ì—…
       - osk_data.parse_yn = 'L' ë¡œ ì‹œì‘
       - osk_ocr_dataì—ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ì„œ ì²­í‚¹/ì„ë² ë”©
       - ì™„ë£Œ ì‹œ parse_yn = 'S', osk_ocr_hist ë¡œê¹…
    
    2. rag_yn='Y': ê¸°ì¡´ ì‘ì—… ìˆ˜ì • (ì‚¬ìš©ìê°€ í˜ì´ì§€ ìˆ˜ì •)
       - osk_ocr_dataì—ì„œ ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ì„œ ì¬ì²­í‚¹/ì„ë² ë”©
       - Milvusì—ì„œ ê¸°ì¡´ ì²­í¬ ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
       - ì™„ë£Œ ì‹œ parse_yn = 'S', osk_ocr_hist ë¡œê¹…
    """
    if not verify_internal_token(x_internal_token):
        raise HTTPException(401, "Unauthorized - Invalid token")
    
    job_id = str(uuid.uuid4())[:8]
    
    background_tasks.add_task(
        process_manual_ocr_and_index,
        job_id=job_id,
        data_id=request.data_id,
        path=request.path,
        file_id=request.file_id,
        callback_url=request.callback_url,
        rag_yn=request.rag_yn
    )
    
    return ConvertAndIndexResponse(
        status="accepted",
        job_id=job_id,
        data_id=request.data_id,
        message=f"processing manual OCR from DB (rag_yn={request.rag_yn})"
    )


@router.post("/sc-index", response_model=ConvertAndIndexResponse)
async def sc_index(
    request: SCIndexRequest,
    background_tasks: BackgroundTasks,
    x_internal_token: Optional[str] = Header(None)
):
    """
    ìë°” â†’ AI íŠ¸ë¦¬ê±° API (ìš´ì˜ìš©) - sc-index (SC ë¬¸ì„œ ì „ìš©)
    
    í”„ë¡œì„¸ìŠ¤:
    1. osk_data_sc í…Œì´ë¸”ì—ì„œ data_id ì¡°íšŒ
    2. preface_text + contents_text + conclusion_text í•©ì¹˜ê¸°
    3. ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬ (1~2 í˜ì´ì§€ ë¶„ëŸ‰)
    4. ì„ë² ë”© í›„ Milvus ì €ì¥
    5. osk_data.parse_yn = 'S', osk_ocr_hist ë¡œê¹…
    """
    if not verify_internal_token(x_internal_token):
        raise HTTPException(401, "Unauthorized - Invalid token")
    
    # SC ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    db = DBConnector()
    sc_doc = db.get_sc_document(request.data_id)
    
    if not sc_doc:
        raise HTTPException(404, f"SC document not found: data_id={request.data_id}")
    
    job_id = str(uuid.uuid4())[:8]
    
    background_tasks.add_task(
        process_sc_index,
        job_id=job_id,
        data_id=request.data_id,
        callback_url=request.callback_url
    )
    
    return ConvertAndIndexResponse(
        status="accepted",
        job_id=job_id,
        data_id=request.data_id,
        message="processing SC document (single chunk)"
    )


@router.post("/delete-document", response_model=DeleteDocumentResponse)
async def delete_document(
    request: DeleteDocumentRequest,
    background_tasks: BackgroundTasks,
    x_internal_token: Optional[str] = Header(None)
):
    """ë¬¸ì„œ ì‚­ì œ API"""
    if not verify_internal_token(x_internal_token):
        raise HTTPException(401, "Unauthorized - Invalid token")
    
    background_tasks.add_task(
        process_delete_document,
        data_id=request.data_id,
        delete_from_minio=request.delete_from_minio,
        callback_url=request.callback_url
    )
    
    return DeleteDocumentResponse(
        status="deleting",
        data_id=request.data_id,
        deleted_chunks=0,
        deleted_files=[],
        message="Deletion started"
    )


@router.post("/batch-delete")
async def batch_delete(
    data_ids: List[str],
    delete_from_minio: bool = True,
    callback_url: Optional[str] = None,
    background_tasks: BackgroundTasks = BackgroundTasks(),
    x_internal_token: Optional[str] = Header(None)
):
    """ë°°ì¹˜ ì‚­ì œ API"""
    if not verify_internal_token(x_internal_token):
        raise HTTPException(401, "Unauthorized - Invalid token")
    
    if not data_ids:
        raise HTTPException(400, "data_ids cannot be empty")
    
    if len(data_ids) > 100:
        raise HTTPException(400, "Maximum 100 documents per batch")
    
    for data_id in data_ids:
        background_tasks.add_task(
            process_delete_document,
            data_id=data_id,
            delete_from_minio=delete_from_minio,
            callback_url=callback_url
        )
    
    return {
        "status": "deleting",
        "count": len(data_ids),
        "data_ids": data_ids,
        "message": f"Batch deletion started for {len(data_ids)} documents"
    }


@router.get("/status/{data_id}", response_model=StatusResponse)
def get_status(data_id: str, x_internal_token: Optional[str] = Header(None)):
    """ìƒíƒœ ì¡°íšŒ API (ìš´ì˜ìš©)"""
    if not verify_internal_token(x_internal_token):
        raise HTTPException(401, "Unauthorized - Invalid token")
    
    db = DBConnector()
    meta = db.get_file_by_id(data_id)
    
    if not meta:
        raise HTTPException(404, f"data_id {data_id} not found")
    
    return StatusResponse(
        data_id=data_id,
        parse_yn=meta.get('parse_yn'),
        parse_start_dt=str(meta.get('parse_start_dt')) if meta.get('parse_start_dt') else None,
        parse_end_dt=str(meta.get('parse_end_dt')) if meta.get('parse_end_dt') else None
    )


@router.get("/health")
def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "ok", 
        "service": "java-router-production",
        "chunking": "advanced (en_tech â†’ law â†’ layout â†’ basic)",
        "manual_ocr": "DB-based (osk_ocr_data)",
        "sc_index": "SC document single chunk (preface + contents + conclusion)"
    }


# ==================== Background Tasks ====================

async def process_convert_and_index_prod(
    job_id: str,
    data_id: str,
    path: str,
    file_id: str,
    callback_url: Optional[str]
):
    """
    ìš´ì˜ìš© ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ - convert-and-index
    
    ğŸ”¥ ìˆ˜ì •ì‚¬í•­:
    1. PDF ì™¸ í™•ì¥ì â†’ PDF ë³€í™˜ (bytes ê¸°ë°˜)
    2. ë³€í™˜ëœ PDFë¥¼ MinIOì— ì—…ë¡œë“œ
    3. DBì—ëŠ” ê²½ë¡œë¥¼ ì“°ì§€ ì•Šê³  ìƒíƒœë§Œ ì—…ë°ì´íŠ¸
    """
    from app.services.file_parser import parse_pdf, parse_pdf_blocks
    
    db = DBConnector()
    m = MinIOStore()
    start_time = datetime.utcnow()
    
    job_state.start(job_id, data_id=data_id, file_id=file_id)
    
    try:
        # ========== Step 1: íŒŒì¼ ê²½ë¡œ í™•ì¸ ==========
        job_state.update(job_id, status="initializing", step="Resolving file path")
        
        raw_path = Path(path)
        base = raw_path if raw_path.is_absolute() else Path(SERVER_BASE_PATH) / raw_path

        # baseê°€ í´ë”ì´ê±°ë‚˜ í™•ì¥ìê°€ ì—†ìœ¼ë©´ file_idë¥¼ ë¶™ì—¬ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
        full_path = base if base.suffix else (base / file_id)

        if not full_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
         
        print(f"[PROD] Processing file: {full_path}")
        
        # ========== Step 2: PDF ë³€í™˜ (í•„ìš” ì‹œ) + MinIO ì—…ë¡œë“œ + (ë³€í™˜ ì‹œ) ë³¼ë¥¨ ì €ì¥ + DB file_idë§Œ ë³€ê²½ ==========
        src_ext = full_path.suffix.lower()
        is_already_pdf = (src_ext == ".pdf")
        
        doc_id = str(data_id)
        object_pdf = f"uploaded/{doc_id}.pdf"  # MinIO ì—…ë¡œë“œ í‚¤
        
        converted_pdf_path: Optional[str] = None
        pdf_bytes: Optional[bytes] = None
        
        if is_already_pdf:
            # (A) ì´ë¯¸ PDFë©´: íŒŒì¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© + MinIO ì—…ë¡œë“œ (DB file_id ë³€ê²½ ì•ˆ í•¨)
            converted_pdf_path = str(full_path)
            print(f"[PROD] Already PDF: {converted_pdf_path}")
        
            with open(full_path, "rb") as f:
                pdf_bytes = f.read()
        
            m.upload_bytes(
                pdf_bytes,
                object_name=object_pdf,
                content_type="application/pdf",
                length=len(pdf_bytes),
            )
            print(f"[PROD] âœ… PDF uploaded to MinIO: {object_pdf}")
        
        else:
            # (B) PDFê°€ ì•„ë‹ˆë©´: ë³€í™˜ â†’ MinIO ì—…ë¡œë“œ â†’ ë™ì¼ í´ë”ì— *.pdf ì €ì¥ â†’ DBì—ëŠ” file_idë§Œ *.pdfë¡œ ë³€ê²½
            job_state.update(job_id, status="converting", step=f"Converting {src_ext} to PDF")
            print(f"[PROD] Converting {src_ext} to PDF: {full_path}")
        
            try:
                # 1) bytes ë³€í™˜ ìš°ì„ 
                with open(full_path, "rb") as f:
                    content = f.read()
        
                pdf_bytes = convert_bytes_to_pdf_bytes(content, src_ext)
        
                # 2) bytes ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë³€í™˜ê¸°ë¡œ ê²½ë¡œ ë³€í™˜
                temp_pdf_path: Optional[str] = None
                if pdf_bytes is None:
                    temp_pdf_path = convert_to_pdf(str(full_path))
                    if not temp_pdf_path or not Path(temp_pdf_path).exists():
                        raise ConvertError("PDF ë³€í™˜ ì‹¤íŒ¨(ì¶œë ¥ ì—†ìŒ)")
                    with open(temp_pdf_path, "rb") as f:
                        pdf_bytes = f.read()
        
                assert pdf_bytes is not None, "pdf_bytes is None"
                print(f"[PROD] âœ… PDF converted: {len(pdf_bytes)} bytes")
        
                # 3) MinIO ì—…ë¡œë“œ
                m.upload_bytes(
                    pdf_bytes,
                    object_name=object_pdf,
                    content_type="application/pdf",
                    length=len(pdf_bytes),
                )
                print(f"[PROD] âœ… PDF uploaded to MinIO: {object_pdf}")
        
                # 4) ë³¼ë¥¨(ê¸°ì¡´ í´ë”) ì— *.pdf ì €ì¥ â€” ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ, íŒŒì¼ëª…ë§Œ .pdf
                save_path = full_path.with_suffix(".pdf")
                save_path.parent.mkdir(parents=True, exist_ok=True)
                with open(save_path, "wb") as fw:
                    fw.write(pdf_bytes)
                converted_pdf_path = str(save_path)
                print(f"[PROD] âœ… PDF saved to volume: {converted_pdf_path}")
        
                # 5) DBì—ëŠ” file_idë§Œ *.pdfë¡œ ì—…ë°ì´íŠ¸ (í´ë”ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
                new_file_id_pdf = save_path.name  # ex) f20231212M3Uv.pdf
                # â†“ DB ì»¤ë„¥í„°ì— ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ í•˜ë‚˜ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆì‹œ)
                db.update_file_id_only(data_id, new_file_id_pdf)
        
                # 6) ì„ì‹œ íŒŒì¼ ì •ë¦¬
                try:
                    if temp_pdf_path and Path(temp_pdf_path).exists():
                        Path(temp_pdf_path).unlink(missing_ok=True)
                except Exception:
                    pass
                
            except Exception as e:
                raise ConvertError(f"PDF ë³€í™˜ ì‹¤íŒ¨: {e}")
                
        # ========== Step 3: OCR ì‹œì‘ ë§ˆí‚¹ ==========
        db.mark_ocr_start(data_id)
        
        # ========== Step 4: í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR í¬í•¨) ==========
        job_state.update(job_id, status="parsing", step="Extracting text with OCR")
        
        print(f"[PROD-PARSE] Extracting text from: {converted_pdf_path}")
        pages = parse_pdf(converted_pdf_path, by_page=True)
        
        if not pages:
            raise RuntimeError("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
        
        print(f"[PROD-PARSE] Extracted {len(pages)} pages")
        
        # ========== Step 5: OCR ê²°ê³¼ DB ì €ì¥ ==========
        # ìë°” ìš”êµ¬ì‚¬í•­: OCR ì¶”ì¶œ ì¢…ë£Œ ì‹œ osk_ocr_dataì— INSERT
        job_state.update(job_id, status="saving_ocr", step="Saving OCR results to DB")
        
        for page_no, text in pages:
            db.insert_ocr_result(data_id, page_no, text)
            print(f"[PROD-OCR-DB] Saved page {page_no} to osk_ocr_data")
        
        # OCR ì„±ê³µ ë§ˆí‚¹ (parse_yn='S')
        db.mark_ocr_success(data_id)
        print(f"[PROD-OCR-DB] âœ… OCR completed and saved to DB: {len(pages)} pages")
        
        # ========== Step 6: ë ˆì´ì•„ì›ƒ ì •ë³´ ì¶”ì¶œ ==========
        blocks_by_page_list = parse_pdf_blocks(converted_pdf_path)
        layout_map = {int(p): blks for p, blks in (blocks_by_page_list or [])}
        print(f"[PROD-PARSE] Layout blocks extracted for {len(layout_map)} pages")
        
        # ========== Step 7: í˜ì´ì§€ ì •ê·œí™” ==========
        pages_std = _normalize_pages_for_chunkers(pages)
        if not any((t or "").strip() for _, t in pages_std):
            print("[PROD-PARSE] Warning: No textual content after parsing")
        
        # ========== Step 8: ê³ ë„í™”ëœ ì²­í‚¹ ==========
        job_state.update(job_id, status="chunking", step="Advanced chunking")
        
        chunks = perform_advanced_chunking(pages_std, layout_map, job_id)
        
        if not chunks:
            raise RuntimeError("ì²­í‚¹ ì‹¤íŒ¨: ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
        
        # ========== Step 9: ì²­í¬ ì •ê·œí™” ==========
        chunks = _coerce_chunks_for_milvus(chunks)
        print(f"[PROD-CHUNK] Normalized {len(chunks)} chunks for Milvus")
        
        # ========== Step 10: ì„ë² ë”© ë° Milvus ì €ì¥ ==========
        job_state.update(job_id, status="embedding", step=f"Embedding {len(chunks)} chunks")
        
        from app.services.embedding_model import embed, get_sentence_embedding_dimension
        from app.services.milvus_store_v2 import MilvusStoreV2
        
        mvs = MilvusStoreV2(dim=get_sentence_embedding_dimension())
        
        # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
        print(f"[PROD] Deleting existing doc (if any): {data_id}")
        try:
            deleted = mvs._delete_by_doc_id(data_id)
            print(f"[PROD] Deleted {deleted} existing chunks")
        except Exception as e:
            print(f"[PROD] Warning during delete: {e}")
        
        # Milvus insert
        print(f"[PROD] Inserting {len(chunks)} chunks to Milvus")
        
        result = mvs.insert(
            doc_id=data_id,
            chunks=chunks,
            embed_fn=embed
        )
        
        print(f"[PROD] âœ… Successfully indexed: {result.get('inserted', 0)} chunks")
        
        # ========== Step 11: RAG ì™„ë£Œ ì²˜ë¦¬ ==========
        pages_count = len(pages_std)
        chunk_count = result.get('inserted', len(chunks))
        
        # ========== Step 11.5: MinIO ë™ê¸°í™” (í”„ë¡ íŠ¸ ëª©ë¡ ë…¸ì¶œìš©) ==========
        try:
            SYNC_TO_MINIO = os.getenv("JAVA_SYNC_TO_MINIO", "1") == "1"
            if SYNC_TO_MINIO:
                doc_id = str(data_id)
                pdf_path_for_upload = converted_pdf_path

                # ğŸ”¹ DBì—ì„œ ì œëª©/ì½”ë“œ ë“± ë©”íƒ€ ì½ê¸°
                row = None
                try:
                    row = db.get_file_by_id(data_id)  # { data_title, data_code, ... }
                except Exception as _e:
                    row = None

                # í‘œì‹œìš© íƒ€ì´í‹€ ê²°ì •: DB data_title ìš°ì„ , ì—†ìœ¼ë©´ íŒŒì¼ëª…
                display_title = None
                if isinstance(row, dict):
                    display_title = (row.get("data_title") or "").strip() or None
                if not display_title:
                    display_title = Path(pdf_path_for_upload).name  # fallback

                if pdf_path_for_upload and Path(pdf_path_for_upload).exists():
                    # ì´ë¯¸ MinIOì— ì—…ë¡œë“œí–ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì—…ë¡œë“œ ë¶ˆí•„ìš”
                    # meta.jsonë§Œ ì—…ë°ì´íŠ¸
                    object_pdf = f"uploaded/{doc_id}.pdf"

                    # meta.json ê°±ì‹ (ì¡´ì¬í•˜ë©´ merge)
                    meta = {}
                    try:
                        if m.exists(META_KEY(doc_id)):
                            meta = m.get_json(META_KEY(doc_id)) or {}
                    except Exception:
                        meta = {}

                    # ğŸ”¹ DB ë©”íƒ€ë¥¼ í•¨ê»˜ ì €ì¥(í•„í„°ì— ì“°ê³  ì‹¶ìœ¼ë©´ í”„ë¡ íŠ¸ì—ì„œ í™œìš© ê°€ëŠ¥)
                    extra_meta = {}
                    if isinstance(row, dict):
                        for k in [
                            "data_id","data_title","data_code","data_code_detail","data_code_detail_sub",
                            "file_folder","file_id","reg_nm","reg_id","reg_dt","reg_type","parse_yn"
                        ]:
                            if k in row:
                                extra_meta[k] = row[k]

                    meta.update({
                        "doc_id": doc_id,
                        "title": display_title,                 # âœ… DB data_title
                        "pdf_key": object_pdf,                  # âœ… MinIO ë³€í™˜ PDF
                        "original_key": None,                   # âœ… MinIO ì˜¤ë¸Œì íŠ¸ê°€ ì•„ë‹ˆë©´ None
                        "original_fs_path": str(full_path),     # âœ… ë¡œì»¬ ê²½ë¡œëŠ” ë³„ë„ í•„ë“œì—
                        "original_name": Path(full_path).name,
                        "is_pdf_original": is_already_pdf,
                        "uploaded_at": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                        "indexed": True,
                        "chunk_count": int(chunk_count),
                        "last_indexed_at": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                        **extra_meta,
                    })
                    m.put_json(META_KEY(doc_id), meta)

                    print(f"[PROD-MINIO] âœ… synced: {object_pdf} (title='{display_title}', chunks={chunk_count})")
                else:
                    print("[PROD-MINIO] âš ï¸ skip: no local pdf to upload")
            else:
                print("[PROD-MINIO] â­ï¸ skip: JAVA_SYNC_TO_MINIO=0")
        except Exception as e:
            print(f"[PROD-MINIO] âŒ sync failed: {e}")

        print(f"[PROD] âœ… Indexing completed: {pages_count} pages, {chunk_count} chunks")
        # RAG ì™„ë£Œ ë§ˆí‚¹ (parse_yn='S' ìœ ì§€, íˆìŠ¤í† ë¦¬ ë¡œê¹…)
        db.update_rag_completed(data_id)
        
        job_state.complete(
            job_id,
            pages=pages_count,
            chunks=chunk_count
        )
        
        # ========== Step 12: ì™„ë£Œ & Webhook ==========
        end_time = datetime.utcnow()
        
        if callback_url:
            payload = WebhookPayload(
                job_id=job_id,
                data_id=data_id,
                status="api_indexed",
                converted=not is_already_pdf,
                metrics={"pages": pages_count, "chunks": chunk_count},
                chunk_count=chunk_count,
                timestamps={
                    "start": start_time.isoformat(), 
                    "end": end_time.isoformat()
                },
                message="indexed successfully (advanced chunking)"
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)
    
    except Exception as e:
        job_state.fail(job_id, str(e))
        db.update_rag_error(data_id, str(e))
        print(f"[PROD] âŒ Error: {e}")
        
        if callback_url:
            payload = WebhookPayload(
                job_id=job_id,
                data_id=data_id,
                status="api_error",
                message=str(e)
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)
        
        raise


async def process_manual_ocr_and_index(
    job_id: str,
    data_id: str,
    path: str,
    file_id: str,
    callback_url: Optional[str],
    rag_yn: str
):
    """
    ìš´ì˜ìš© ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ - manual-ocr-and-index
    
    í”„ë¡œì„¸ìŠ¤:
    1. rag_yn='N': ì‹ ê·œ OCR ì‘ì—…
       - osk_data.parse_yn = 'L' ë¡œ ì‹œì‘
       - osk_ocr_dataì—ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ì„œ ì²­í‚¹/ì„ë² ë”©
       - ì™„ë£Œ ì‹œ parse_yn = 'S', osk_ocr_hist ë¡œê¹…
    
    2. rag_yn='Y': ê¸°ì¡´ ì‘ì—… ìˆ˜ì • (ì‚¬ìš©ìê°€ í˜ì´ì§€ ìˆ˜ì •)
       - osk_ocr_dataì—ì„œ ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ ê°€ì ¸ì™€ì„œ ì¬ì²­í‚¹/ì„ë² ë”©
       - Milvusì—ì„œ ê¸°ì¡´ ì²­í¬ ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
       - ì™„ë£Œ ì‹œ parse_yn = 'S', osk_ocr_hist ë¡œê¹…
    """
    db = DBConnector()
    start_time = datetime.utcnow()
    
    job_state.start(job_id, data_id=data_id, file_id=file_id)
    rag_yn = (rag_yn or "N").upper()
    
    # rag_ynì— ë”°ë¥¸ DB ì²˜ë¦¬
    if rag_yn == "N":
        db.mark_ocr_start(data_id)
        print(f"[MANUAL-OCR] ì‹ ê·œ ì‘ì—…: data_id={data_id}, parse_yn='L'")
    else:
        print(f"[MANUAL-OCR] ê¸°ì¡´ ì‘ì—… ìˆ˜ì •: data_id={data_id}, rag_yn='Y'")
    
    try:
        # ========== Step 1: DBì—ì„œ OCR í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ==========
        job_state.update(job_id, status="loading", step="Loading OCR text from DB")
        
        print(f"[MANUAL-OCR] Loading OCR text from osk_ocr_data for data_id={data_id}")
        
        pages_std = db.get_ocr_text_by_data_id(data_id)
        
        if not pages_std:
            raise RuntimeError(f"DBì— OCR í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤ (osk_ocr_data): data_id={data_id}")
        
        print(f"[MANUAL-OCR] Loaded {len(pages_std)} pages from DB")
        
        # ========== Step 2: ê³ ë„í™”ëœ ì²­í‚¹ ==========
        job_state.update(job_id, status="chunking", step="Advanced chunking from DB text")
        
        # Manual OCRì€ ë ˆì´ì•„ì›ƒ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ dict ì „ë‹¬
        layout_map = {}
        
        chunks = perform_advanced_chunking(pages_std, layout_map, job_id)
        
        if not chunks:
            raise RuntimeError("ì²­í‚¹ ì‹¤íŒ¨: ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
        
        # ========== Step 3: ì²­í¬ ì •ê·œí™” ==========
        chunks = _coerce_chunks_for_milvus(chunks)
        print(f"[MANUAL-OCR-CHUNK] Normalized {len(chunks)} chunks for Milvus")
        
        # ========== Step 4: ì„ë² ë”© ë° Milvus ì €ì¥ ==========
        job_state.update(job_id, status="embedding", step=f"Embedding {len(chunks)} chunks")
        
        from app.services.embedding_model import embed, get_sentence_embedding_dimension
        from app.services.milvus_store_v2 import MilvusStoreV2
        
        mvs = MilvusStoreV2(dim=get_sentence_embedding_dimension())
        
        # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
        print(f"[MANUAL-OCR] Deleting existing doc (if any): {data_id}")
        try:
            deleted = mvs._delete_by_doc_id(data_id)
            print(f"[MANUAL-OCR] Deleted {deleted} existing chunks")
        except Exception as e:
            print(f"[MANUAL-OCR] Warning during delete: {e}")
        
        # Milvus insert
        print(f"[MANUAL-OCR] Inserting {len(chunks)} chunks to Milvus")
        
        result = mvs.insert(
            doc_id=data_id,
            chunks=chunks,
            embed_fn=embed
        )
        
        print(f"[MANUAL-OCR] âœ… Successfully indexed: {result.get('inserted', 0)} chunks")
        
        # ========== Step 5: RAG ì™„ë£Œ ì²˜ë¦¬ ==========
        pages_count = len(pages_std)
        chunk_count = result.get('inserted', len(chunks))
        
        print(f"[MANUAL-OCR] âœ… Indexing completed: {pages_count} pages, {chunk_count} chunks")
        # RAG ì™„ë£Œ ë§ˆí‚¹ (parse_yn='S' ìœ ì§€, íˆìŠ¤í† ë¦¬ ë¡œê¹…)
        db.update_rag_completed(data_id)
        
        job_state.complete(
            job_id,
            pages=pages_count,
            chunks=chunk_count
        )
        
        # ========== Step 6: ì™„ë£Œ & Webhook ==========
        end_time = datetime.utcnow()
        
        if callback_url:
            payload = WebhookPayload(
                job_id=job_id,
                data_id=data_id,
                status="corrected_indexed",
                converted=False,
                metrics={"pages": pages_count, "chunks": chunk_count},
                chunk_count=chunk_count,
                timestamps={
                    "start": start_time.isoformat(), 
                    "end": end_time.isoformat()
                },
                message="indexed successfully from manual OCR (advanced chunking)"
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)
    
    except Exception as e:
        job_state.fail(job_id, str(e))
        db.update_rag_error(data_id, str(e))
        print(f"[MANUAL-OCR] âŒ Error: {e}")
        
        if callback_url:
            payload = WebhookPayload(
                job_id=job_id,
                data_id=data_id,
                status="corrected_error",
                message=str(e)
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)
        
        raise


async def process_sc_index(
    job_id: str,
    data_id: str,
    callback_url: Optional[str]
):
    """
    ìš´ì˜ìš© ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ - sc-index (SC ë¬¸ì„œ ì „ìš©)
    
    í”„ë¡œì„¸ìŠ¤:
    1. osk_data_sc í…Œì´ë¸”ì—ì„œ data_id ì¡°íšŒ
    2. preface_text + contents_text + conclusion_text í•©ì¹˜ê¸°
    3. ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬ (1~2 í˜ì´ì§€ ë¶„ëŸ‰)
    4. ì„ë² ë”© í›„ Milvus ì €ì¥
    5. osk_data.parse_yn = 'S', osk_ocr_hist ë¡œê¹…
    """
    db = DBConnector()
    m = MinIOStore()
    start_time = datetime.utcnow()
    
    job_state.start(job_id, data_id=data_id, file_id="sc_document")
    
    try:
        # ========== Step 1: OCR ì‹œì‘ ë§ˆí‚¹ ==========
        db.mark_ocr_start(data_id)
        print(f"[SC-INDEX] ì‹ ê·œ SC ë¬¸ì„œ ì‘ì—…: data_id={data_id}, parse_yn='L'")
        
        # ========== Step 2: DBì—ì„œ SC ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ==========
        job_state.update(job_id, status="loading", step="Loading SC document from DB")
        
        print(f"[SC-INDEX] Loading SC document from osk_data_sc for data_id={data_id}")
        
        combined_text = db.get_sc_combined_text(data_id)
        
        if not combined_text:
            raise RuntimeError(f"SC ë¬¸ì„œ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: data_id={data_id}")
        
        print(f"[SC-INDEX] Loaded SC text: {len(combined_text)} characters")
        
        # ========== Step 3: ì²­í¬ ìƒì„± (í† í° ê¸¸ì´ì— ë”°ë¼ ë¶„í• ) ==========
        job_state.update(job_id, status="chunking", step="Creating chunks for SC document")
        
        # í† í° ìˆ˜ ê³„ì‚°
        from app.services.embedding_model import get_embedding_model
        import os
        
        embedding_model = get_embedding_model()
        tokenizer = getattr(embedding_model, "tokenizer", None)
        
        # ì„ë² ë”© ëª¨ë¸ì˜ ìµœëŒ€ í† í° ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
        max_seq_length = int(getattr(embedding_model, "max_seq_length", 1024))
        embed_max_tokens = int(os.getenv("EMBED_MAX_TOKENS", "1024"))
        # ë‘˜ ì¤‘ ì‘ì€ ê°’ ì‚¬ìš© (ì•ˆì „ ë§ˆì§„ 20% í™•ë³´)
        safe_max_tokens = int(min(max_seq_length, embed_max_tokens) * 0.8)
        
        print(f"[SC-INDEX] Max seq length: {max_seq_length}, EMBED_MAX_TOKENS: {embed_max_tokens}")
        print(f"[SC-INDEX] Safe max tokens per chunk: {safe_max_tokens}")
        
        # ì „ì²´ í† í° ìˆ˜ ê³„ì‚°
        if tokenizer:
            tokens = tokenizer.encode(combined_text, add_special_tokens=False)
            total_token_count = len(tokens)
        else:
            # tokenizer ì—†ìœ¼ë©´ ëŒ€ëµì  ì¶”ì •
            total_token_count = len(combined_text) // 4
            tokens = None
        
        print(f"[SC-INDEX] SC document total token count: {total_token_count}")
        
        chunks = []
        
        # í† í° ìˆ˜ê°€ ì•ˆì „ ë²”ìœ„ ë‚´ë©´ ë‹¨ì¼ ì²­í¬
        if total_token_count <= safe_max_tokens:
            print(f"[SC-INDEX] Creating single chunk (within token limit)")
            chunk = (
                combined_text,
                {
                    "page": 1,
                    "pages": [1],
                    "section": "SC Document",
                    "token_count": total_token_count,
                    "bboxes": {},
                    "type": "sc_document"
                }
            )
            chunks = [chunk]
        else:
            # í† í° ìˆ˜ ì´ˆê³¼ ì‹œ ë¶„í• 
            print(f"[SC-INDEX] Token count ({total_token_count}) exceeds limit ({safe_max_tokens}), splitting into multiple chunks")
            
            if tokenizer and tokens:
                # tokenizerê°€ ìˆìœ¼ë©´ ì •í™•í•˜ê²Œ í† í° ê¸°ë°˜ ë¶„í• 
                num_chunks = (total_token_count + safe_max_tokens - 1) // safe_max_tokens
                print(f"[SC-INDEX] Splitting into {num_chunks} chunks")
                
                for i in range(num_chunks):
                    start_idx = i * safe_max_tokens
                    end_idx = min((i + 1) * safe_max_tokens, total_token_count)
                    chunk_tokens = tokens[start_idx:end_idx]
                    chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
                    
                    chunk = (
                        chunk_text,
                        {
                            "page": i + 1,
                            "pages": [i + 1],
                            "section": f"SC Document (Part {i + 1}/{num_chunks})",
                            "token_count": len(chunk_tokens),
                            "bboxes": {},
                            "type": "sc_document_part"
                        }
                    )
                    chunks.append(chunk)
                    print(f"[SC-INDEX] Created chunk {i + 1}/{num_chunks}: {len(chunk_tokens)} tokens")
            else:
                # tokenizer ì—†ìœ¼ë©´ ë¬¸ì ê¸°ë°˜ ë¶„í•  (ëŒ€ëµì )
                chars_per_chunk = safe_max_tokens * 4  # 1 í† í° â‰ˆ 4ì
                num_chunks = (len(combined_text) + chars_per_chunk - 1) // chars_per_chunk
                print(f"[SC-INDEX] Splitting into {num_chunks} chunks (character-based)")
                
                for i in range(num_chunks):
                    start_idx = i * chars_per_chunk
                    end_idx = min((i + 1) * chars_per_chunk, len(combined_text))
                    chunk_text = combined_text[start_idx:end_idx]
                    
                    chunk = (
                        chunk_text,
                        {
                            "page": i + 1,
                            "pages": [i + 1],
                            "section": f"SC Document (Part {i + 1}/{num_chunks})",
                            "token_count": len(chunk_text) // 4,
                            "bboxes": {},
                            "type": "sc_document_part"
                        }
                    )
                    chunks.append(chunk)
                    print(f"[SC-INDEX] Created chunk {i + 1}/{num_chunks}: ~{len(chunk_text)} chars")
        
        print(f"[SC-INDEX] Total chunks created: {len(chunks)}")
        
        # ========== Step 4: ì²­í¬ ì •ê·œí™” ==========
        chunks = _coerce_chunks_for_milvus(chunks)
        print(f"[SC-INDEX] Normalized {len(chunks)} chunk(s) for Milvus")
        
        # ========== Step 5: ì„ë² ë”© ë° Milvus ì €ì¥ ==========
        job_state.update(job_id, status="embedding", step=f"Embedding {len(chunks)} chunk(s)")
        
        from app.services.embedding_model import embed, get_sentence_embedding_dimension
        from app.services.milvus_store_v2 import MilvusStoreV2
        
        mvs = MilvusStoreV2(dim=get_sentence_embedding_dimension())
        
        # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
        print(f"[SC-INDEX] Deleting existing doc (if any): {data_id}")
        try:
            deleted = mvs._delete_by_doc_id(data_id)
            print(f"[SC-INDEX] Deleted {deleted} existing chunks")
        except Exception as e:
            print(f"[SC-INDEX] Warning during delete: {e}")
        
        # Milvus insert
        print(f"[SC-INDEX] Inserting {len(chunks)} chunk(s) to Milvus")
        
        result = mvs.insert(
            doc_id=data_id,
            chunks=chunks,
            embed_fn=embed
        )
        
        print(f"[SC-INDEX] âœ… Successfully indexed: {result.get('inserted', 0)} chunk(s)")

        # ========== Step 6.5: MinIO sync (SC) ==========
        try:
            SYNC_TO_MINIO = os.getenv("JAVA_SYNC_TO_MINIO", "1") == "1"
            if SYNC_TO_MINIO:
                doc_id = str(data_id)
                m = MinIOStore()

                # í‘œì‹œìš© íƒ€ì´í‹€: osk_data.data_title ìš°ì„ 
                row = None
                try:
                    row = db.get_file_by_id(data_id)  # { data_title, ... }
                except Exception:
                    row = None
                display_title = None
                if isinstance(row, dict):
                    display_title = (row.get("data_title") or "").strip() or None
                if not display_title:
                    display_title = f"SC Document {doc_id}"

                # SC í…ìŠ¤íŠ¸ë¥¼ PDFë¡œ ìƒì„± (ì„ì‹œ ê²½ë¡œ)
                import tempfile
                from pathlib import Path as _Path
                tmpdir = tempfile.gettempdir()
                local_pdf = _Path(tmpdir) / f"sc_{doc_id}.pdf"

                try:
                    _render_text_pdf(combined_text, str(local_pdf))
                except ImportError as e:
                    # reportlab ë¯¸ì„¤ì¹˜ ì‹œ ì•ˆë‚´ ë¡œê·¸
                    raise RuntimeError(
                        "reportlabì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ SC PDF ìƒì„±ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                        "ì»¨í…Œì´ë„ˆ/í™˜ê²½ì— 'pip install reportlab'ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”."
                    ) from e

                # PDF ì—…ë¡œë“œ
                object_pdf = f"uploaded/{doc_id}.pdf"
                with open(local_pdf, "rb") as f:
                    data = f.read()
                m.upload_bytes(
                    data,
                    object_name=object_pdf,
                    content_type="application/pdf",
                    length=len(data),
                )

                # meta.json merge
                def META_KEY(doc_id: str) -> str:
                    return f"uploaded/__meta__/{doc_id}/meta.json"

                meta = {}
                try:
                    if m.exists(META_KEY(doc_id)):
                        meta = m.get_json(META_KEY(doc_id)) or {}
                except Exception:
                    meta = {}

                # DB ë©”íƒ€ë¥¼ ì¡°ê¸ˆ ë” ë„£ê³  ì‹¶ë‹¤ë©´ ì¶”ê°€
                extra_meta = {}
                if isinstance(row, dict):
                    for k in [
                        "data_id","data_title","data_code","data_code_detail","data_code_detail_sub",
                        "file_folder","file_id","reg_nm","reg_id","reg_dt","reg_type","parse_yn"
                    ]:
                        if k in row:
                            extra_meta[k] = row[k]

                meta.update({
                    "doc_id": doc_id,
                    "title": display_title,           # âœ… í”„ë¡ íŠ¸ ëª©ë¡/ë·°ì–´ ì œëª©
                    "pdf_key": object_pdf,            # âœ… í”„ë¡ íŠ¸ê°€ ì—¬ëŠ” í‚¤
                    "original_key": None,             # íŒŒì¼ ì›ë³¸ ê°œë… ì—†ìŒ
                    "original_fs_path": None,
                    "original_name": f"sc_{doc_id}.pdf",
                    "is_pdf_original": True,
                    "uploaded_at": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                    "indexed": True,
                    "chunk_count": int(result.get('inserted', 0)),
                    "last_indexed_at": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                    "type": "sc_document",
                    **extra_meta,
                })
                m.put_json(META_KEY(doc_id), meta)

                print(f"[SC-MINIO] âœ… synced: {object_pdf} (title='{display_title}', chunks={result.get('inserted', 0)})")
            else:
                print("[SC-MINIO] â­ï¸ skip: JAVA_SYNC_TO_MINIO=0")
        except Exception as e:
            print(f"[SC-MINIO] âŒ sync failed: {e}")

        # ========== Step 6: OCR ì„±ê³µ ë§ˆí‚¹ ==========
        # SC ë¬¸ì„œëŠ” í˜ì´ì§€ ê°œë…ì´ ì—†ìœ¼ë¯€ë¡œ osk_ocr_dataì— ì €ì¥í•˜ì§€ ì•ŠìŒ
        # ë°”ë¡œ OCR ì„±ê³µ ì²˜ë¦¬
        db.mark_ocr_success(data_id)
        print(f"[SC-INDEX] âœ… Marked OCR success for SC document: data_id={data_id}")
        
        # ========== Step 7: RAG ì™„ë£Œ ì²˜ë¦¬ ==========
        chunk_count = result.get('inserted', len(chunks))
        
        print(f"[SC-INDEX] âœ… Indexing completed: 1 SC document, {chunk_count} chunk(s)")
        # RAG ì™„ë£Œ ë§ˆí‚¹ (parse_yn='S' ìœ ì§€, íˆìŠ¤í† ë¦¬ ë¡œê¹…)
        db.update_rag_completed(data_id)
        
        job_state.complete(
            job_id,
            pages=1,
            chunks=chunk_count
        )
        
        # ========== Step 8: ì™„ë£Œ & Webhook ==========
        end_time = datetime.utcnow()
        
        if callback_url:
            payload = WebhookPayload(
                job_id=job_id,
                data_id=data_id,
                status="sc_indexed",
                converted=False,
                metrics={
                    "pages": 1, 
                    "chunks": chunk_count, 
                    "type": "sc_document",
                    "was_split": chunk_count > 1,
                    "total_tokens": total_token_count
                },
                chunk_count=chunk_count,
                timestamps={
                    "start": start_time.isoformat(), 
                    "end": end_time.isoformat()
                },
                message=f"indexed successfully (SC document {'split into ' + str(chunk_count) + ' chunks' if chunk_count > 1 else 'single chunk'})"
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)
    
    except Exception as e:
        job_state.fail(job_id, str(e))
        print(f"[SC-INDEX] âŒ Error: {e}")
        
        if callback_url:
            payload = WebhookPayload(
                job_id=job_id,
                data_id=data_id,
                status="sc_error",
                message=str(e)
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)
        
        raise


async def process_delete_document(
    data_id: str,
    delete_from_minio: bool,
    callback_url: Optional[str]
):
    """ë¬¸ì„œ ì‚­ì œ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬"""
    from app.services.milvus_store_v2 import MilvusStoreV2
    from app.services.embedding_model import get_sentence_embedding_dimension
    
    try:
        # Milvus ì‚­ì œ
        mvs = MilvusStoreV2(dim=get_sentence_embedding_dimension())
        deleted_chunks = mvs._delete_by_doc_id(data_id)
        print(f"[DELETE] Deleted {deleted_chunks} chunks from Milvus for data_id={data_id}")
        
        # MinIO ì‚­ì œ (ì„ íƒ)
        deleted_files = []
        if delete_from_minio:
            m = MinIOStore()
            doc_id = str(data_id)
            
            # PDF ì‚­ì œ
            pdf_key = f"uploaded/{doc_id}.pdf"
            if m.exists(pdf_key):
                m.delete(pdf_key)
                deleted_files.append(pdf_key)
            
            # meta.json ì‚­ì œ
            meta_key = META_KEY(doc_id)
            if m.exists(meta_key):
                m.delete(meta_key)
                deleted_files.append(meta_key)
            
            print(f"[DELETE] Deleted {len(deleted_files)} files from MinIO")
        
        # Webhook
        if callback_url:
            payload = WebhookPayload(
                job_id=str(uuid.uuid4())[:8],
                data_id=data_id,
                status="deleted",
                converted=False,
                metrics={"deleted_chunks": deleted_chunks, "deleted_files": len(deleted_files)},
                message=f"Document deleted: {deleted_chunks} chunks, {len(deleted_files)} files"
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)
    
    except Exception as e:
        print(f"[DELETE] âŒ Error: {e}")
        
        if callback_url:
            payload = WebhookPayload(
                job_id=str(uuid.uuid4())[:8],
                data_id=data_id,
                status="delete_error",
                message=str(e)
            )
            await send_webhook(callback_url, payload, SHARED_SECRET)