# app/services/milvus_store_v2.py
from __future__ import annotations

import os
import re
from typing import List, Tuple, Dict, Any, Callable, Optional

from pymilvus import (
    connections,
    utility,
    Collection,
    CollectionSchema,
    FieldSchema,
    DataType,
    MilvusException,
)

# ì»¬ë ‰ì…˜ ì´ë¦„(í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥)
COLLECTION = os.getenv("MILVUS_COLLECTION", "rag_chunks_v2")
SECTION_MAX = int(os.getenv("MILVUS_SECTION_MAX", "512"))
DOC_ID_MAX  = int(os.getenv("MILVUS_DOCID_MAX",  "256"))
CHUNK_MAX   = int(os.getenv("MILVUS_CHUNK_MAX",  "8192"))


def _safe_truncate_text(text: str, max_len: int) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ UTF-8 ë°”ì´íŠ¸ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ìë¥´ê¸°.
    ë¬¸ì ìˆ˜ê°€ ì•„ë‹Œ ë°”ì´íŠ¸ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ì—¬ Milvus VARCHAR ì œí•œì„ ì ˆëŒ€ ë„˜ì§€ ì•ŠìŒ.
    """
    if not text:
        return ""
    
    # 1ë‹¨ê³„: ë°”ì´íŠ¸ ê¸¸ì´ ì²´í¬
    encoded = text.encode('utf-8', errors='ignore')
    if len(encoded) <= max_len:
        return text
    
    # 2ë‹¨ê³„: ë°”ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ì•¼ í•¨
    # ì•ˆì „ ë§ˆì§„ (10% ì—¬ìœ )
    target_bytes = int(max_len * 0.9)
    
    # ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ìë¥´ë˜, UTF-8 ë¬¸ì ê²½ê³„ë¥¼ ì§€í‚´
    truncated_bytes = encoded[:target_bytes]
    
    # UTF-8 ë””ì½”ë”© ì‹œë„ (ë¶ˆì™„ì „í•œ ë©€í‹°ë°”ì´íŠ¸ ë¬¸ì ì œê±°)
    try:
        truncated = truncated_bytes.decode('utf-8', errors='ignore')
    except:
        # í˜¹ì‹œ ëª¨ë¥¼ ì—ëŸ¬ì— ëŒ€ë¹„
        truncated = text[:int(max_len * 0.5)]
    
    # 3ë‹¨ê³„: ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
    if len(truncated) > target_bytes * 0.5:  # ì ˆë°˜ ì´ìƒ ë‚¨ì•„ìˆìœ¼ë©´
        last_sentence_end = max(
            truncated.rfind('.'),
            truncated.rfind('!'),
            truncated.rfind('?'),
            truncated.rfind('ã€‚'),
        )
        if last_sentence_end > len(truncated) * 0.5:
            truncated = truncated[:last_sentence_end + 1]
    
    # 4ë‹¨ê³„: ìµœì¢… ë°”ì´íŠ¸ ê¸¸ì´ ê²€ì¦ ë° ê°•ì œ ìë¥´ê¸°
    final_encoded = truncated.encode('utf-8', errors='ignore')
    while len(final_encoded) > max_len and truncated:
        # 10% ì”© ì¤„ì´ê¸°
        cut_point = int(len(truncated) * 0.9)
        truncated = truncated[:cut_point]
        final_encoded = truncated.encode('utf-8', errors='ignore')
    
    return truncated.strip()


def _vmax(field):
    """
    Milvus 2.2.xëŠ” field.params í˜•íƒœê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë°©ì–´ì ìœ¼ë¡œ ì¶”ì¶œ
    - field.params.get("max_length")
    - field.params.get("type_params", {}).get("max_length")
    - getattr(field, "max_length", 0)
    """
    try:
        p = getattr(field, "params", {}) or {}
        if isinstance(p, dict):
            if "max_length" in p:
                return int(p["max_length"])
            tp = p.get("type_params") or {}
            if "max_length" in tp:
                return int(tp["max_length"])
    except Exception:
        pass
    try:
        return int(getattr(field, "max_length", 0) or 0)
    except Exception:
        return 0
    
def _get_schema_limits(col: Collection) -> dict:
    f = {x.name: x for x in col.schema.fields}
    return {
        "doc_id":  _vmax(f.get("doc_id"))  or DOC_ID_MAX,
        "section": _vmax(f.get("section")) or SECTION_MAX,
        "chunk":   _vmax(f.get("chunk"))   or CHUNK_MAX,
    }


class MilvusStoreV2:
    """
    ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ RAGìš© Milvus ìŠ¤í† ì–´(V2)
      - ìŠ¤í‚¤ë§ˆ:
          id (INT64, auto_id, primary)
          doc_id (VARCHAR 256)
          seq (INT64)
          page (INT64)
          section (VARCHAR 512)
          chunk (VARCHAR 8192)
          embedding (FLOAT_VECTOR dim={dim})
      - ì¸ë±ìŠ¤: HNSW + IP (Milvus 2.2.x ì—ì„œ COSINE ë¯¸ì§€ì› â†’ IP ì‚¬ìš©)
      - ì„ë² ë”©ì€ normalize_embeddings=Trueë¡œ ì¸ì½”ë”©í•˜ì—¬ IP == cosineë¡œ ë™ì‘
    """

    def __init__(self, dim: int, name: Optional[str] = None):
        self.dim = int(dim)
        self.collection_name = name or COLLECTION

        host = os.getenv("MILVUS_HOST", "milvus")
        port = os.getenv("MILVUS_PORT", "19530")
        if not connections.has_connection("default"):
            connections.connect(alias="default", host=host, port=port)

        force_reset = os.getenv("RAG_RESET_COLLECTION", "0") == "1"

        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if utility.has_collection(self.collection_name):
            col = Collection(self.collection_name)
            # ìŠ¤í‚¤ë§ˆ/ì°¨ì› ë¶ˆì¼ì¹˜ ì‹œ ì¬ìƒì„± or ì—ëŸ¬
            if force_reset or self._schema_mismatch(col, self.dim):
                print(f"âš ï¸ drop & recreate collection: {self.collection_name} (force_reset={force_reset})")
                utility.drop_collection(self.collection_name)
                self.col = self._create_collection()
            else:
                self.col = col
                self._ensure_index()
        else:
            self.col = self._create_collection()

        # load ì‹œë„ (ì¸ë±ìŠ¤ ì—†ê±°ë‚˜ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ ë¬´ì‹œ)
        try:
            self.col.load()
        except MilvusException as e:
            print(f"âš ï¸ load skipped: {e}")

    # ---------------- internal ----------------

    def _schema_mismatch(self, col: Collection, expect_dim: int) -> bool:
        """ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆ(íŠ¹íˆ embedding dim) ë¶ˆì¼ì¹˜ ì‹œ True"""
        try:
            fdict = {f.name: f for f in col.schema.fields}
            # í•„ìˆ˜ í•„ë“œ ì²´í¬
            required = ("doc_id", "seq", "page", "section", "chunk", "embedding")
            if any(r not in fdict for r in required):
                return True
            emb = fdict["embedding"]
            # dim ì½ê¸° (ë²„ì „ì— ë”°ë¼ params ë˜ëŠ” ì†ì„±)
            emb_dim = emb.params.get("dim") if hasattr(emb, "params") else getattr(emb, "dim", None)
            if int(emb_dim or 0) != int(expect_dim):
                return True
            
            def vmax(field):
                try:
                    return int(field.params.get("max_length"))
                except Exception:
                    try:
                        return int(getattr(field, "max_length", 0))
                    except Exception:
                        return 0
            
            # í™˜ê²½ë³€ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ ì²´í¬
            current_doc_id_max = vmax(fdict["doc_id"])
            current_section_max = vmax(fdict["section"])
            current_chunk_max = vmax(fdict["chunk"])
            
            # í˜„ì¬ ìŠ¤í‚¤ë§ˆì™€ í™˜ê²½ë³€ìˆ˜ ë¹„êµ
            if current_doc_id_max != DOC_ID_MAX:
                print(f"âš ï¸ Schema mismatch: doc_id max_length {current_doc_id_max} != {DOC_ID_MAX}")
                return True
            if current_section_max != SECTION_MAX:
                print(f"âš ï¸ Schema mismatch: section max_length {current_section_max} != {SECTION_MAX}")
                return True
            if current_chunk_max != CHUNK_MAX:
                print(f"âš ï¸ Schema mismatch: chunk max_length {current_chunk_max} != {CHUNK_MAX}")
                return True
            
            return False
        except Exception as e:
            print(f"âš ï¸ Schema check error: {e}")
            return True

    def _create_collection(self) -> Collection:
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=DOC_ID_MAX),
            FieldSchema(name="seq", dtype=DataType.INT64),
            FieldSchema(name="page", dtype=DataType.INT64),
            FieldSchema(name="section", dtype=DataType.VARCHAR, max_length=SECTION_MAX),
            FieldSchema(name="chunk", dtype=DataType.VARCHAR, max_length=CHUNK_MAX),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dim),
        ]
        schema = CollectionSchema(fields, description="RAG chunks with metadata (v2)")
        col = Collection(self.collection_name, schema)

        # ì¸ë±ìŠ¤ ìƒì„± (Milvus 2.2.x: COSINE ë¯¸ì§€ì› â†’ IP)
        col.create_index(
            field_name="embedding",
            index_params={
                "index_type": "HNSW",
                "metric_type": "IP",
                "params": {"M": 16, "efConstruction": 200},
            },
        )
        print(f"âœ… created collection: {self.collection_name} (dim={self.dim})")
        return col

    def _ensure_index(self) -> None:
        """ì»¬ë ‰ì…˜ë§Œ ìˆê³  ì¸ë±ìŠ¤ ì—†ëŠ” ìƒíƒœ ë³´ê°•"""
        try:
            if not getattr(self.col, "indexes", []):
                self.col.create_index(
                    field_name="embedding",
                    index_params={
                        "index_type": "HNSW",
                        "metric_type": "IP",
                        "params": {"M": 16, "efConstruction": 200},
                    },
                )
                print("âœ… created missing index on existing collection")
        except Exception as e:
            print(f"âš ï¸ ensure index failed: {e}")

    def _replace_doc_if_needed(self, doc_id: str) -> None:
        """ê°™ì€ doc_id ë¬¸ì„œë¥¼ êµì²´(ì‚­ì œ í›„ ì¬ì‚½ì…)í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©.
           RAG_REPLACE_DOC=1 ì´ë©´ í™œì„±í™”.
        """
        if os.getenv("RAG_REPLACE_DOC", "1") != "1":
            return
        try:
            deleted = self._delete_by_doc_id(doc_id)
            print(f"replaced doc: {doc_id} (deleted {deleted})")
        except Exception as e:
            print(f"âš ï¸ replace_doc failed: {e}")

    # ---------------- public ----------------

    def insert(
        self, 
        doc_id: str, 
        chunks: List[Tuple[str, Dict[str, Any]]], 
        embed_fn: Callable[[List[str]], List[List[float]]],
    ) -> Dict[str, Any]:
        """
        ì¤‘ë³µ ë°©ì§€ & ì•ˆì „ ì‚½ì…:
        - RAG_SKIP_IF_EXISTS=1  : ê°™ì€ doc_id ì¡´ì¬ ì‹œ ìŠ¤í‚µ
        - RAG_REPLACE_DOC=1     : ê°™ì€ doc_id ì¡´ì¬ ì‹œ ì‚­ì œ í›„ ì‚½ì…
        - RAG_DEDUP_MANIFEST=1  : MinIO docs/{doc_id}.json ì— sha256 ê¸°ë¡/ë¹„êµ
        - RAG_UNIQUE_SUFFIX_ON_CONFLICT=1 : ì¶©ëŒì¸ë° REPLACE ì•„ë‹˜ â†’ doc_id__hash ë¡œ ìƒˆë¡œ ì‚½ì…
        
        **ì¤‘ìš”**: í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œì„ ì„ë² ë”© ì „ì— ì ìš©í•˜ì—¬ Milvus ì—ëŸ¬ ë°©ì§€
        """
        out = {"inserted": 0, "skipped": False, "reason": None, "doc_id": doc_id}
        if not chunks:
            out["skipped"] = True
            out["reason"] = "empty_chunks"
            return out

        # -------- 0) í˜„ì¬ ìƒíƒœ ì¡°íšŒ
        try:
            exists_cnt = self.count_by_doc(doc_id)
        except Exception:
            exists_cnt = 0

        SKIP_IF_EXISTS  = os.getenv("RAG_SKIP_IF_EXISTS", "0") == "1"
        REPLACE_DOC     = os.getenv("RAG_REPLACE_DOC", "0") == "1"
        USE_MANIFEST    = os.getenv("RAG_DEDUP_MANIFEST", "0") == "1"
        UNIQUE_SUFFIX   = os.getenv("RAG_UNIQUE_SUFFIX_ON_CONFLICT", "1") == "1"

        # -------- 1) ë§¤ë‹ˆí˜ìŠ¤íŠ¸(í•´ì‹œ) ë¹„êµ
        import hashlib
        
        # ===== ì¤‘ìš”: í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œì„ ì„ë² ë”© ì „ì— ì ìš© =====
        # ë¨¼ì € í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì•ˆì „í•˜ê²Œ ìë¥´ê¸°
        raw_texts = [(c[0] or "") for c in chunks]
        safe_texts = [_safe_truncate_text(t, CHUNK_MAX) for t in raw_texts]
        
        # í•´ì‹œ ê³„ì‚°ì€ ì•ˆì „í•˜ê²Œ ì˜ë¦° í…ìŠ¤íŠ¸ ê¸°ì¤€
        text_blob = "\n\n".join(safe_texts).encode("utf-8", errors="ignore")
        doc_hash = hashlib.sha256(text_blob).hexdigest()

        manifest_key = f"docs/{doc_id}.json"

        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¹„êµ
        if USE_MANIFEST:
            try:
                from app.services.minio_store import MinIOStore
                prev = MinIOStore().get_json(manifest_key)
                if prev and prev.get("sha256") == doc_hash:
                    out["skipped"] = True
                    out["reason"] = "manifest_match"
                    return out
            except Exception:
                pass

        # -------- 2) ì¡´ì¬ ì—¬ë¶€ ì²´í¬
        if exists_cnt > 0:
            if SKIP_IF_EXISTS:
                out["skipped"] = True
                out["reason"] = "exists"
                return out
            if REPLACE_DOC:
                self._replace_doc_if_needed(doc_id)
            elif UNIQUE_SUFFIX:
                doc_id = doc_id + "__" + doc_hash[:8]
                out["doc_id"] = doc_id

        # -------- 3) ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ê¸¸ì´ ì œí•œ ì ìš©
        limits = _get_schema_limits(self.col)
        SEC_MAX = limits["section"]
        CHK_MAX = limits["chunk"]
        
        # ì•ˆì „ ë§ˆì§„ì„ ìœ„í•œ ì‹¤ì œ ì œí•œê°’ (ìŠ¤í‚¤ë§ˆë³´ë‹¤ ì‘ê²Œ)
        SAFE_SECTION_LIMIT = min(SEC_MAX, 480)  # 512 - ë§ˆì§„
        SAFE_CHUNK_LIMIT = min(CHK_MAX, 8000)   # 8192 - ë§ˆì§„
        
        metas = [c[1] for c in chunks]
        pages = []
        sections = []
        seqs = list(range(len(chunks)))
        
        for m in metas:
            try:
                pages.append(int(m.get("page", 0)))
            except Exception:
                pages.append(0)
            
            # section í•„ë“œë„ ì•ˆì „í•˜ê²Œ ìë¥´ê¸°
            s = "" if m.get("section") is None else str(m.get("section"))
            sections.append(_safe_truncate_text(s, SAFE_SECTION_LIMIT))

        # -------- 4) í…ìŠ¤íŠ¸ ìµœì¢… ì •ë¦¬ (ì´ë¯¸ safe_textsë¡œ ì˜ë¦¼)
        # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: ë‹¤ì‹œ í•œë²ˆ ì²´í¬
        final_texts = [_safe_truncate_text(t, SAFE_CHUNK_LIMIT) for t in safe_texts]
        
        # ë””ë²„ê·¸ ë¡œê·¸
        for i, (orig, safe) in enumerate(zip(raw_texts, final_texts)):
            if len(orig) > len(safe):
                print(f"âš ï¸ Chunk {i} truncated: {len(orig)} â†’ {len(safe)} chars")

        # -------- 5) ì„ë² ë”© ìƒì„± (ì´ì œ ì•ˆì „í•˜ê²Œ ì˜ë¦° í…ìŠ¤íŠ¸ë¡œ)
        print(f"[Milvus] Embedding {len(final_texts)} chunks...")
        vecs = embed_fn(final_texts)
        
        if not vecs or len(vecs) != len(final_texts):
            raise RuntimeError("embedding failed: empty or count mismatch")
        
        dim0 = len(vecs[0])
        if dim0 != self.dim:
            raise RuntimeError(f"embedding dim mismatch: expect {self.dim}, got {dim0}")
        
        for i, v in enumerate(vecs):
            if len(v) != dim0:
                raise RuntimeError(f"embedding dim mismatch at {i}: {len(v)}")

        # -------- 6) doc_id ê¸¸ì´ ì œí•œ
        if len(doc_id) > DOC_ID_MAX:
            suf = hashlib.sha256(doc_id.encode("utf-8", "ignore")).hexdigest()[:8]
            doc_id = doc_id[:(DOC_ID_MAX - 10)] + "__" + suf
            out["doc_id"] = doc_id

        # -------- 7) entities ìƒì„± ì „ ìµœì¢… ê²€ì¦
        # ìŠ¤í‚¤ë§ˆ ì œí•œ í™•ì¸
        schema_limits = _get_schema_limits(self.col)
        actual_chunk_max = schema_limits["chunk"]
        actual_section_max = schema_limits["section"]
        
        print(f"[Milvus] Schema limits: chunk={actual_chunk_max}, section={actual_section_max}")
        
        # ìµœì¢… ì•ˆì „ì¥ì¹˜: ìŠ¤í‚¤ë§ˆ ì œí•œë³´ë‹¤ ì‘ê²Œ ìë¥´ê¸°
        final_texts = [_safe_truncate_text(t, actual_chunk_max - 100) for t in final_texts]
        sections = [_safe_truncate_text(s, actual_section_max - 32) for s in sections]
        
        # ê²€ì¦: ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ì œí•œ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        for i, t in enumerate(final_texts):
            byte_len = len(t.encode('utf-8', errors='ignore'))
            if byte_len > actual_chunk_max:
                print(f"âš ï¸ CRITICAL: Chunk {i} still exceeds limit! {byte_len} > {actual_chunk_max}")
                # ê°•ì œ ìë¥´ê¸°
                final_texts[i] = t[:actual_chunk_max - 100]
        
        # -------- 8) entities ìƒì„± ë° ì‚½ì…
        entities = [
            [doc_id] * len(final_texts),   # doc_id
            seqs,                           # seq
            pages,                          # page
            sections,                       # section
            final_texts,                    # chunk (ì´ë¯¸ ì•ˆì „í•˜ê²Œ ì˜ë¦¼)
            vecs,                           # embedding
        ]

        print(f"[Milvus] Inserting {len(final_texts)} chunks for doc_id={doc_id}...")
        mr = self.col.insert(entities)
        self.col.flush()
        
        try:
            self.col.load()
        except Exception:
            pass

        out["inserted"] = len(final_texts)

        # -------- 9) ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ê¸°ë¡(ì˜µì…˜)
        if USE_MANIFEST:
            try:
                from app.services.minio_store import MinIOStore
                MinIOStore().put_json(manifest_key, {
                    "doc_id": doc_id,
                    "sha256": doc_hash,
                    "chunks": len(final_texts),
                    "dim": self.dim,
                })
            except Exception:
                pass

        return out

    def delete_by_doc(self, doc_id: str) -> int:
        try:
            res = self.col.delete(expr=f'doc_id == "{doc_id}"')
            self.col.flush()
            return getattr(res, "delete_count", 0) or 0
        except Exception:
            return 0
        
    def _delete_by_doc_id(self, doc_id: str) -> int:
        """doc_idë¡œ í•´ë‹¹ ë¬¸ì„œì˜ PK(id)ë“¤ì„ ì¡°íšŒí•œ ë’¤, PK in [...] ë°©ì‹ìœ¼ë¡œ ì‚­ì œ"""
        try:
            self.col.load()
        except Exception:
            pass

        # 1) doc_idë¡œ PK(id) ì¡°íšŒ
        safe = str(doc_id).replace('"', r'\"')
        rows = self.col.query(
            expr=f'doc_id == "{safe}"',
            output_fields=["id"],
        ) or []

        ids = [r["id"] for r in rows if "id" in r]
        if not ids:
            return 0

        # 2) PKë¡œ ì‚­ì œ (MilvusëŠ” ê¸´ ë¦¬ìŠ¤íŠ¸ ì‚­ì œê°€ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë°°ì¹˜)
        BATCH = 16384  # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
        for i in range(0, len(ids), BATCH):
            batch = ids[i : i + BATCH]
            self.col.delete(expr=f"id in {batch}")

        self.col.flush()
        return len(ids)
    
    def delete_by_doc_id(self, doc_id: str) -> int:
        """ì™¸ë¶€ í˜¸ì¶œìš© alias"""
        return self._delete_by_doc_id(doc_id)
    
    def search(
        self, 
        query: str, 
        embed_fn: Callable[[List[str]], List[List[float]]], 
        topk: int = 20
    ) -> List[Dict[str, Any]]:
        """IP metric + normalize ì„ë² ë”© ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ topk ë°˜í™˜"""
        if not query:
            return []
        qv = embed_fn([query])[0]

        try:
            self.col.load()
        except Exception:
            pass

        res = self.col.search(
            data=[qv],
            anns_field="embedding",
            param={"metric_type": "IP", "params": {"ef": 64}},
            limit=topk,
            output_fields=["doc_id", "seq", "page", "section", "chunk"],
            consistency_level="Strong",
        )

        out: List[Dict[str, Any]] = []
        for hit in res[0]:
            ent = hit.entity
            out.append({
                "score": float(hit.distance),
                "doc_id": ent.get("doc_id"),
                "seq": int(ent.get("seq")),
                "page": int(ent.get("page")),
                "section": ent.get("section"),
                "chunk": ent.get("chunk"),
            })
        return out
    
    def search_in_docs(
        self,
        query: str,
        embed_fn: Callable[[List[str]], List[List[float]]],
        doc_ids: List[str],
        topk: int = 20,
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • doc_id ëª©ë¡ ì•ˆì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ë²„ì „
        - doc_ids: osk_data.data_id ëª©ë¡ (ë¬¸ìì—´)
        """
        if not query or not doc_ids:
            return []

        qv = embed_fn([query])[0]

        try:
            self.col.load()
        except Exception:
            pass

        # Milvus exprìš©ìœ¼ë¡œ doc_id in ["...", "..."] í˜•íƒœë¡œ ë³€í™˜
        safe_ids = [str(d).replace('"', "").replace("\\", "") for d in doc_ids]
        expr = "doc_id in [" + ",".join(f'"{i}"' for i in safe_ids) + "]"

        res = self.col.search(
            data=[qv],
            anns_field="embedding",
            param={"metric_type": "IP", "params": {"ef": 64}},
            limit=topk,
            expr=expr,  # ğŸ”¹ ì—¬ê¸°ì„œ í•„í„° ì ìš©
            output_fields=["doc_id", "seq", "page", "section", "chunk"],
            consistency_level="Strong",
        )

        out: List[Dict[str, Any]] = []
        for hit in res[0]:
            ent = hit.entity
            out.append(
                {
                    "score": float(hit.distance),
                    "doc_id": ent.get("doc_id"),
                    "seq": int(ent.get("seq")),
                    "page": int(ent.get("page")),
                    "section": ent.get("section"),
                    "chunk": ent.get("chunk"),
                }
            )
        return out

    def count_by_doc(self, doc_id: str) -> int:
        """íŠ¹ì • doc_idì˜ ì²­í¬ ê°œìˆ˜ ì¡°íšŒ"""
        try:
            self.col.load()
        except Exception:
            pass
        res = self.col.query(
            expr=f'doc_id == "{doc_id}"',
            output_fields=["doc_id"],
            limit=1
        )
        return len(res) if res else 0

    def stats(self) -> dict:
        """ì»¬ë ‰ì…˜ ìƒíƒœ ìš”ì•½"""
        try:
            num = self.col.num_entities
        except Exception:
            num = -1
        idx = []
        try:
            for ix in getattr(self.col, "indexes", []):
                params = {}
                try:
                    params = ix.params
                except Exception:
                    pass
                idx.append(params)
        except Exception:
            pass
        return {
            "collection": self.col.name,
            "num_entities": num,
            "indexes": idx,
            "schema_fields": [f.name for f in self.col.schema.fields],
        }

    def query_by_doc(self, doc_id: str, limit: int = 10) -> list[dict]:
        """íŠ¹ì • doc_idë¡œ ì €ì¥ëœ ì²­í¬ í™•ì¸"""
        expr = f'doc_id == "{doc_id}"'
        rows = self.col.query(
            expr=expr,
            output_fields=["doc_id", "seq", "page", "section", "chunk"],
            limit=limit
        )
        max_preview = int(os.getenv("DEBUG_PEEK_MAX_CHARS", "300"))
        return [
            {
                "doc_id": r.get("doc_id"),
                "seq": int(r.get("seq", -1)),
                "page": int(r.get("page", -1)),
                "section": r.get("section", ""),
                "chunk": (r.get("chunk", "")[:max_preview] + "..." 
                         if max_preview > 0 and len(r.get("chunk", "")) > max_preview 
                         else r.get("chunk", "")),
            }
            for r in rows
        ]

    def peek(self, limit: int = 10) -> list[dict]:
        """ì»¬ë ‰ì…˜ì˜ ì¼ë¶€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"""
        try:
            self.col.load()
        except Exception:
            pass
        
        rows = self.col.query(
            expr="id >= 0",
            output_fields=["id", "doc_id", "seq", "page", "section", "chunk"],
            limit=limit
        )
        
        max_preview = int(os.getenv("DEBUG_PEEK_MAX_CHARS", "300"))
        return [
            {
                "id": r.get("id"),
                "doc_id": r.get("doc_id"),
                "seq": int(r.get("seq", -1)),
                "page": int(r.get("page", -1)),
                "section": r.get("section", ""),
                "chunk": (r.get("chunk", "")[:max_preview] + "..." 
                         if max_preview > 0 and len(r.get("chunk", "")) > max_preview 
                         else r.get("chunk", "")),
            }
            for r in rows
        ]