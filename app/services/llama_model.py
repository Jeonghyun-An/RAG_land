# app/services/llama_model.py
from __future__ import annotations

import os, json, torch
from typing import Dict, Tuple, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM

from app.services.llm_client import chat_complete, list_vllm_models
from app.config import HUGGINGFACE_TOKEN

USE_VLLM = os.getenv("USE_VLLM", "1") == "1"
# ğŸ‘‡ í•˜ë“œì½”ë”© ê¸ˆì§€: ENVì—ì„œ aliasâ†’HF ID ë§¤í•‘(JSON)ë§Œ ì½ìŒ
# ì˜ˆ) MODEL_ALIASES='{"llama-1b":"meta-llama/Llama-3.2-1B-Instruct"}'
try:
    MODEL_ALIASES: Dict[str, str] = json.loads(os.getenv("MODEL_ALIASES", "{}"))
except Exception:
    MODEL_ALIASES = {}

LOADED_MODELS: Dict[str, Tuple[AutoModelForCausalLM, AutoTokenizer]] = {}

def _auth_kwargs() -> dict:
    return {"token": HUGGINGFACE_TOKEN} if HUGGINGFACE_TOKEN else {}

def _resolve_to_hf_id(name: str) -> Optional[str]:
    """
    nameì´ aliasë©´ ENV ë§¤í•‘ìœ¼ë¡œ HF ID ë°˜í™˜.
    'org/repo' ê°™ì€ HF IDë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    ë‘˜ ë‹¤ ì•„ë‹ˆë©´ None.
    """
    if not name:
        return None
    if name in MODEL_ALIASES:
        return MODEL_ALIASES[name]
    if "/" in name or os.path.isdir(name):
        return name
    return None  # ëª¨ë¥´ëŠ” í† í°(ë³„ì¹­ë„, HF IDë„ ì•„ë‹˜)

def load_model(model_id: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    if model_id in LOADED_MODELS:
        return LOADED_MODELS[model_id]

    cache_dir = os.environ.get("HF_HOME") or os.environ.get("TRANSFORMERS_CACHE")
    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    device_map: Optional[str | dict] = "auto" if torch.cuda.is_available() else {"": "cpu"}

    tok = AutoTokenizer.from_pretrained(
        model_id, use_fast=True, trust_remote_code=True, cache_dir=cache_dir, **_auth_kwargs()
    )
    mdl = AutoModelForCausalLM.from_pretrained(
        model_id, trust_remote_code=True, torch_dtype=torch_dtype,
        device_map=device_map, low_cpu_mem_usage=True, cache_dir=cache_dir, **_auth_kwargs()
    )
    LOADED_MODELS[model_id] = (mdl, tok)
    return mdl, tok

def generate_answer(
    prompt: str,
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    max_new_tokens: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
) -> str:
    try:
        messages = [{"role": "user", "content": prompt}]
        input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True)
    except Exception:
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids

    device = next(model.parameters()).device
    input_ids = input_ids.to(device)

    eos_id = tokenizer.eos_token_id
    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else eos_id

    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids, max_new_tokens=max_new_tokens, do_sample=True,
            temperature=temperature, top_p=top_p, top_k=top_k,
            eos_token_id=eos_id, pad_token_id=pad_id,
        )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

def generate_answer_unified(prompt: str, name_or_id: Optional[str]):
    """
    1) vLLMê°€ ì‚´ì•„ ìˆê³ , ìš”ì²­ê°’ì´ vLLMì˜ served nameê³¼ ì¼ì¹˜í•˜ë©´ vLLMë¡œ ë°”ë¡œ í˜¸ì¶œ
    2) ì•„ë‹ˆë©´ ìš”ì²­ê°’ì„ aliasâ†’HF ID ë˜ëŠ” 'ê·¸ëŒ€ë¡œ HF ID'ë¡œ í•´ì„
       - vLLMê°€ ìˆë”ë¼ë„ served nameì´ ë‹¤ë¥´ë©´ í´ë°±(Transformers)
    3) ë§ˆì§€ë§‰ì— ë‘˜ ë‹¤ ëª» í•˜ë©´ ì¹œì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€
    """
    name = (name_or_id or "").strip()

    # 1) vLLM served namesì— ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ vLLM í˜¸ì¶œ
    if USE_VLLM:
        served = set(list_vllm_models())  # ex: {"llama-1b", "meta-llama/Llama-3.2-1B-Instruct", ...}
        if name and name in served:
            return chat_complete(name, prompt)

    # 2) alias/HF ID í•´ì„
    hf_id = _resolve_to_hf_id(name) or _resolve_to_hf_id(os.getenv("DEFAULT_MODEL_ALIAS", "llama-1b"))
    if hf_id:
        # vLLMë¡œë„ í˜¸ì¶œí•´ë³´ë˜, served nameì´ ë‹¤ë¥´ë©´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì˜ˆì™¸ ë¬´ì‹œí•˜ê³  í´ë°±
        if USE_VLLM and served:
            try:
                # served ì´ë¦„ì´ HF IDì™€ ê°™ì„ ìˆ˜ë„/ë‹¤ë¥¼ ìˆ˜ë„ ìˆìŒ. ê°™ìœ¼ë©´ ìš´ ì¢‹ê²Œ ë°”ë¡œ ë¨.
                if hf_id in served:
                    return chat_complete(hf_id, prompt)
            except Exception:
                pass
        # Transformers í´ë°±
        model, tok = load_model(hf_id)
        return generate_answer(prompt, model, tok)

    # 3) ì „ë¶€ ì‹¤íŒ¨: ê°€ëŠ¥í•œ ì´ë¦„ ì œì•ˆ
    suggestions = []
    if USE_VLLM and served:
        suggestions += list(sorted(served))
    if MODEL_ALIASES:
        suggestions += [f"{k} -> {v}" for k, v in MODEL_ALIASES.items()]
    hint = "; ".join(suggestions) or "í™˜ê²½ë³€ìˆ˜ MODEL_ALIASESì— alias ë§¤í•‘ì„ ì„¤ì •í•˜ì„¸ìš”."
    raise RuntimeError(f"ëª¨ë¸ ì‹ë³„ ì‹¤íŒ¨: '{name_or_id}'. ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¦„(ì¼ë¶€): {hint}")
