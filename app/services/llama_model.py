# app/services/llama_model.py
from __future__ import annotations

import os, json, torch
from typing import Dict, Tuple, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM

from app.services.llm_client import chat_complete, list_vllm_models
from app.config import HUGGINGFACE_TOKEN

# íŒŒì¼ ìƒë‹¨ import ì˜†ì— ì¶”ê°€
OPENAI_ALIAS_URLS = json.loads(os.getenv("OPENAI_ALIAS_URLS", "{}"))


USE_VLLM = os.getenv("USE_VLLM", "1") == "1"
# ğŸ‘‡ í•˜ë“œì½”ë”© ê¸ˆì§€: ENVì—ì„œ aliasâ†’HF ID ë§¤í•‘(JSON)ë§Œ ì½ìŒ
# ì˜ˆ) MODEL_ALIASES='{"llama-1b":"meta-llama/Llama-3.2-1B-Instruct"}'
try:
    MODEL_ALIASES: Dict[str, str] = json.loads(os.getenv("MODEL_ALIASES", "{}"))
except Exception:
    MODEL_ALIASES = {}

LOADED_MODELS: Dict[str, Tuple[AutoModelForCausalLM, AutoTokenizer]] = {}

def _auth_kwargs() -> dict:
    return {"token": HUGGINGFACE_TOKEN} if HUGGINGFACE_TOKEN else {}

def _resolve_to_hf_id(name: str) -> Optional[str]:
    """
    nameì´ aliasë©´ ENV ë§¤í•‘ìœ¼ë¡œ HF ID ë°˜í™˜.
    'org/repo' ê°™ì€ HF IDë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    ë‘˜ ë‹¤ ì•„ë‹ˆë©´ None.
    """
    if not name:
        return None
    if name in MODEL_ALIASES:
        return MODEL_ALIASES[name]
    if "/" in name or os.path.isdir(name):
        return name
    return None  # ëª¨ë¥´ëŠ” í† í°(ë³„ì¹­ë„, HF IDë„ ì•„ë‹˜)

def load_model(model_id: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    if model_id in LOADED_MODELS:
        return LOADED_MODELS[model_id]

    cache_dir = os.environ.get("HF_HOME") or os.environ.get("TRANSFORMERS_CACHE")
    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    device_map: Optional[str | dict] = "auto" if torch.cuda.is_available() else {"": "cpu"}

    tok = AutoTokenizer.from_pretrained(
        model_id, use_fast=True, trust_remote_code=True, cache_dir=cache_dir, **_auth_kwargs()
    )
    mdl = AutoModelForCausalLM.from_pretrained(
        model_id, trust_remote_code=True, torch_dtype=torch_dtype,
        device_map=device_map, low_cpu_mem_usage=True, cache_dir=cache_dir, **_auth_kwargs()
    )
    LOADED_MODELS[model_id] = (mdl, tok)
    return mdl, tok

def generate_answer(
    prompt: str,
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    max_new_tokens: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
) -> str:
    try:
        messages = [{"role": "user", "content": prompt}]
        input_ids = tokenizer.apply_chat_template(
            messages, return_tensors="pt", add_generation_prompt=True,
        )
    except Exception:
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids

    device = next(model.parameters()).device
    input_ids = input_ids.to(device)

    eos_id = tokenizer.eos_token_id
    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else eos_id

    model.eval()  # ì¶”ë¡  ëª¨ë“œ
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            eos_token_id=eos_id,
            pad_token_id=pad_id,
            use_cache=True,
        )

    # "ìƒì„±ëœ ë¶€ë¶„ë§Œ" ë””ì½”ë”© (í”„ë¡¬í”„íŠ¸ ì—ì½” ì œê±°)
    gen_ids = output_ids[0, input_ids.shape[-1]:]
    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

def generate_answer_unified(prompt: str, name_or_id: Optional[str]):
    """
    ìš°ì„ ìˆœìœ„:
    1) aliasë³„ vLLM URL(OPENAI_ALIAS_URLS)ì´ ìˆìœ¼ë©´ ê·¸ë¦¬ë¡œ (served name=alias)
    2) í˜„ì¬ vLLMê°€ ë‚´ë†“ì€ served name ëª©ë¡ì— ìš”ì²­ê°’ì´ ìˆìœ¼ë©´ ê±°ê¸°ë¡œ
    3) alias/HF ID í•´ì„(hf_id) â†’ vLLM served nameì´ hf_idë©´ vLLM, ì•„ë‹ˆë©´ Transformers í´ë°±
    """
    name = (name_or_id or "").strip()
    alias = name if name in MODEL_ALIASES else None

    # ì•ˆì „ ê°€ë“œ
    served = set()

    # 1) aliasë³„ vLLM ë¼ìš°íŒ… (ko-8b, llama-1b ë“±)
    if USE_VLLM and alias and alias in OPENAI_ALIAS_URLS:
        try:
            from app.services.llm_client import chat_complete_on
            return chat_complete_on(OPENAI_ALIAS_URLS[alias], alias, prompt)
        except Exception:
            pass  # ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë‹¨ê³„ë¡œ

    # 2) vLLMê°€ í˜„ì¬ ë‚´ë†“ì€ served nameì— ì§ì ‘ ë§¤ì¹­ë˜ë©´ ê·¸ê±¸ë¡œ
    if USE_VLLM:
        try:
            served = set(list_vllm_models())  # ex) {"llama-1b","ko-8b"} í˜¹ì€ HF ID
        except Exception:
            served = set()
        if name and name in served:
            try:
                return chat_complete(name, prompt)
            except Exception:
                pass

    # 3) alias/HF ID í•´ì„ â†’ vLLM(hf_id ë§¤ì¹­) ë˜ëŠ” Transformers í´ë°±
    hf_id = _resolve_to_hf_id(name) or _resolve_to_hf_id(os.getenv("DEFAULT_MODEL_ALIAS", "llama-1b"))
    if hf_id:
        if USE_VLLM and hf_id in served:
            try:
                return chat_complete(hf_id, prompt)
            except Exception:
                pass
        # Transformers í´ë°±
        model, tok = load_model(hf_id)
        return generate_answer(prompt, model, tok)

    # ì „ë¶€ ì‹¤íŒ¨ ì‹œ íŒíŠ¸
    hints = []
    if USE_VLLM and served:
        hints += sorted(served)
    if MODEL_ALIASES:
        hints += [f"{k} -> {v}" for k, v in MODEL_ALIASES.items()]
    msg = "; ".join(hints) or "í™˜ê²½ë³€ìˆ˜ MODEL_ALIASESì— alias ë§¤í•‘ì„ ì„¤ì •í•˜ì„¸ìš”."
    raise RuntimeError(f"ëª¨ë¸ ì‹ë³„ ì‹¤íŒ¨: '{name_or_id}'. ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¦„(ì¼ë¶€): {msg}")
