# app/services/chunker.py
from __future__ import annotations
import re, hashlib, os, itertools
from typing import List, Tuple, Callable, Iterable, Any, Dict

# ===================== ìœ í‹¸ =====================

def _ensure_encode(encode: Callable[[str], Iterable[Any]]) -> Callable[[str], List[Any]]:
    def _enc(s: str) -> List[Any]:
        try:
            out = encode(s) or []
        except Exception:
            out = []
        if not isinstance(out, list):
            out = list(out)
        return out
    return _enc

def _toklen(enc: Callable[[str], List[Any]], s: str) -> int:
    return len(enc(s)) if s else 0

def _norm_text(s: str) -> str:
    """
    ì „ì—­ ì¤‘ë³µ íŒì •ì„ ìœ„í•œ ê°•í•œ ì •ê·œí™”:
    - ê³µë°±/ê°œí–‰ ì••ì¶•
    - í˜ì´ì§€ë°”(â€¦ | 12) ì œê±°
    - ì—°ì† êµ¬ë‘ì /ê³µë°± ì •ë¦¬
    """
    if not s:
        return ""
    t = s
    # ì–‘ìª½ ê³µë°± ì •ë¦¬
    t = t.strip()
    # í˜ì´ì§€ ë°” íŒ¨í„´ ì œê±°: "â€¦ | 12" ë˜ëŠ” "â€¦ | 6" ë“±
    t = re.sub(r"\s*\|\s*\d+\s*$", "", t, flags=re.M)
    # ì—°ì† ê³µë°± ì••ì¶•
    t = re.sub(r"[ \t]+", " ", t)
    # ì¤„ë‹¨ìœ„ íŠ¸ë¦¼ í›„ ë¹ˆì¤„ 1ê°œë¡œ
    lines = [ln.strip() for ln in t.splitlines()]
    lines = [ln for ln in lines if ln]
    t = "\n".join(lines)
    return t

def _hash_text(s: str) -> str:
    return hashlib.sha1(_norm_text(s).encode("utf-8", errors="ignore")).hexdigest()

# ---- bbox helper ----
def _clamp_bbox(bb: List[float]) -> List[float]:
    x0, y0, x1, y1 = map(float, bb[:4])
    x0 = max(0.0, x0); y0 = max(0.0, y0)
    x1 = max(x0, x1); y1 = max(y0, y1)
    return [x0, y0, x1, y1]
# ===================== íŒ¨í„´ =====================

# ì œëª©/ì¡°ë¬¸ í—¤ë”© íƒì§€: í•œêµ­ì–´ ë³´ê³ ì„œ/ë§¤ë‰´ì–¼ì— í”í•œ íŒ¨í„´ë“¤ì„ í­ë„“ê²Œ í—ˆìš©
HEADING_RE = re.compile(
    r"^\s*(?:"
    r"ì œ?\s*\d+(?:\.\d+)*\s*ì¥?"        # ì œ1ì¥ / 1. / 1.2.3 / 1 ì¥
    r"|[IVXLC]+\.?"                    # I. / II / IV ë“±
    r"|\d+\s*[)\]]"                    # 1) / 2] ë“±
    r"|[A-Z]\)"                        # A) / B)
    r"|#{1,6}\s+"                      # markdown í—¤ë”©
    r"|[â– â–¡â–ªâ—¦â€¢Â·\-\*]\s*\d*"             # ê¸€ë¨¸ë¦¬í‘œ(ì˜µì…˜ ìˆ«ì)
    r")\s*\S.*$",
    re.M,
)
LIST_RE    = re.compile(r"^\s*[-*â€¢Â·]\s+\S|^\s*\d+\.\s+\S", re.M)
SENT_SPLIT_RE = re.compile(r"(?<=[\.!?])\s+|(?<=[ã€‚ï¼ï¼Ÿ])|(?<=\n)")

# ===================== ë¬¸ì¥/ë¬¸ë‹¨ ë¶„ë¦¬ =====================

def _split_sentences(text: str) -> List[str]:
    if not text:
        return []
    parts = [s.strip() for s in SENT_SPLIT_RE.split(text) if s and s.strip()]
    fused: List[str] = []
    buf: List[str] = []
    for p in parts:
        buf.append(p)
        if p.endswith(("ë‹¤.", "ìš”.", ".", "!", "?", "â€¦")):
            fused.append(" ".join(buf).strip()); buf = []
    if buf:
        fused.append(" ".join(buf).strip())
    return fused

def _split_to_paragraphs(text: str) -> List[str]:
    """ë¹ˆ ì¤„ë¡œ í¬ê²Œ ë‚˜ëˆ„ê³ , ë¸”ë¡ ë‚´ë¶€ëŠ” ì œëª©/ë¦¬ìŠ¤íŠ¸ ì‹œì‘ì¤„ ê¸°ì¤€ìœ¼ë¡œ ìª¼ê°¬."""
    if not text:
        return []
    blocks = re.split(r"\n{2,}", text.strip())
    out: List[str] = []
    for b in blocks:
        b = (b or "").strip()
        if not b:
            continue
        lines = b.splitlines()
        piece: List[str] = []

        def _emit():
            if piece:
                seg = "\n".join(piece).strip()
                if seg:
                    out.append(seg)

        for ln in lines:
            if HEADING_RE.match(ln) or LIST_RE.match(ln):
                _emit()
                piece = [ln]
            else:
                piece.append(ln)
        _emit()
    return out

# ===================== ê³¼ëŒ€ ë¬¸ë‹¨ ì„¸ë¶„í™” =====================

def _split_oversize_to_tokens(text: str, encode: Callable[[str], List[Any]], target_tokens: int) -> List[str]:
    out: List[str] = []
    sents = _split_sentences(text)
    cur = ""
    for s in sents:
        if _toklen(encode, s) > target_tokens:
            if cur:
                out.append(cur.strip()); cur = ""
            words = re.split(r"(\s+)", s)  # ê³µë°± ë³´ì¡´
            seg = ""
            for w in words:
                candidate = (seg + w) if seg else w
                if _toklen(encode, candidate) > target_tokens:
                    if seg.strip():
                        out.append(seg.strip())
                    # ë‹¨ì–´ ìì²´ê°€ í° ê²½ìš° ë¬¸ì ë‹¨ìœ„ë¡œ ê°•ì œ ë¶„ì ˆ
                    if _toklen(encode, w) > target_tokens:
                        buf = ""
                        for ch in list(w):
                            cand = buf + ch
                            if _toklen(encode, cand) > target_tokens:
                                if buf.strip():
                                    out.append(buf.strip())
                                buf = ch
                            else:
                                buf = cand
                        if buf.strip():
                            out.append(buf.strip())
                        seg = ""
                    else:
                        seg = w
                else:
                    seg = candidate
            if seg.strip():
                out.append(seg.strip())
            continue
        joined = (cur + " " + s).strip() if cur else s
        if _toklen(encode, joined) <= target_tokens:
            cur = joined
        else:
            if cur.strip():
                out.append(cur.strip())
            cur = s
    if cur.strip():
        out.append(cur.strip())
    return out

def _normalize_paras(paras: List[str], encode: Callable[[str], List[Any]], target_tokens: int) -> List[str]:
    out: List[str] = []
    for p in paras:
        if _toklen(encode, p) <= target_tokens:
            if p.strip():
                out.append(p.strip())
        else:
            out.extend(_split_oversize_to_tokens(p, encode, target_tokens))
    return out

# ===================== í˜ì´ì§€ í—¤ë”/í‘¸í„° ì œê±° =====================
from collections import defaultdict
import re
from typing import List, Tuple, Set, Dict, Iterable, Any

_WS_RE = re.compile(r"\s+")

def _normalize_line(s: str) -> str:
    return _WS_RE.sub(" ", (s or "").strip())

def _iter_nonempty_lines(text: str) -> List[str]:
    return [ln for ln in (text or "").splitlines() if ln.strip()]

def _extract_page_texts(pages: Iterable[Any]) -> List[str]:
    """pagesê°€ str / (page_no, text) / {'page':..,'text':..} ë“±ì„ ì„ì–´ë„ í…ìŠ¤íŠ¸ë§Œ ë½‘ì•„ëƒ„."""
    out: List[str] = []
    for it in pages or []:
        if isinstance(it, str):
            out.append(it)
        elif isinstance(it, (list, tuple)):
            # (page_no, text, ...) í˜•íƒœ ì§€ì›
            if len(it) >= 2:
                out.append(str(it[1] or ""))
            else:
                out.append(str(it[0] or ""))
        elif isinstance(it, dict):
            out.append(str(it.get("text", "") or ""))
        else:
            out.append(str(it or ""))
    return out

def _detect_repeating_lines(
    pages: List[Any],
    *,
    # ì‹  íŒŒë¼ë¯¸í„°ëª…
    k_head: int = 3,
    k_tail: int = 3,
    min_len: int = 6,
    max_len: int = 120,
    min_support: float = 0.8,
    # êµ¬ íŒŒë¼ë¯¸í„°ëª…(í˜¸í™˜ìš©)
    head_n: int | None = None,
    tail_n: int | None = None,
    min_ratio: float | None = None,
) -> List[Tuple[str, int]]:
    """
    ì—¬ëŸ¬ í˜ì´ì§€ì— ë°˜ë³µë˜ëŠ” í—¤ë”/í‘¸í„° ë¼ì¸ì„ ê°ì§€.
    ë°˜í™˜: [(line, page_count), ...]
    - pages: str ë˜ëŠ” (page_no, text) ë˜ëŠ” {'page':..,'text':..} ì„ì—¬ ìˆì–´ë„ OK
    - head_n/tail_n/min_ratio(êµ¬ëª…)ë„ ë°›ìœ¼ë©°, ìˆìœ¼ë©´ ìš°ì„  ì ìš©
    """
    # êµ¬ íŒŒë¼ë¯¸í„°ëª… ìš°ì„  ì ìš©
    if head_n is not None:
        k_head = int(head_n)
    if tail_n is not None:
        k_tail = int(tail_n)
    if min_ratio is not None:
        min_support = float(min_ratio)

    texts = _extract_page_texts(pages)
    if not texts:
        return []

    N = len(texts)
    threshold = max(2, int(N * min_support))

    page_appear: Dict[str, set[int]] = defaultdict(set)

    for pi, pg_text in enumerate(texts):
        lines = _iter_nonempty_lines(pg_text)
        if not lines:
            continue

        head = lines[:k_head] if k_head > 0 else []
        tail = lines[-k_tail:] if k_tail > 0 else []
        candidates = head + tail

        for raw in candidates:
            line = _normalize_line(raw)
            if not (min_len <= len(line) <= max_len):
                continue
            page_appear[line].add(pi)

    repeating = [(line, len(idx_set)) for line, idx_set in page_appear.items() if len(idx_set) >= threshold]
    repeating.sort(key=lambda x: (-x[1], x[0]))
    return repeating

def _repeating_to_set(repeating: Any) -> Set[str]:
    """
    repeatingì´ set[str] / dict[str,int] / list[tuple[str,int]] / list[str]
    ì–´ëŠ ê²ƒì´ë“  ì •ê·œí™”ëœ set[str]ë¡œ ë³€í™˜
    """
    if not repeating:
        return set()
    if isinstance(repeating, set):
        return { _normalize_line(x) for x in repeating }
    if isinstance(repeating, dict):
        return { _normalize_line(k) for k in repeating.keys() }
    if isinstance(repeating, (list, tuple)):
        out: Set[str] = set()
        for it in repeating:
            if isinstance(it, (list, tuple)) and it:
                out.add(_normalize_line(str(it[0])))
            else:
                out.add(_normalize_line(str(it)))
        return out
    # ê¸°íƒ€ íƒ€ì…
    return { _normalize_line(str(repeating)) }


def _strip_repeating_lines(text: str, repeating_any: Any) -> str:
    """
    textì—ì„œ ë°˜ë³µ ë¼ì¸ ì œê±°. repeating_anyëŠ” set/dict/list[tuple]/list[str] ë“± ë¬´ì—‡ì´ë“  OK.
    """
    if not text:
        return ""
    repeating = _repeating_to_set(repeating_any)
    if not repeating:
        return text

    out: List[str] = []
    for ln in (text or "").splitlines():
        # í˜ì´ì§€ë°” " | 12" ê°™ì€ ê¼¬ë¦¬ ì œê±° í›„ ë¹„êµ
        key = re.sub(r"\s*\|\s*\d+\s*$", "", ln.strip())
        if _normalize_line(key) in repeating:
            continue
        out.append(ln)
    return "\n".join(out)


# ===================== í† í° íŒ¨í‚¹(ì˜¤ë²„ë©/ì¤‘ë³µ ë³´í˜¸) =====================

def _tail_paras_by_tokens(paras: List[str], encode: Callable[[str], List[Any]], max_tokens: int) -> tuple[List[str], List[Any]]:
    tail: List[str] = []
    tail_ids: List[Any] = []
    for p in reversed(paras):
        ids = encode(p)
        if len(ids) + len(tail_ids) > max_tokens:
            break
        tail.insert(0, p)
        tail_ids = ids + tail_ids
    return tail, tail_ids

def pack_by_tokens(
    paras: List[str],
    encode: Callable[[str], List[Any]],
    target_tokens: int = 128,
    overlap_tokens: int = 32,
) -> List[str]:
    """
    ì˜¤ë²„ë©ì€ ìœ ì§€í•˜ë˜, 'ì™„ì „íˆ ë™ì¼í•œ ë¬¸ë‹¨'ì´ ë‘ ë²ˆ ë“¤ì–´ê°€ë©´ ë°©ì§€.
    ë„ˆë¬´ ì‘ì€ ì²­í¬(í•˜í•œ ë¯¸ë§Œ)ëŠ” ë‹¤ìŒ/ì´ì „ê³¼ ìë™ ë³‘í•©
    """
    MIN_TOK = int(os.getenv("RAG_MIN_CHUNK_TOKENS", "0") or 0)
    chunks: List[str] = []
    cur_paras: List[str] = []
    cur_ids: List[Any] = []

    def _emit(final: bool=False):
        if not cur_paras:
            return
        piece = "\n\n".join(cur_paras).strip()
        if not piece:
            return
        # í•˜í•œ ë¯¸ë§Œì¸ë° ìµœì¢… emitì´ ì•„ë‹ˆë©´ ì¼ë‹¨ ë³´ë¥˜(ë‹¤ìŒ ë¬¸ë‹¨ê³¼ í•©ì¹˜ê¸° ëŒ€ê¸°)
        if MIN_TOK > 0 and _toklen(encode, piece) < MIN_TOK and not final:
            return

        # ìµœì¢… emitì¸ë° í•˜í•œ ë¯¸ë§Œì´ë©´ ì§ì „ ì²­í¬ì— ë³‘í•© ì‹œë„
        if MIN_TOK > 0 and _toklen(encode, piece) < MIN_TOK and final and chunks:
            prev = chunks[-1]
            merged = (prev + "\n\n" + piece).strip()
            if _toklen(encode, merged) <= target_tokens and _norm_text(prev) != _norm_text(merged):
                chunks[-1] = merged
                cur_paras.clear(); cur_ids.clear()
                return
            # ë³‘í•© ë¶ˆê°€ë©´ ê·¸ëƒ¥ ë‚´ë³´ëƒ„ (ìœ ì‹¤ ë°©ì§€)

        # ë°”ë¡œ ì§ì „ê³¼ ì™„ì „ ë™ì¼í•˜ë©´ ìŠ¤í‚µ
        if chunks and _norm_text(chunks[-1]) == _norm_text(piece):
            cur_paras.clear(); cur_ids.clear()
            return
        chunks.append(piece)
        cur_paras.clear(); cur_ids.clear()

    for p in paras:
        if not p or not p.strip():
            continue
        ids = encode(p)
        # ë¬¸ë‹¨ì´ targetë³´ë‹¤ í¬ë©´ ë‹¨ë… ì²­í¬ë¡œ(ë‹¨, ì§ì „ê³¼ ë™ì¼ì‹œ ìŠ¤í‚µ)
        if len(ids) > target_tokens:
            _emit()
            candidate = p.strip()
            if not (chunks and _norm_text(chunks[-1]) == _norm_text(candidate)):
                chunks.append(candidate)
            cur_paras, cur_ids = [], []
            continue

        if not cur_paras:
            cur_paras, cur_ids = [p], ids
            continue

        if len(cur_ids) + len(ids) <= target_tokens:
            cur_paras.append(p)
            cur_ids += ids
        else:
            # ---- í•˜í•œ ë³´í˜¸: ëŠê¸° ì§ì „ì— ìƒˆ ë¬¸ë‹¨ì„ ë” ë¶™ì—¬ë„ targetì„ ë„˜ì§€ ì•Šìœ¼ë©´ ë¶™ì—¬ì„œ ìíˆ¬ë¦¬ ë°©ì§€ ----
            if MIN_TOK > 0 and cur_paras:
                tmp = "\n\n".join(cur_paras + [p]).strip()
                if _toklen(encode, tmp) <= target_tokens:
                    cur_paras.append(p)
                    cur_ids += ids
                    continue
            _emit()
            tail_paras, tail_ids = _tail_paras_by_tokens(cur_paras, encode, max(0, overlap_tokens))
            # tail + p ê°€ target ì´ˆê³¼í•˜ë©´ pë¥¼ ë‹¨ë…ìœ¼ë¡œ
            if len(tail_ids) + len(ids) > target_tokens:
                candidate = p.strip()
                if not (chunks and _norm_text(chunks[-1]) == _norm_text(candidate)):
                    chunks.append(candidate)
                cur_paras, cur_ids = [], []
            else:
                cur_paras, cur_ids = tail_paras + [p], tail_ids + ids
    _emit(final=True)

    # ì¸ì ‘ ë™ì¼ ì œê±°(ì´ë¯¸ í–ˆì§€ë§Œ 2ì°¨ ì•ˆì „ë§)
    dedup: List[str] = []
    for c in chunks:
        if not dedup or _norm_text(dedup[-1]) != _norm_text(c):
            dedup.append(c)
    return dedup

# ===================== í¼ë¸”ë¦­ API =====================

def chunk_text(text: str, max_length: int = 500) -> list[str]:
    lines = (text or "").split("\n")
    chunks, buf = [], ""
    for line in lines:
        if len(buf) + len(line) < max_length:
            buf += line + "\n"
        else:
            if buf.strip():
                chunks.append(buf.strip())
            buf = line + "\n"
    if buf.strip():
        chunks.append(buf.strip())
    return chunks

def smart_chunk_pages(
    pages: List[Tuple[int, str]],
    encode,
    target_tokens: int | None = None,
    overlap_tokens: int | None = None,
) -> List[Tuple[str, dict]]:
    """
    ì…ë ¥: [(page_no, text)]
    ì¶œë ¥: [(chunk_text, {"page":int, "section":str, "idx":int})]
    - í˜ì´ì§€ í—¤ë”/í‘¸í„° ìë™ ì œê±°
    - ì „ì—­ ì¤‘ë³µ ì œê±°(ê°•í•œ ì •ê·œí™” í•´ì‹œ)
    """
    enc = _ensure_encode(encode)

    # ê¸°ë³¸ ê¸¸ì´
    if target_tokens is None or overlap_tokens is None:
        try:
            from app.services.embedding_model import get_embedding_model
            m = get_embedding_model()
            max_len = int(getattr(m, "max_seq_length", 128))
        except Exception:
            max_len = 128
        if target_tokens is None:
            target_tokens = max(64, max_len - 16)
        if overlap_tokens is None:
            overlap_tokens = min(32, target_tokens // 4)

    target_tokens = int(max(16, target_tokens))
    overlap_tokens = int(max(0, min(overlap_tokens, target_tokens // 2)))

    # 1) ë°˜ë³µ ë¼ì¸ ê°ì§€
    repeating = _detect_repeating_lines(pages)

    results: List[Tuple[str, dict]] = []
    seen_hashes: set[str] = set()

    for item in pages:
        if isinstance(item, (list, tuple)) and len(item) >= 2:
            page_no, text = item[0], item[1]
        elif isinstance(item, dict):
            page_no, text = item.get("page", 1), item.get("text", "")
        else:
            continue

        try:
            page_no = int(page_no)
        except Exception:
            page_no = 1
        text = str(text or "").strip()
        if not text:
            continue

        # 2) í˜ì´ì§€ ë°˜ë³µ ìƒ/í•˜ë‹¨ ì œê±°
        text = _strip_repeating_lines(text, repeating)

        # 3) ë¬¸ë‹¨ â†’ ê³¼ëŒ€ ë¬¸ë‹¨ ì„¸ë¶„í™” â†’ í† í° íŒ¨í‚¹
        paras = _split_to_paragraphs(text)
        safe_paras: List[str] = []
        for p in paras:
            if _toklen(enc, p) <= target_tokens:
                if p.strip():
                    safe_paras.append(p.strip())
            else:
                safe_paras.extend(_split_oversize_to_tokens(p, enc, target_tokens))

        chs = pack_by_tokens(safe_paras, enc, target_tokens=target_tokens, overlap_tokens=overlap_tokens)

        # 4) ì „ì—­ ì¤‘ë³µ ì œê±°
        for i, ch in enumerate(chs):
            norm = _norm_text(ch)
            if not norm:
                continue
            h = _hash_text(norm)
            if h in seen_hashes:
                continue
            seen_hashes.add(h)

            # section(ì œëª©) ì¶”ì¶œ
            section = ""
            for line in ch.splitlines():
                if HEADING_RE.match(line):
                    section = line.strip()
                    break

            results.append((ch, {"page": page_no, "section": section, "idx": i}))

    return results

def token_safe_chunks(text: str, target_tokens: int | None = None, overlap_tokens: int = 32) -> list[str]:
    from app.services.embedding_model import get_embedding_model
    try:
        model = get_embedding_model()
        tok = getattr(model, "tokenizer", None)
        max_len = int(getattr(model, "max_seq_length", 128))
    except Exception:
        tok, max_len = None, 128

    if tok is None or not hasattr(tok, "encode"):
        return chunk_text(text, max_length=1000)

    if target_tokens is None:
        target_tokens = max(64, max_len - 16)
    target_tokens = int(max(16, target_tokens))
    overlap_tokens = int(max(0, min(overlap_tokens, target_tokens // 4)))

    def _enc(s: str) -> List[int]:
        return tok.encode(s, add_special_tokens=False) or []

    paras = _split_to_paragraphs(text or "")
    safe_paras = _normalize_paras(paras, _enc, target_tokens)
    chs = pack_by_tokens(safe_paras, _enc, target_tokens=target_tokens, overlap_tokens=overlap_tokens)

    # ì „ì—­ ì¤‘ë³µ í•œ ë²ˆ ë” ë°©ì§€(í•¨ìˆ˜ ë‹¨ìœ„)
    dedup: List[str] = []
    seen: set[str] = set()
    for c in chs:
        h = _hash_text(c)
        if h in seen:
            continue
        seen.add(h)
        if not dedup or _norm_text(dedup[-1]) != _norm_text(c):
            dedup.append(c)
    return dedup


def _guess_section_for_paragraph(paragraph: str, last_section: str) -> str:
    # ë¬¸ë‹¨ ë§¨ ì• ë¼ì¸ì´ ì œëª© íŒ¨í„´ì´ë©´ í•´ë‹¹ ë¼ì¸ì„ ì„¹ì…˜ìœ¼ë¡œ, ì•„ë‹ˆë©´ ì§ì „ ì„¹ì…˜ ê³„ìŠ¹
    for ln in (paragraph or "").splitlines():
        if HEADING_RE.match(ln):
            return ln.strip()
        break
    return last_section

def _clean(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[\s\p{P}]+", " ", s, flags=re.UNICODE)
    return s.strip()

def _token_set(s: str) -> set[str]:
    return set(_clean(s).split())

def _similar(a: str, b: str) -> float:
    A, B = _token_set(a), _token_set(b)
    if not A or not B: 
        return 0.0
    inter = len(A & B)
    return inter / min(len(A), len(B))   # ë¶€ë¶„ì¼ì¹˜ì— ê´€ëŒ€

def _attach_bboxes_to_paragraph(para: str, page_blocks: list[dict]) -> list[list[float]]:
    if not para or not page_blocks:
        return []
    p = para.replace("\n", " ").strip()
    if not p:
        return []
    bboxes: list[list[float]] = []
    # ì„ê³„ì¹˜: 0.5 ì •ë„ë©´ ë³´ìˆ˜ì , 0.3ì´ë©´ ê´€ëŒ€
    THRESH = 0.35
    for blk in page_blocks:
        t = (blk.get("text") or "").strip()
        if not t:
            continue
        if _similar(p[:200], t[:200]) >= THRESH or _similar(p, t) >= THRESH:
            bb = blk.get("bbox")
            if isinstance(bb, (list, tuple)) and len(bb) == 4:
                bboxes.append(_clamp_bbox([float(bb[0]), float(bb[1]), float(bb[2]), float(bb[3])]))
    # ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for bb in bboxes:
        key = tuple(round(v, 2) for v in bb)  # ì¢Œí‘œ ë°˜ì˜¬ë¦¼ìœ¼ë¡œ ê·¼ì ‘ ì¤‘ë³µ ì œê±°
        if key not in seen:
            seen.add(key); uniq.append(bb)
    return uniq

from typing import Any  # ğŸ”¸ íŒŒì¼ ìƒë‹¨ì— ì—†ìœ¼ë©´ ì¶”ê°€

def smart_chunk_pages_plus(
    pages: List[Tuple[int, str]],
    encode,
    target_tokens: int | None = None,
    overlap_tokens: int | None = None,
    layout_blocks: dict[int, list[dict]] | None = None,
) -> List[Tuple[str, dict]]:
    """
    í™•ì¥ ì²­í‚¹:
      - ì„¹ì…˜(ì œëª©) ê³„ìŠ¹
      - í˜ì´ì§€ ê²½ê³„ ì˜¤ë²„ë©(ì˜µì…˜)
      - ê° ì²­í¬ì— í¬í•¨ëœ í˜ì´ì§€ ëª©ë¡ + í˜ì´ì§€ë³„ BBOX ë¦¬ìŠ¤íŠ¸ í¬í•¨
    ë°˜í™˜: [(chunk_text, {"page":int, "section":str, "pages":[int], "bboxes":{page:[bbox...]}})]
    """
    # 0) í† í¬ë‚˜ì´ì €/ê¸°ë³¸ ê¸¸ì´
    enc = _ensure_encode(encode)
    try:
        if target_tokens is None or overlap_tokens is None:
            from app.services.embedding_model import get_embedding_model
            m = get_embedding_model()
            max_len = int(getattr(m, "max_seq_length", 128))
    except Exception:
        max_len = 128

    if target_tokens is None:
        target_tokens = max(64, max_len - 16)
    if overlap_tokens is None:
        overlap_tokens = min(96, target_tokens // 3)

    target_tokens = int(max(16, target_tokens))
    # ë¬¸ë‹¨ ë‹¨ìœ„ ì˜¤ë²„ë©(í† í° ê°’ì€ ìƒí•œìœ¼ë¡œë§Œ ì‚¬ìš©)
    overlap_tokens = int(max(0, min(overlap_tokens, target_tokens // 2)))
    cross_page = os.getenv("RAG_CROSS_PAGE_CHUNK", "1") == "1"

    # 1) ë°˜ë³µ ë¼ì¸(ëŸ¬ë‹í—¤ë”/í‘¸í„°) ì œê±° í›„ë³´ ìˆ˜ì§‘
    repeating = _detect_repeating_lines(pages, head_n=3, tail_n=3, min_ratio=0.2)

    # 2) í˜ì´ì§€â†’ë¬¸ë‹¨ ì „ê°œ + ì„¹ì…˜ ì¶”ì • + paraë³„ bbox ë¶€ì°©
    para_items: list[dict] = []
    last_section = ""
    for page_no, text in pages:
        txt = _strip_repeating_lines(text, repeating)
        paras = _split_to_paragraphs(txt)

        # ê³¼ëŒ€ ë¬¸ë‹¨ ì„¸ë¶„í™”
        safe_paras: List[str] = []
        for p in paras:
            if not p.strip():
                continue
            if _toklen(enc, p) <= target_tokens:
                safe_paras.append(p.strip())
            else:
                safe_paras.extend(_split_oversize_to_tokens(p, enc, target_tokens))

        # para each â†’ attach section/bbox
        blocks = (layout_blocks or {}).get(int(page_no), [])
        for p in safe_paras:
            sec = _guess_section_for_paragraph(p, last_section)
            if sec and sec != last_section:
                last_section = sec

            # ìƒˆ í˜ì´ì§€ ì²« ë¬¸ë‹¨ì´ ì§ì „ ì„¹ì…˜ ì œëª©ê³¼ ê°™ì€ í—¤ë”©ì´ë©´ ì œê±°
            lines = p.splitlines()
            if lines and HEADING_RE.match(lines[0] or "") and lines[0].strip() == last_section:
                p = "\n".join(lines[1:]).strip()
                if not p:
                    continue

            # bbox ë¶€ì°©(ìˆ˜ì‹  ê°’ ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ í˜¼ì¬ ë°©ì–´)
            bbs_raw = _attach_bboxes_to_paragraph(p, blocks) or []
            bbs: list[list[float]] = []
            seen_bb: set[tuple[float, float, float, float]] = set()
            for bb in bbs_raw:
                try:
                    x0, y0, x1, y1 = bb
                    key = (round(float(x0), 3), round(float(y0), 3),
                           round(float(x1), 3), round(float(y1), 3))
                except Exception:
                    continue
                if key not in seen_bb:
                    seen_bb.add(key)
                    bbs.append([key[0], key[1], key[2], key[3]])

            para_items.append({
                "page": int(page_no),
                "section": last_section,
                "text": p,
                "bboxes": bbs,
                "toklen": _toklen(enc, p),
            })

    # 3) í† í° íŒ¨í‚¹(ë¬¸ë‹¨ ë‹¨ìœ„), í˜ì´ì§€ ê²½ê³„ ì œì–´ + ì˜¤ë²„ë©
    chunks: List[Tuple[str, dict]] = []
    cur_texts: List[str] = []
    cur_pages: List[int] = []
    cur_bboxes: dict[int, list[list[float]]] = {}
    cur_tokens: int = 0
    cur_section: str = ""
    SECTION_CAP = int(os.getenv("RAG_SECTION_MAX", "160"))
    seen_text_hashes: set[str] = set()

    def _dedup_page_bboxes(pb: dict[int, list[list[float]]]) -> dict[int, list[list[float]]]:
        out: dict[int, list[list[float]]] = {}
        for pg, lst in pb.items():
            seen: set[tuple[float, float, float, float]] = set()
            ulst: list[list[float]] = []
            for bb in lst:
                try:
                    key = (round(float(bb[0]), 3), round(float(bb[1]), 3),
                           round(float(bb[2]), 3), round(float(bb[3]), 3))
                except Exception:
                    continue
                if key not in seen:
                    seen.add(key)
                    ulst.append([key[0], key[1], key[2], key[3]])
            if ulst:
                out[int(pg)] = ulst
        return out

    def _emit():
        nonlocal cur_texts, cur_pages, cur_bboxes, cur_tokens, cur_section
        if not cur_texts:
            return
        piece = "\n\n".join(cur_texts).strip()
        if not piece:
            # reset
            cur_texts, cur_pages, cur_bboxes, cur_tokens, cur_section = [], [], {}, 0, ""
            return

        norm = _norm_text(piece)
        h = _hash_text(norm)
        if h in seen_text_hashes:
            cur_texts, cur_pages, cur_bboxes, cur_tokens, cur_section = [], [], {}, 0, ""
            return
        seen_text_hashes.add(h)

        meta = {
            "type": "text",
            "section": (cur_section or "")[:SECTION_CAP],
            "pages": sorted(list(dict.fromkeys(cur_pages))),
            "bboxes": _dedup_page_bboxes(cur_bboxes),
        }
        meta_line = "META: " + json.dumps(meta, ensure_ascii=False)
        chunk_text = (meta_line + "\n" + piece) if piece else meta_line

        comp_meta = {
            "page": int(meta["pages"][0]) if meta["pages"] else (cur_pages[0] if cur_pages else 0),
            "section": meta["section"],
            "pages": meta["pages"],
            "bboxes": meta["bboxes"],
        }
        if not chunks or _norm_text(chunks[-1][0]) != _norm_text(chunk_text):
            chunks.append((chunk_text, comp_meta))

        # reset
        cur_texts, cur_pages, cur_bboxes, cur_tokens, cur_section = [], [], {}, 0, ""

    import json  # í•œ ë²ˆë§Œ import

    # ë¬¸ë‹¨ ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ì¼ë¶€ ë¬¸ë‹¨ì„ ë³´ê´€
    def _start_new_with_overlap(prev_texts: List[str], prev_pages: List[int],
                                prev_bboxes: dict[int, list[list[float]]],
                                section: str):
        nonlocal cur_texts, cur_pages, cur_bboxes, cur_tokens, cur_section
        if overlap_tokens <= 0 or not prev_texts:
            cur_texts, cur_pages, cur_bboxes, cur_tokens, cur_section = [], [], {}, 0, section
            return
        # ë’¤ì—ì„œë¶€í„° ë¬¸ë‹¨ì„ ëª¨ì•„ overlap_tokens ê·¼ì‚¬ ë§Œì¡±
        toks = 0
        sel: List[str] = []
        sel_pages: List[int] = []
        sel_bboxes: dict[int, list[list[float]]] = {}
        for t, pg in zip(reversed(prev_texts), reversed(prev_pages)):
            tl = _toklen(enc, t)
            if toks + tl > overlap_tokens and sel:
                break
            sel.insert(0, t)
            if (not sel_pages) or pg != sel_pages[-1]:
                sel_pages.append(pg)
            if pg in (prev_bboxes or {}):
                sel_bboxes.setdefault(pg, []).extend(prev_bboxes[pg])
            toks += tl

        cur_texts = sel
        cur_pages = list(dict.fromkeys(sel_pages))
        cur_bboxes = {k: v[:] for k, v in sel_bboxes.items()}
        cur_tokens = sum(_toklen(enc, t) for t in cur_texts)
        cur_section = section

    for it in para_items:
        ids_len = it["toklen"]

        # ì²« ë¬¸ë‹¨
        if not cur_texts:
            cur_texts = [it["text"]]
            cur_tokens = ids_len
            cur_pages = [it["page"]]
            if it["bboxes"]:
                cur_bboxes[it["page"]] = list(it["bboxes"])
            cur_section = it["section"]
            continue

        # ì„¹ì…˜ ë³€ê²½ ì‹œ ëŠê³  ìƒˆë¡œ ì‹œì‘(+ì˜¤ë²„ë© ì—†ìŒ: ì„¹ì…˜ ê²½ê³„ ìš°ì„ )
        if it["section"] and it["section"] != cur_section:
            _emit()
            # ì„¹ì…˜ ë°”ë€Œë©´ ì˜¤ë²„ë© ì—†ì´ ìƒˆë¡œ ì‹œì‘
            cur_texts = [it["text"]]
            cur_tokens = ids_len
            cur_pages = [it["page"]]
            cur_bboxes = {it["page"]: list(it["bboxes"])} if it["bboxes"] else {}
            cur_section = it["section"]
            continue

        # cross_pageê°€ êº¼ì ¸ ìˆê³  ìƒˆ í˜ì´ì§€ë©´ ëŠê¸°
        if (not cross_page) and (it["page"] != cur_pages[-1]):
            prev_texts, prev_pages, prev_bboxes, prev_sec = cur_texts[:], cur_pages[:], {k: v[:] for k, v in cur_bboxes.items()}, cur_section
            _emit()
            _start_new_with_overlap(prev_texts, prev_pages, prev_bboxes, prev_sec)
            # ì´ì–´ì„œ í˜„ì¬ ë¬¸ë‹¨ì„ ë„£ì–´ë³¸ë‹¤
            if cur_tokens + ids_len > target_tokens and cur_texts:
                _emit()
                cur_texts = [it["text"]]
                cur_tokens = ids_len
                cur_pages = [it["page"]]
                cur_bboxes = {it["page"]: list(it["bboxes"])} if it["bboxes"] else {}
                cur_section = it["section"]
            else:
                cur_texts.append(it["text"])
                cur_tokens += ids_len
                if it["page"] not in cur_pages:
                    cur_pages.append(it["page"])
                if it["bboxes"]:
                    cur_bboxes.setdefault(it["page"], []).extend(it["bboxes"])
            continue

        # ê°™ì€ ì„¹ì…˜ ì•ˆì—ì„œ ìš©ëŸ‰ ê²€ì‚¬
        if cur_tokens + ids_len <= target_tokens:
            cur_texts.append(it["text"])
            cur_tokens += ids_len
            if it["page"] not in cur_pages:
                cur_pages.append(it["page"])
            if it["bboxes"]:
                cur_bboxes.setdefault(it["page"], []).extend(it["bboxes"])
        else:
            # ë„˜ì¹˜ë©´ emit + ì˜¤ë²„ë© í›„ ìƒˆ chunk ì‹œì‘
            prev_texts, prev_pages, prev_bboxes, prev_sec = cur_texts[:], cur_pages[:], {k: v[:] for k, v in cur_bboxes.items()}, cur_section
            _emit()
            _start_new_with_overlap(prev_texts, prev_pages, prev_bboxes, prev_sec)
            # í˜„ì¬ ë¬¸ë‹¨ ì¶”ê°€ ì‹œë„
            if cur_tokens + ids_len > target_tokens and cur_texts:
                # ì˜¤ë²„ë©ë§Œìœ¼ë¡œë„ ê½‰ ì°¼ë‹¤ë©´ í•œ ë²ˆ ë” ë¹„ìš°ê³  ë‹¨ë… ì‹œì‘
                _emit()
                cur_texts = [it["text"]]
                cur_tokens = ids_len
                cur_pages = [it["page"]]
                cur_bboxes = {it["page"]: list(it["bboxes"])} if it["bboxes"] else {}
                cur_section = it["section"]
            else:
                cur_texts.append(it["text"])
                cur_tokens += ids_len
                if it["page"] not in cur_pages:
                    cur_pages.append(it["page"])
                if it["bboxes"]:
                    cur_bboxes.setdefault(it["page"], []).extend(it["bboxes"])

    _emit()

    # 4) ìµœí›„ í´ë°±: ë¬¸ë‹¨ ë‹¨ê³„ì—ì„œ ì•„ë¬´ê²ƒë„ ëª» ë§Œë“¤ì—ˆìœ¼ë©´ raw í˜ì´ì§€ í…ìŠ¤íŠ¸ë¡œ ê³ ì •ê¸¸ì´ ì²­í‚¹
    if not chunks:
        raw_text = "\n\n".join([t for _, t in pages if (t or "").strip()])
        if raw_text.strip():
            ids = enc(raw_text)
            if not ids:
                # í† í¬ë‚˜ì´ì €ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì£¼ë©´ í†µì§œë¡œ 1ê°œë¼ë„
                meta = {"type": "text", "section": "", "pages": [p for p, _ in pages], "bboxes": {}}
                chunks = [("META: " + json.dumps(meta, ensure_ascii=False) + "\n" + raw_text,
                           {"page": pages[0][0] if pages else 0, "section": "", "pages": meta["pages"], "bboxes": {}})]
            else:
                # í† í° ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
                def detok_slice(start_ids, end_ids):
                    # ê°„ë‹¨: í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ê²½ê³„ê°€ ì•„ë‹Œ í† í° ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œë§Œ ì˜ë¼ë‚¸ë‹¤(ì•ˆì „)
                    # ì‹¤ì œ ë³µì›ì€ ì–´ë ¤ìš°ë‹ˆ ids ê¸¸ì´ë¡œë§Œ ê·¼ì‚¬, ì‹¤ì „ì—ì„œ ì¶©ë¶„
                    return raw_text  # ì‹¤ì‚¬ìš© í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ë¶ˆê°€ â†’ í†µì§œ ì²˜ë¦¬
                i = 0
                while i < len(ids):
                    j = min(len(ids), i + target_tokens)
                    piece = raw_text if i == 0 and j == len(ids) else raw_text  # ê·¼ì‚¬
                    meta = {"type": "text", "section": "", "pages": [p for p, _ in pages], "bboxes": {}}
                    chunk_text = "META: " + json.dumps(meta, ensure_ascii=False) + "\n" + piece
                    chunks.append((chunk_text, {"page": pages[0][0] if pages else 0, "section": "", "pages": meta["pages"], "bboxes": {}}))
                    if j >= len(ids):
                        break
                    i = j - overlap_tokens if overlap_tokens > 0 else j

    return chunks
